{"/":{"title":"🗡 lonerapier.xyz","content":"\nI always wanted a corner of my own on this wild zoo that we call internet, and finally made one. I'm Sambhav and I try to write as much as I can to slow down my brain. It's mostly my thoughts and notes which I occassionally write after having some trash thought.\n\nPresently, I work as a Software Engineer at [cred.club](https://cred.club) and have discovered my love for complex engineering and writing. I am a big fan of OSS and try to contribute as much as I can to some mind-blowing projects made and maintanined by truly amazing folks.\n\nMy curiosity generally takes me to some unseen and uninviting places. Currently, it's all about applied cryptography and privacy.\n\nI'll try to mold this website into something of my own.","lastmodified":"2023-07-24T05:57:52.800601582Z","tags":null},"/posts/Optimistic-Rollups":{"title":"Optimistic Rollups","content":"\n# Asserter/Checker Problem\n\n- Asserter makes a claim $A$, checker checks the claim for cost $C$ and gets R as reward, if successful.\n- If Asserter cheats without getting caught, Checker loses $L$ in the form of loss of items of value.\n- Two main threats to worry about: Laziness and Bribery\n- Bribery: Asserter bribes Checker more than the reward $R$. Prevent this by large bond by Asserter so that bribe can’t be bigger than the reward $R$.\n- Laziness: Checker does not check intentionally.\n- If we assume, asserter cheats with prob. X, then\nchecker’s utility comes as:\n\n\u003e $-X*L$, if checker doesn’t\n\u003e\n\u003e $R*X-C$, if checker checks\n\n- So, checking is only worthwhile, if utility of checking \u003e not checking.\n\n\u003e $X \u003e C/(R+L)$\n\n- Thus, *asserter* can cheat with random prob \u003c required and not get caught.\n- This doesn’t depend on how much *asserter* gains from cheating, as long as it’s non-zero which is a bad result.\n- Even adding more *checkers* doesn’t help, as the reward gets distributed which only reduces *checker’s* incentives.\n\nReason this is a problem:\n\n- *Asserter* controls the behaviour of the checker, as the utility of checker depends on asserter’s prob.\n- Need to add an attention parameter in *checker’s* incentives where checker pre-computes *asserter’s* claim off-chain beforehand and has to verify it on-chain time to time.\n- Two new parameters:\n\n\u003e $P$, fraction of time *checker's* will post response\n\u003e\n\u003e $A$, penalty in case *checker* gives wrong answer\n\n- New equation becomes,\n\n$$R*X-C$$\n\n$$-L*X-P*A$$\n\n- If $P*A \u003e C$, then checking is better than not checking\n- cost of checking is low\n\n\u003e Assume 1 assertion / 5 mins and $C = $0.001$ If P = 0.3%, checker will deposit $3.\ncost per assertion = $0.0003 =\u003e $0.01*0.3%\ninterest cost of locking = $0.0003\ntotal cost = $0.0006\n\n- Multiple checkers need to submit proofs differently thus, scaling to multiple checkers efficient.\n\n# Technical Details\n\nChecker: private key $k$, public key $g^k$\n\nHash fn: $H$\n\nComputation to solve: $f(x)$\n\nAsserter challenge: $(x, g^r)$\n\nChecker post on-chain iff $H(g^{rk}, f(x)) \u003c T$\n\n\u003e Note: Only checker and asserter knows $g^k$ and $r$, and $T$ requires $f(x)$\n\nChecker can guess $f(x)$ with prob. G, then multiply deposit with $1/1-G$\n\n- Asserter publishes f(x), can challenge checker's response while publishing r\n- Check the accusation and penalise checker, half the deposit to asserter\n- If asserter $f(x)$ incorrect, accusation reverted.\n- Each checker will have different prob of posting on-chain due to use of private key, thus can’t copy others computation.\n- Asserter now instead of bribing checker, will try to mislead him into giving false information on-chain.\n\n# Links\n\n[(Almost) Everything you need to know about Optimistic Rollup](https://research.paradigm.xyz/rollups)\n\n[The Cheater Checking Problem: Why the Verifier’s Dilemma is Harder Than You Think](https://medium.com/offchainlabs/the-cheater-checking-problem-why-the-verifiers-dilemma-is-harder-than-you-think-9c7156505ca1)\n","lastmodified":"2023-07-24T05:57:52.800601582Z","tags":null},"/posts/SoK":{"title":"Communication Across Distributed Ledgers","content":"\n[Communication Across Distributed\nLedgers](https://eprint.iacr.org/2019/1128.pdf)\n\nAims to develop a guide for designing protocols bridging different types of blockchains (distributed ledgers).\n\nShows that CCC is impossible without **_third party._**\n\nPresents a framework keeping these trust assumptions in mind. Classifies current CCC protocols on the basis of framework.\n\n## Introduction\n\n- NB-AC (_Non-Blocking Atomic Commit_) is used in distributed databases to ensure that correct processes don't have to wait for crashed processes to recover.\n- Can be extrapolated to distributed ledgers by handling _byzantine failures._\n\n## Distributed Ledger Model\n\n- $X, Y$: Blockchains\n- $Lx$, $Ly$: ledgers with _states_ as dynamically evolving sequences of _transactions_\n- state of ledgers progresses in round _r._\n- $L^P[r]$: state of ledger _L_ at round _r_ after all txs till _r-1_, according to some party _P._\n- Consistency is defined by the system\n- ($TX$, $Lx^P[r]$): tx _TX_ is valid for _Lx_ at round _r_ according to _P._\n- $TX$ ∈ $L^P[r]$: TX is included in _L_ as position _r._\n- **Time** $L^P[t]$**:** ledger state at round r or time t.\n\n**Persistence**: $L^P[t] \u003c= L^Q[t’]$, $L^P$ at time $t$ is prefix of $L^Q$ at time $t$’.\n\n**Liveness**: if tx $TX$ is included in ledger $L$ at time $t$, then it will appear in ledger at time $t$’.\n\n### CCC System Model\n\n- $P: TX_P, Q: TX_Q$: separate processes running on two different ledgers with txs\n- $P$ possesses a description $d_Q$ which characterises the transaction $TX_Q$, while $Q$ possesses $d_P$ which characterises $TX_P$\n- Thus, $P$ wants $Q$ to be written to $Ly$ and vice-versa.\n- $m_P, m_Q$: boolean error variables for malicious processes\n\n## Formalisation of correct CCC\n\nGoal: sync of P and Q such that Q is included iff P is included. For example, they can constitute an exchange of assets which must be completed atomically.\n\n**Effectiveness:** if both correct, then both will be included, otherwise none\n\n**Atomicity**: no outcome in which $TX_P$ included but $TX_Q$ not at time $t$’ or vice versa.\n\n**Timeliness**: If a process behaves correctly, $TX_P$ will be included and $Q$ will verify. It is a liveness property.\n\n## Generic CCC Protocol\n\n\u003e $u_x$: liveness delay\n\u003e\n\u003e $k_x$: depth parameter\n\n1. **Setup**: inherently done by both blockchains due to the properties defined above\n2. **Pre-Commit on X**: $P$ writes $TX_P$ to $L^P_X$ at time $t$ in round $r$. Due to persistence and liveness, all honest parties report TX_P as valid in $r+u_x+k_x$.\n3. **Verify**: Q verifies $TX_P$.\n4. **Commit on Y:** $Q$ writes $TX_Q$ to $L^Q_Y$ at time $t$’ in round $r$’.\n5. **Abort**: revert $TX_P$ on $Lx$ in case of verification failure or $Q$ fails\n\n![Image.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/doc/2875A4D6-F00A-45A2-96EE-7222C31E634F/490D063A-EBE6-49DA-A5C7-D53342042837_2/q21xMyyBAkdRygr7FHp2PQr7J452ctL6JuxUysHHwccz/Image.png)\n\nCCC protocols follow two-phase commit design.\n\nPre-commit and commit on Y is executed in parallel following verification and abortion, if required.\n\n## Impossibility of CCC without TTP (Trusted Third Party)\n\nAnalogous to **_Fair Exchange_** Problem.\n\nTTP is basically any entity, be it individual or a committee that either confirms a tx has been successfully included or enforce correct behaviour of $Q$ on $Ly$.\n\nLemma 1: Let $M$ be a system model. Let $C$ be a protocol which solves $CCC$ in $M$. Then there exists a protocol $S$ which solves _Fair Exchange_ in $M$.\n\nSketch: to complete exchange, $TX_Q \\in  Ly$ and $TX_P \\in Lx$.\n\n- _effectiveness_ enforces correct transfer for correct behaviour.\n- Persistence and liveness enforce both txs to be eventually written to respective ledgers.\n- Atomicity $\u003c-\u003e$ Strong Fairness in Fair exchange\n\nSmart contracts or code based solutions can be used to write $TX_Q$ to $Y$, in this case consensus becomes TTP to execute this smart contract.\n\nTTP either becomes the process $P$ itself or another party which submits proof of $P$ inclusion to $Q$.\n\nMany other frameworks for designing a CCC protocol:\n\n- Incentivizing third party\n- Slashing the rewards\n- Optimistic\n\n## CCC Design Framework\n\nThree main types of trust model:\n\n- TTP\n- Synchrony\n- Hybrid\n\n### Pre-Commit Phase\n\n#### Model 1: TTP (Coordinators)\n\nCan participate in two ways:\n\n- Custody of Assets: taking control of protocol participant funds to enforce rules\n- Involvement in consensus: in case of smart contracts, when consensus participants are TTP\n\nCoordinator Implementations\n\n- External Custodians: Committee\n- Consensus Level Custodians (Consensus Committee)\n- External Escrows (Multisig Contracts)\n\n#### Model 2: Synchrony (Locking)\n\n- Locks based on hashes\n- Locks based on signatures\n- Timelock puzzles \u0026 Verifiable delay fns\n- Smart Contracts\n\n#### Model 3: Hybrid\n\nWatchtowers (Other external parties) to be used as fallback if one of the service fails or crashes\n\n### Verification Phase\n\nSame models but applied on verification part\n\n1. External Validators/Smart Contracts\n2. Direct Observation/Relay SCs (Using light clients)\n3. Hybrid using watchtowers\n\n### Abort Phase\n\n## Classification of Existing Protocols\n\n### Exchange Protocols\n\nAtomic exchange of digital goods: $x$ on Chain $X$ again $y$ on $Y$. Both parties pre-commit, then verify and abort in case of failure.\n\n#### Pre-Commit\n\nDone through atomic swaps\n\n- Both parties lock assets on-chain with identical release conditions. _Hashed Timelock contracts_ are the closest implementation of symmetric locks. Signature locks using _ECDSA_ are also used.\n- On turing-complete blockchains, atomic swaps can be handled through smart contracts which can verify the state of chain $Y$ (_chain relay_).\n- Hybrid: symmetric with TTP is used to solve usability challenges in atomic swaps.\n\n#### Verify\n\nDone through external validators in symmetric swaps or through chain relays in SPV based atomic swaps.\n\n#### Abort\n\nTimelocks are set up on assets for a pre-defined duration to prevent indefinite lock up in case of failures.\n\n### Migration Protocols\n\nMigrate the asset $x$ from chain $X$ using write locks on $x$ preventing further use on $X$ and creating a wrapped version of same asset on $Y$.\n\nFour main use cases of these protocols:\n\n- Wrapped version of assets between chains\n- communication b/w shards\n- sidechains\n- bootstrapping a new chain\n\n#### Pre-commit\n\nRelies on a single/committee based external custodian for TTP or through multisigs.\n\n**Sidechains**: same approach of depositing on chain $x$ controlled via multisigs which approve asset $y$ on chain $Y$.\n\n**Shards**: utilises the same security and consensus model as the main chain is same for all shards.\n\n_Bi-directional chain relays_ can also be used if both chains support smart contracts and thus, locking/minting of assets can be handled through these contracts.\n\n**Proof of Burn**: used for uni-directional flow as asset $x$ is burned on chain $X$.\n\n#### Verify\n\n- Chain relay contracts\n- Consensus committees to sign to verify pre-commit step.\n\n#### Abort\n\nMigration protocol doesn’t have explicit abort phase.\n\n## CCC Challenges\n\n### Heterogeneous Models and Parameters Across Chains\n\n- Different parameters used by different chains\n- security models\n- consensus differences: consensus execution, finality\n\n### Cryptographic Primitives\n\ndifferent cryptographic algorithms for hash locks or signatures\n\nZK proofs may provide a workaround but increases complexity, communication costs.\n\n### Collateralization and Exchange Rates\n\nUsing collaterals to prevent malicious behaviour among custodians or TTPs, incentivising correct behaviour but different types and rates of collateral b/w different chains.\n\nDynamic Collateralization based on exchange rates among different blockchains\n\n#### Lack of Formal Security Analysis\n\n- Replay Attacks on state verification: if proofs are submitted multiple times either on the same chain or on different chains can lead to multiple spendings of assets.\n- Data availability: timely requirements of proofs and data, if not reached in time, leads to incorrect behaviour of process.\n\nNeed more research on this topic as current solution increases complexity and decreases efficiency.\n\n#### Lack of Formal Privacy Analysis\n\ndidn’t understand perfectly\n\n### Upcoming Research\n\n- Interoperability chains: Cosmos and polkadot Layer 0 based ecosystems.\n- Light Clients: for better verification\n- Off-Chain Protocols\n  - Communication across off-chain channels\n  - Communication b/w on-chain and off-chain networks\n","lastmodified":"2023-07-24T05:57:52.800601582Z","tags":null},"/posts/eth-basics":{"title":"Ethereum Yellow Paper","content":"\nTransaction based state-machine\n\nstate of the blockchain changes after executing transactions.\n\nstores states in ***state root***\n\n# Accounts\n\n20-byte hex addresses whose state is stored on the blockchain\n\nTwo types of accounts:\n\n1. EOA: externally owned account\n2. Contract Accounts\n\n# Each Accounts Has Four Parts:\n\n1. Nonce: for EOA, number of transactions sent from the address. For Contracts, number of contracts creations.\n2. Balance: amount of ether owned by the account.\n3. storageRoot: Merkle-Patricia tree that stores data related to the account. Stored in the top-level state root tree.\n4. codeHash: For EOA, hash of empty string. For contracts, hash of the init code.\n\n# Transactions\n\nPiece of data signed by an external actor.\n\nTwo types of txs:\n\n1. Transaction which result in message calls\nMessage Calls: Done by contract account, when executing `CALL` opcode.\n2. Contract creation\n\nFields in a tx:\n\n1. nonce\n2. gasLimit\n3. gasPrice\n4. to\n5. value\n6. v, r, s: signature identifying sender\n7. init: in case of contract creating tx, returns code of the contract without constructor\n8. data: in case of message call tx, data being passed in call\n\n# Blocks\n\naggregate transactions and include in the blockchain\n\nContains:\n\n1. parentHash\n2. ommerHash\n3. beneficiary\n4. stateRoot\n5. transactionsRoot\n6. receiptsRoot\n7. timestamp\n8. number\n9. difficulty\n10. gasLimit\n11. gasUsed\n12. extraData\n13. mixHash\n14. nonce\n\n# GHOST\n\n`GHOST` protocol is used by ethereum to prevent mining centralization and enhance protocol security.\n\nLongest chain isn’t just the chain with more blocks as ancestor, but it also includes other stale(uncle) blocks in the calculation.\n\nUncle blocks are child of the ancestor of the block and not directly related to the block.\n\n# RLP\n\nIt is the data encoding used by the protocol to store data in tries.\nThere are several rules for encoding mentioned below.\n\n[Data structure in Ethereum | Episode 1: Recursive Length Prefix (RLP) Encoding/Decoding.](https://medium.com/coinmonks/data-structure-in-ethereum-episode-1-recursive-length-prefix-rlp-encoding-decoding-d1016832f919)\n\n# HP Encoding\n\nThis encoding is used for trie paths.\n\n[Data structure in Ethereum | Episode 1+: Compact (Hex-prefix) encoding.](https://medium.com/coinmonks/data-structure-in-ethereum-episode-1-compact-hex-prefix-encoding-12558ae02791)\n\n[Ethereum: Tutorials - LayerX Research](https://scrapbox.io/layerx/Ethereum:_Tutorials)\n\nPatricia tree\n\n[patricia-tree](https://eth.wiki/fundamentals/patricia-tree)\n[Understanding trie databases](https://medium.com/shyft-network-media/understanding-trie-databases-in-ethereum-9f03d2c3325d)\n","lastmodified":"2023-07-24T05:57:52.904603418Z","tags":null},"/posts/eth-consensus":{"title":"ETH Consensus","content":"\nELI5 understanding of ETH 2.0 specs\n\n**Safety**: guarantees that something bad never happen. Examples: Tendermint from Cosmos that uses BFT style consensus.\n\n**Liveness**: something good eventually occurs. Example: POW, Casper used by Ethereum.\n\n# **Why PoS**\n\nAn individual autonomy should always be greater than the power of any state. Cryptography solved this issue, by using ECC, individuals can now have a pair of keys that only he can access and thus, has the power to defend even state-level attacks.\n\nConsensus in blockchain is what drives the value of the network, and from where the network derives its value. No attacker should have incentives to attack the network for his own gain. PoW style networks are based on rewards where a consensus participant has almost nothing to lose in case the network is attacked. PoS solves this problem by allocating stakes to network particpants and imposing penalties in case of any malicious actors in the system. The ratio of rewards v/s penalties determines the incentives of consensus participants to behave honestly as penalties are directly proportional to number of wrong validations, if more validators behave maliciously, then slashing is higher.\n\n# Different Types of PoS\n\n- Chain-Based PoS: pseudo-random validator assigned at each time slot to create new block behind a previous block\n- BFT-style PoS: (partially synchronous) randomly assigned validator proposes a block and canonical chain is assigned using a voting process on which each validator votes for the valid chain\n\n# **Proof Of Stake**\n\n- Stake ETH to become validator\n  - For each 32 ETH staked, a validator is activated. Anyone can stake any number of ETHs and activate and control validators and execute **validator clients**.\n\n  \u003e Validator clients has the functionality of following and reading the Beacon Chain. A validator client can make calls into the Beacon nodes.\n\n- Validator, responsible for adding blocks to the chain by verifying txs and policing blocks being added by other validators\n  - Earns by successfully adding blocks\n  - Staked eths slashed if illegal txs added\n- Save Energy as selected randomly and not competing\n- No need to mine, just validate blocks known as **Attesting**\n\n# **Phases**\n\n- Beacon Chain\n- Sharding\n- Execution\n\n# **Sharding**\n\n- Scaling nodes horizontally\n- At least 128 Validators randomly assigned to a shard where a new block will be added in each slot of an epoch i.e., after 32 slots\n- ETH plans for 64 shards\n\n# **Beacon Chain**\n\n- Main functioning body in the blockchain, managing all the shards and the validators\n- Functions as heart of the chain\n- Creates **committee**, which are the validators used to validate a blockchain on which everyone verifies, stores and downloads\n\n# **Slots \u0026 Epochs**\n\n- Chain is divided into **slots** and **epochs**\n- **Slot** → timeframe to propose and validate a new block\n  - In a time period pre-determined in a blockchain, 12 seconds in case of Ethereum, all shard blocks are added into the blockchain\n  - Slots can be empty in case a validator fails to **propose** the block or the committee fails to attest\n  - Genesis blocks added to both Beacon chain and shards at block 0\n- **Epochs** → 32 slots comprises an epoch. \u003e 12 sec * 32 slots = 6.4 mins\n\n# **Crosslinks**\n\n- reference to shard blocks\n- basically the proof that a shard is valid\n- created at beacon chain for every successful block proposed by a shard\n- only after a crosslink does a validator get its reward\n\n# **Committee**\n\n- **Beacon chain** gets its name from the random numbers that it emits to the public\n- uses RANDAO process to randomly select a group of validators for an epoch to a shard to attest transactions in a block\n- crosslinks are made after attestation from validators of that slot\n- Every epoch, validators randomnly assigned to a slot which is then subdivided into committees.\n- Each committee is assigned a particular shard and they attempt to crosslink a shard block to Beacon chain head in order to gain rewards.\n- 64 shards, each assigned 128 validators per committee -\u003e Thus, has atleast 8192 validators\n\n## **Beacon Chain Checkpoints**\n\n![Beacon Chain Checkpoints](posts/assets/Beacon-Chain-Checkpoints.jpeg)\n\nA `checkpoint` or an `epoch boundary` is the first block in an epoch. If no such block, it is the most recent preceding block.\n\n## Votes\n\n- **`LMD GHOST`**: validator vote for beacon chain head, i.e. what they believe beacon chain head is.\n- **`Casper FFG`**: When casting LMD GHOST vote, validators also vote for checkpoint in current epoch, called `target`. This also includes previous epoch checkpoint called `source`.\n\n# Finality\n\n\u003e `Supermajority vote`: which is made by 2/3 of the total **balance** of all validators.\n\n![image](posts/assets/Beacon-Chain-Justification-and-Finalization.png)\n\nWhen an epoch checkpoint gets supermajority vote, it is said to be *justified*.\nAn epoch is *finalised* when it is justified and next epoch checkpoint gets justified.\nWhen a epoch checkpoint, i.e. slot gets finalised, all preceding blocks also gets finalised.\n\n- Finality is important as it gives gurantees to shards and ethereum parties regarding transactions.\n- Reduces complexity with cross-shard communications.\n\n# FLP Impossibility\n\n![image](posts/assets/FLP-Impossibility.png)\n\nIn a distributed system, it is not possible to simultaneously have safetly, liveness and full asynchrony unless some unreasonable assumptions are made.\n\nEthereum uses both `LMD-GHOST` and `Casper FFG` as its protocol to justify and finalise blocks.\n\n**LMD-GHOST** preferes liveness over safety in the form that validator can attest to a chain head and keep producing blocks while **Casper** prefers safety over liveness such that a block is finalised only when it is justified and in a later epoch, majority of validators attest it again to finalise it. Once, a block is finalised, it is added forever in the chain.\n\nDue to **LMD-GHOST** prefering liveness over safety, there is a chance of reorgs. Capser FFG helps here as it prefers safety and decisions made under Casper is considered final. It has phases, in which nodes indicate they'd like to agree on something(justification), then agree that they've seen each other agreeing(finalisation).\n\n# **Questions**\n\n1. MEV in PoS\n2. Reorgs\n\n# **Checkpoints**\n\n1. [https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/](https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/)\n2. [eth2book.info](https://eth2book.info/altair/contents)\n3. [https://github.com/ethereum/annotated-spec/blob/master/phase1/beacon-chain.md#introduction](https://github.com/ethereum/annotated-spec/blob/master/phase1/beacon-chain.md#introduction)\n4. [https://ethos.dev/beacon-chain/](https://ethos.dev/beacon-chain/)\n5. [https://ethresear.ch/t/two-ways-to-do-cross-links/2074](https://ethresear.ch/t/two-ways-to-do-cross-links/2074)\n6. [https://vitalik.ca/general/2017/12/31/pos_faq.html](https://vitalik.ca/general/2017/12/31/pos_faq.html)\n7. [https://medium.com/codechain/safety-and-liveness-blockchain-in-the-point-of-view-of-flp-impossibility-182e33927ce6](https://medium.com/codechain/safety-and-liveness-blockchain-in-the-point-of-view-of-flp-impossibility-182e33927ce6)\n8. [https://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51](https://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51)\n9. [Ben Eddington blog](https://hackmd.io/@benjaminion/eth2_news/https%3A%2F%2Fhackmd.io%2F%40benjaminion%2Fwnie2_220311)\n10. [eth2book](https://eth2.incessant.ink/book/03__eth1/07__clients.html)\n11. \u003chttps://arxiv.org/pdf/2203.01315.pdf\u003e\n12. [LMD GHOST and Casper FFG](https://blog.ethereum.org/2020/02/12/validated-staking-on-eth2-2-two-ghosts-in-a-trench-coat/)\n13. [https://blog.ethereum.org/2019/12/30/eth1x-files-state-of-stateless-ethereum/](https://blog.ethereum.org/2019/12/30/eth1x-files-state-of-stateless-ethereum/)\n14. [Understanding validator effective balance](https://www.attestant.io/posts/understanding-validator-effective-balancehttps://www.attestant.io/posts/understanding-validator-effective-balance/)\n15. [0xfoobar's Proof of Stake](https://0xfoobar.substack.com/p/ethereum-proof-of-stake)\n","lastmodified":"2023-07-24T05:57:52.904603418Z","tags":null},"/thoughts/dag-based-consensus":{"title":"DAG-based consensus","content":"\nBefore understanding what even a dag based protocol does. First, we need to understand how it's different than a traditional blockchain protocol like Bitcoin. A blockchain is a distributed ledger technology that can record and store data in a public, immutable, distributed chain. Participants can send transactions to the protocol that is recorded and confirmed as valid by global network nodes.\n\nIn a standard blockchain, only one single sequence of blocks is considered as canonical chain. Forks occurring in the protocol is identified as security threat as basically if a fork becomes too large, then it gives the malicious party more power in the network and affects the main chain of blocks. Validators/miners keep doing their work by batching the transactions in a **block** using a consensus rule and creating a block with it, which is then relayed to the entire network.\n\nDAG protocols comes with a new idea, i.e. why not make every fork count. Why can't a block have multiple parents? It is just a *directed acyclic graph* where a transaction added to the graph can have *multiple parents* and *multiple children*.\n\n\u003e The idea of DAG-based BFT consensus (e.g., [HashGraph](https://eclass.upatras.gr/modules/document/file.php/CEID1175/Pool-of-Research-Papers%5B0%5D/31.HASH-GRAPH.pdf) and [Aleph](https://arxiv.org/abs/1908.05156)) is to separate the network communication layer from the consensus logic.\n\n# Terminology\n\n- **Parties**: A party $p$ is the one producing the blocks.\n- **Nodes**: A block is represented as a node in the DAG.\n- **Edges**: An edge is when a block $u$ points **directly** to another block $v$. We say, $u$ *acknowledges* $v$.\n- **Observe**: Transitive closure of acknowledge. Block $u$ observes $v$, if there is a directed path from $u$ to $v$.\n\n# Simple Payment Channel\n\n- N permissioned (blocks can include own set of transactions) parties produce blocks and points to all existing leaves.\n- works in asynchronous settings, i.e. Messages take finite amount of time to arrive.\n- Block $u$ approves $v$ produced by $p$, if it observes $v$ and doesn't observe any $p$-block that equivocates with $v$.\n- $p'$ approves $v$ if some $p'$ block approves $v$.\n- Honest party $p'$ can't approve two equivocating blocks produced by $p$.\n- Block $v$ is approved by *supermajority* if it is approved by $n-f$ parties, if $n\u003e3f$, then *safety* is achieved as two equivocating blocks produced by $p can't be approved by a supermajority (due to quorum intersection).\n- Liveness is trivial as honest parties will keep producing blocks\n\n# Cordial Miners\n\n![dag-1](thoughts/images/dag_1.png)\n\n- Give more structure to DAG.\n- have notion of *rounds*, with each honest party producing one block in each *round*.\n- an honest party produces block in round $r+1$, as soon as it sees $n-f$ blocks in round $r$. The block produced in $r+1$ must point to those $n-f$ blocks in round $r$ and also any leaves in previous rounds.\n- Have leader blocks, and $k$ consecutive rounds are called as *wave* (for cordial-miners $k=5$). First round of each wave, select the leader. Block proposed by the leader at the first round will be called *Leader Block*.\n- Now, to get total ordering we need to extend the DAG, we suppose a function $\\tau$ which extends a partially ordered DAG to a totally ordered. Although only total ordering is not sufficient as we need to satisfy the SMR properties of safety and liveness.\n\n\u003e Notation: For a block $u$, $[u]$ denotes the initial segment of the DAG defined by $u$.\n\n![total-ordering](thoughts/images/total_ordering.png)\n\nNow, the idea is to use $\\tau$ to arrange the leader blocks in some way such that total ordering is achieved. We define total ordering to be:\n\n$$\\tau([u_{1}]) * \\tau([u_{2}-u_{1}]) * \\ldots$$\n\nBut we can't use any leader blocks, there has to some concept of finality so that only final blocks are included in the total ordering.\n\n## Defining Finality\n\n**Supermajority** of blocks means set of blocks produced by a supermajority of miners. A block $v$ is *ratified* by block $u$ if $[u]$ includes a supermajority of blocks approving $v$. Can be thought of as seeing stage-1 QC for $v$, thus producing stage-2 vote in tendermint sense.\n\nNow a leader block of wave $w$ is **final** if it is ratified by a supermajority of blocks in the final round of wave $w$. This is like $v$ receiving stage-2 QC vote in tendermint.\n\nNow, define total ordering as: If dag $D$ has no final leader, then $ord(D) := \\phi$ else, let $u$ be the last final leader, then $ord(D) := \\tau'(u)$, where $\\tau$ is defined recursively:\n\n$$\\tau'(u) := \\tau([u]) or \\tau'([u'])*\\tau([u]-[u'])$$\n\nSince, we've defined our total ordering. All left now is to extract safety and liveness from total ordering.\n\n## Proving Safety\n\n$Lemma.$ If a leader block is *final*, then it is ratified by every subsequent leader block.\n\n$Claim.$ If $D_1$, $D_2$ are DAGs held by honest parties $p_1$, $p_2$, then $ord(D_1)$ and $ord(D_2)$ are consistent.\n\nLet's assume the case where $D_{1} \\subset D_{1} \\cup D_{2}$ and $D_{2} \\subset D_{1} \\cup D_{2}$, then $ord(D_{1})$ and $ord(D_{2})$ must be consistent with $ord(D_{1} \\cup D_{2})$. Let's suffice to deal with the case $D_{1} \\subset D_{2}$.\n\nNow, if $D_{1}$ has no final leader block, then as per definition of $ord$, $ord(D_{1)}=\\phi$. If $u$ is the final leader block then $u$ must be ratified by any subsequent leader block in $D_2$.\n\n## Proving Liveness\n\n$Lemma.$ Existence of inifinitely many *honest* final leader blocks.\n\nInstead of announcing leader at the start of the round as then the leader can be delayed by the adversary, we let the parties produce the blocks and form DAG, then in the last round leader is selected randomly. This makes a scenario where all of the blocks in the last round ratify supermajority of the blocks from the first round and thus selecting a leader randomly now gives $\u003e2/3$ chance of selecting a final leader block for the wave.\n\n## DAG-Rider\n\n### Reliable Broadcast\n\nSame as Byzantine broadcast but due to asynchronous setting, relax the termination condition.\n\n- Designate a broadcaster which has input $v \\in V$\n- Agreement: If an honest party terminates, then all must terminate with the same output in $V$.\n- Validity: If broadcaster is honest, then all honest parties must terminate giving the broadcaster's input as their output.\n\n#### Bracha's Broadcast\n\nSimple way to model reliable broadcast. Works in asynchronous setting.\n\n- broadcaster start with input $v$, sends value to all\n- Three circumstances in which any party $p$ will speak:\n\t- $p$ receives a first value $v$ from the broadcaster, they send $(echo,v)$ to all parties.\n\t- $p$ receives $n-f (echo, v)$ messages from the parties and if $p$ hasn't voted, then $p$ votes for $v$ by sending $(vote, v)$ to all parties.\n\t- $p$ receives $(vote, v)$ from $f+1$ distinct parties and $p$ hasn't voted, then $p$ votes by sending  $(vote, v)$ to all parties.\n\n---\n\nSimilar to cordial miners but do not let equivocating blocks get added to the DAG. This is done using Reliable broadcast.\n\n- reduces number of round in each wave by 1.\n- But cost is to reliably broadcast the block which increases latency.\n- efficient version of reliable broadcast reduces amortized time complexity per transaction $O(n)$ for DAG-Rider.\n\n## Narwhal\u0026Tusk\n\nDecouples data dissemination from metadata ordering. [Narwhal\u0026Tusk](https://arxiv.org/abs/2105.11827) paper introduced a highly scalable and efficient DAG construction method using Narwhal and total ordering of DAG with BFT properties using Tusk/Bullshark.\n\n### Narwhal\n\n\u003e Narwhal off-loads reliable transaction dissemination to the mempool protocol.\n\nNarwhal main task is to create a DAG using $N$ nodes in the network with upto $f\u003cN/3$ byzantine nodes. Since, communication happens in an asynchronous network, Narwhal alone is not sufficient to guarantee liveness property and thus, Tusk/Bullshark was introduced that totally orders the DAG and satisifies BFT properties. DAG formation happens in a round-based structure.\n\nDesign Goals for Narwhal:\n\n- Reduce need for double transmissions when leaders propose blocks\n- enabling scaling out when more resources are available\n\n#### Block Structure in Narwhal:\n\n![block_structure](thoughts/images/narwhal_block_structure.webp)\n\nA block $b$ consists of:\n\n- Set of transactions\n- $n-f$ block certificates from previous rounds\n- signature of $i$\n\nThe certificates encode a causal *'happened-before'* relation between the blocks denoted as $b \\rightarrow b'$\n\n#### Validators\n\n- Each validator consists of set of workers and a primary.\n- workers collect transactions from the clients and broadcast batches of data to workers.\n- Upon receiving $2f+1$ acks, workers forwards a digest of batches to primary.\n- primaries form a round-based DAG on metadata (vertices contain digests).\n- Thus, data dissemination is done by the workers at network speed regarding the metadata DAG construction by primaries.\n\nValidators use following reliable broadcast protocol:\n\n- each validator sends metadata (digest of batches and $n-f$ references to previous rounds' blocks) to all other validators.\n- each receiver replies with a signature if:\n\t- workers sign a PoA so that the data corresponding to the digests is made available\n\t- it has not replied to this validator in the round before (for non-equivocation)\n- sender sends a quorum certificate after getting $n-f$ such signatures.\n- validator advances to next round if it has received $n-f$ certificates from previous round.\n\nQuorum Certificates has following advantages:\n\n- Non-equivocation: honest validators only sign one vertex per validator per round, and $n-f$ signatures are required to create a QC. Thus, byzantine validator won't be able to create two different QC for different blocks due to quorum intersection.\n- Data availability: validators sign only if they locally store the data corresponding to the digests in the vertex.\n- history availability: a certificate of a block guarantees data availability, and certified blocks contain certificates from previous rounds. Thus, a validator store entire causal history of the block. A new validator joining can just learn about the DAG using certificates from previous rounds.\n\nThus, Narwhal satisfies following properties:\n\n- Integrity: For any digest $d$, two different invocations of $read(d)$ to honest parties will return the same value.\n\n  This happens because honest parties can only reply with a signature when they've received the data corresponding to the digest.\n\n- Block-availability: If a $read(d)$ is invoked by an honest party after $write(d,b)$ succeeds for an honest party, then $read(d)$ eventually completes and returns $b$.\n\n  $write(d,b)$ succeeds when sender has received $n-f$ certificates from other validators which signifies that they've received the data and will persist it. Thus, if a leader proposes a certificate, this means that the block will be available for later purposes.\n\n- 2/3-Causality: successful $read\\_causal(d)$ returns a set $B$ that contains at least $2/3$ of the blocks written successfully before $write(d,b)$ succeeded.\n\n  To successfully write a block, a sender requires at least $n-f$ certificates from previous round signifying that the current proposed block references those causally *'happened-before'* blocks. Thus, at least 2/3 of the blocks are included in the set.\n\n- 1/2-Chain quality: At least half of the blocks in set $B$ returned by $read\\_causal(d)$ are written by honest parties.\n\n  Out of the $n-f$ certificates from previous round, at least half has to be honest due to assumption that $f\u003cn/3$.\n\n- Containment: let $B$ be the set returned by $read\\_causal(d)$, then for every $b' \\in B$, the set $B'$ returned by $read\\_causal(d')$, $B' \\subseteq B$.\n\n### Using Narwhal for Consensus\n\nNarwhal as a high-throughput mempool can be combined with an asynchronous or eventually-synchronous protocol to achieve total-ordering. This is advantageous in many sense:\n\n- Same causal history of a block given a certificate. Any deterministic rule then can be used for total ordering\n- Bulk transaction information is evenly shared among all validators and doesn't lead to uneven utilisation of resources\n- continues to work even in asynchronous environments, i.e. validators keep sending blocks and certificates.\n\n### Tusk\n\nLike all protocols, Narwhal is also prone to impossibility result in asynchrony and thus, to retain liveness during asynchronous phase, Tusk was proposed in the Narwhal\u0026Tusk paper which converts the causally ordered DAG to a totally ordered with zero extra communication.\n\n- Has a concept of waves which comprises of 3 rounds. Propose, vote, and produce randomness to elect a leader\n\n### Bullshark\n\n## Resources\n\n- [a16z crypto research - DAG based consensus protocols](https://www.youtube.com/watch?v=v7h2rXNtrV0)\n- [DAG meets BFT - decentralizedthoughts](https://decentralizedthoughts.github.io/2022-06-28-DAG-meets-BFT/)\n- [DAG based BFT](https://malkhi.com/posts/2022/07/dag-fo/)\n- [BFT on a DAG](https://blog.chain.link/bft-on-a-dag/)\n- [SoK: Diving into DAG-based Blockchain Systems](https://arxiv.org/pdf/2012.06128.pdf)\n- [All You Need is DAG](https://arxiv.org/pdf/2102.08325.pdf)\n- [Bullshark: DAG BFT protocols made Practical](https://arxiv.org/pdf/2201.05677.pdf)\n- [Scaling Celo blockchain with Narwhal](https://www.youtube.com/watch?v=XP41IsXCUrw)\n- [DAG vs Blockchain](https://hedera.com/learning/distributed-ledger-technologies/dag-vs-blockchain)\n- [MEV Protection on a DAG](https://arxiv.org/pdf/2208.00940.pdf)\n- [MEV mitigation using modular DAG-based mempools](https://docs.google.com/presentation/d/12xWSvocGeIrRiJ4otSP2OWvFDNshd1x8buArux0BzG8/edit#slide=id.p)\n- [Sreeram Kannan's thread on Bullshark](https://twitter.com/sreeramkannan/status/1555635155796049920)\n- [samuel's thread](https://twitter.com/samlafer/status/1566285789515771904)\n","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/distributed-consensus":{"title":"Distributed Consensus","content":"\n# QBFT\n\n- [](https://consensys.net/docs/goquorum/en/latest/configure-and-manage/configure/consensus-protocols/qbft/)\n\n# PBFT\n\n# Hotstuff\n\n- Partial synchronous\n- Leader-based\n\n# Resources\n","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/divisors":{"title":"Divisors","content":"\nDefinition of divisor of a rational function $z \\in k(C)$:\n\n![divisor definition](thoughts/images/divisors.png)\n\nDegree of a divisor as $deg \\space D = \\sum_{P \\in \\mathbb{C^*}} n_p$\n\n# Divisor of a Meromorphic Function\n\nFor a [[meromorphic-functions|meromorphic function]] $f(z)$, we can define the **divisor** of $f$ as,\n\n$$div \\space f = \\sum_{P \\in \\mathbb{C^*}} (ord_{P} f)(P)$$\n\nIt is also known that for any non-zero meromorphic function $f \\in K(\\mathbb{C^*})$, then $deg \\space div \\space f = 0$.\n\n## Example\n\nLet the polynomial be $f(z) = z^2 + 1$. We have a zero at $i$ and $-i$. But since the domain is $\\mathbb{C^*}$, we need to consider point at infinity. In this case, we have a pole at infinity with degree 2 as $f(1/z) = 0$ at $z = 0$.\n\nSo, the divisor of f is,\n\n$$div \\space f = (i) + (-i) - 2(\\infty)$$\n\n# Divisors of Elliptic Curves\n\nIn the case of Riemann sphere, meromorphic functions are considered. In the case of Elliptic curves, rational functions are considered.\n\n## Prerequisites\n1. Riemann Sphere: Denoted as $\\mathbb{C^*}$ and contains $C \\bigcup \\lbrace \\infty \\rbrace$\n\n# Resources\n- [Examples of divisor of a function](https://math.stackexchange.com/questions/1290619/example-of-a-divisor-of-a-function)\n- [Divisor of a line function on EC](https://math.stackexchange.com/questions/2434900/let-f-be-the-divisor-of-a-function-on-an-elliptic-curve-why-does-degf)\n- [Divisors and Pairings](https://klwu.co/knowledge/ec-basics-3-divisors/)\n- [Riemann Sphere](https://mathimages.swarthmore.edu/index.php/Riemann_Sphere)\n","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/elliptic-curves":{"title":"Theory of Elliptic Curves","content":"\nOnce again, this is just another of my notes aggregated from various sources. This is a very dumbed down version for just my understanding and it's my advice to follow the resources attached as they are more thorough and detailed explained by experts on the topic. I'm just a novice who is interested in learning cryptography.\n\n\u003e *Walking side by side with death*\n\u003e\n\u003e *The devil mocks their every step*\n\u003e\n\u003e *The snow drives back the foot that's slow*\n\u003e\n\u003e *The dogs of doom are howling more*\n\nNow, for the song. This time we'll listen [No Quarter](https://open.spotify.com/track/55ZL7fjGAWfClmpnsK6Xon?si=2a10756c4a094826) by Led Zeppelin. No description for this. I guess they don't need one :)\n\n# Discrete Logarithm Problem\n\n[Discrete logarithm Problem](https://river.com/learn/terms/d/discrete-log-problem-dlp/) (DLP) for a group $G$ and an element $g \\in G$ is,\n\n\u003e Given an element $h$ in the subgroup generated by $g$, find an integer $m$ satisfying $h = g^m$.\n\nThis can be computed using $m = \\log_g(h)$. DLP for some groups is said to be very easy and for some difficult which allowed cryptographers to experiment with it and invent some amazing primitives in cryptography.\n\nDLP is easy for:\n\n- $\\frac{\\mathbb{Z}}{m\\mathbb{Z}}$ under addition\n- $\\mathbb{R}^{*}$ or $\\mathbb{C}^{*}$ under multiplication\n\nAnd for some groups, it is difficult but not computably infeasible:\n\n- $\\mathbb{F}_p^*$ under multiplication: said to be subexponential\n\n# Elliptic Curves\n\n$$\n\\begin{array}{rcl}\n  \\left\\{(x, y) \\in \\mathbb{R}^2 \\right. \u0026 \\left. | \\right. \u0026 \\left. y^2 = x^3 + ax + b, \\right. \\\\\n  \u0026 \u0026 \\left. 4a^3 + 27b^2 \\ne 0\\right\\}\\ \\cup\\ \\left\\{0\\right\\}\n\\end{array}\n$$\n\n- Can be defined on any fields, such as $\\mathbb{F}_{p}, \\mathbb{Q}, \\mathbb{R}$.\n- EC defined on $\\mathbb{F}_{p}$ are finite groups.\n- ECDLP is discrete logarithm problem for the EC defined on finite field which has exponential time complexity to solve.\n- Best known algorithm for an EC defined over $\\mathbb{F}_p$ takes $O(\\sqrt{p})$.\n\n## Group Law of Elliptic Curves\n\n- Elements of the group are points on elliptic curve.\n- identity element is the point at infinity $O$\n- inverse of point $P$ is symmetric about $x$-axis\n- addition rule: given three aligned, non-zero points $P,Q,R$ on EC, $P+Q+R=0$\n\n## Elliptic Curves in $\\mathbb{F}_{p}$\n\n$$\\begin{array}{rcl}\n  \\left\\{(x, y) \\in (\\mathbb{F}_p)^2 \\right. \u0026 \\left. | \\right. \u0026 \\left. y^2 \\equiv x^3 + ax + b \\pmod{p}, \\right. \\\\\n  \u0026 \u0026 \\left. 4a^3 + 27b^2 \\not\\equiv 0 \\pmod{p}\\right\\}\\ \\cup\\ \\left\\{0\\right\\}\n\\end{array}$$\n\nEvery EC has an order $N$ which represents the number of points on the curve. This can be explained using group theory. Let's say we have a group, and the order of group denotes the number of points on the group. Now, if we take a point on the curve, then it tends to repeat itself in a cycle after some points.\n\nFor Example: Let's take a curve $y^{2} \\equiv x^{3}+2x+3 \\pmod{p}$ and the point $P = (3,6)$. The multiples of P are just 5 distinct point on the curve $(0, P, 2P, 3P, 4P)$ and they are just repeating themselves.\n\nThis makes set of the multiples of $P$ a **cyclic subgroup** of the group formed by the elliptic curve. The point $P$ is called the **generator** or **base point** of the cyclic subgroup. Finding order of the subgroup is done by finding smallest $n$ such that $nP=0$.\n\n## Elliptic Curve Cryptography\n\n$$ y^2 = x^3 + ax^2 + bx + c $$\n\n[$secp256k1$](\u003chttps://river.com/learn/terms/s/secp256k1/):\u003e used by Bitcoin and Ethereum to implement public key cryptography. Elliptic curve over a field $z_p$ where $p$ is a 256-bit prime.\n\nECDSA: Elliptic Curve Digital Signature Algorithm\n\nPublic key cryptography uses this method to calculate public keys which is a point on ECC curve.\n\n$$ K = (k * G) \\% p $$\n\n- $K$ = 512-bit public key\n- $k$ = 256-bit randomly generated private key\n- $G$ = base point on the curve\n- $p$ = prime number\n\nTake a [base point](\u003chttps://medium.com/asecuritysite-when-bob-met-alice/picking-a-base-point-in-ecc-8d7b852b88a6)\u003e $G$, add it $n$ (private key) times to make $nG (\\% p)$ (public key).\n\n\u003e **Note**: addition here means addition in elliptic curve and not addition in field of integers mod p.\n\n[Order](\u003chttps://medium.com/asecuritysite-when-bob-met-alice/whats-the-order-in-ecc-ac8a8d5439e8)\u003e of a base point is when keys generated using this point starts to form a cycle. Max number of points on the curve.\n\nThus, choosing a good base point is necessary in any public key generation curves.\n\nsecp256k1:\n\n```other\nx = 0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798\ny = 0x483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8\np = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F\n```\n\nThe order is:\n\n```other\nN = FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141\n```\n\nEthereum public keys are serialisation of 130 hex characters\n\n```other\n04 + x-coord (64) + y-coord (64)\n```\n\n\u003e 04 is prefixed as it is used to define uncompressed point on the ECC.\n\nEthereum addresses are hexadecimal numbers, identifiers derived from the last 20 bytes of the Keccak256 hash of the public key.\n\n## EIP55\n\nMixed capitalisation of letters in the address\n\nTake keccak256 hash of the address, capitalise character if hex digit of hash is greater than `8`.\n\n### Why Discrete Logarithm?\n\nECC is significant because solving $k * G$ is trivial but obtaining $k$ from product $k * G$ is not.\n\n$k*G$ can be obtained using Fast-Exponentiation algorithm but solving for $k$ requires computing discrete logarithms.\n\n## Security\n\nBig-O Notation of discrete logarithm problem is $O(\\sqrt{n})$.\n\nBase point $G$, is chosen to be closer to $2^{256}$ and thus is in the order of `256`.\n\nSo, $\\sqrt{256} = 128$ bits level of security is provided by curves like $secp256k1$.\n\n## [secp256k1 v/s secp256r1](\u003chttps://dappworks.com/why-did-satoshi-decide-to-use-secp256k1-instead-of-secp256r1/)\u003e\n\n[$secp256k1$](\u003chttps://www.johndcook.com/blog/2018/08/21/a-tale-of-two-elliptic-curves/)\u003e is a Koblitz curve defined in a characteristic 2 finite field while $secp256r1$ is a prime field curve.\n\nNot going into details as to what a characteristic 2 finite field is, we can specify $secp256r1$ as a pseudo-randomised curve and $secp256k1$ as completely random curve which can’t be solved using discrete logarithm problem **yet**.\n\n## Pairing Friendly Curves\n\nPairing-based cryptography is a new cryptographic primitive that has been developed in recent times, enabling new applications like short digital signatures that are aggregatable, identity-based cryptography, MPC, efficient [[polynomial-commitments|polynomial commitments]].\n\nThey have a favourable embedding degree, and a large prime-order subgroup.\n\n### BLS12-381\n\n\u003e Note: Most of these notes are taken from this amazing [post](\u003chttps://hackmd.io/@benjaminion/bls12-381)\u003e by Ben\n\nThis was first introduced by Sean Bowe from ZCash in 2017 and has been used in various things from then. Now for the naming,\n- BLS stands for Barreto, Lynn, Scott.\n- 12 stands for embedding degree of the curve\n- 381 stands for the order of the prime used for the field $2^{381}$, i.e. number of bits used to represent coordinates on the curve. Why 381? because 48 bytes per field element and remaining 3 bytes for flags or arithmetic optimisations.\n\nEquation: $y^{2}=x^{3}+4$\n\nKey parameters of the curve are derived from a single parameter $\\tt x$ = -0xd201000000010000.\n\n- Field Modulus -\u003e $q: \\frac{1}{3}({\\tt x}-1)^2({\\tt x}^4-{\\tt x}^2+1)+{\\tt x}$\n- Subgroup Size or the number of points on the curve -\u003e $r: ({\\tt x}^4-{\\tt x}^2+1)$\n\n#### Field Extensions\n\nThe two curves used in BLS12-381 are defined on $\\mathbb{F}_{q}$ and $\\mathbb{F}_{q^{12}}$. This power raised is known as an extension field of field $\\mathbb{F}_q$.\n\nLet's construct $\\mathbb{F}_{q^{2}}$, quadratic extension of $\\mathbb{F}_q$.\n\nelement representation $\\mathbb{F}_{q^{2}}$: $a_{0}+a_{1}x$ also written as $(a_{0},a_1)$.\n- addition: $(a,b)+(c,d)=(a+c,b+d)$\n- multiplication: $(ac-bd, ad+bc)$\n\nUnderstanding multiplication is a tricky thing and it takes time to get accustomed to field arithmetic. Basically, the original multiplication has a $x^2$ term which doesn't make any sense in the $\\mathbb{F}_{q^2}$ field, so we need a mechanism to remove this term. This is where reduction of polynomial terms is used. Certain rules are used to reduce the polynomials having degree greater than 2. Rule in this case is $x^2+1=0$\n\nThere are only 2 rules about this rule:\n\n1. the polynomial we're reducing must be a $k$ degree polynomial, where $k$ is the extension degree.\n2. It must be irreducible in the field it is getting extended.\n\nBLS12-381 consists of two curves: $\\mathbb{G}_{1}$ and $\\mathbb{G}_{2}$. $\\mathbb{G}_{1}$ is a fairly simple curve defined over finite field $\\mathbb{F}_q$. We can call this curve $E(\\mathbb{F}_{q})$. Curve equation for $\\mathbb{G}_1$ is $y^2=x^3+4$.\n\nThe other curve is defined over an extension of $\\mathbb{F}_{q}$ to $\\mathbb{F}_{q^{12}}$. Since arithmetic on $\\mathbb{F}_{q^12}$ is fairly complex, it is reduced to $\\mathbb{F}_{q^2}$. So, we'll call this curve $E'(\\mathbb{F}_{q^2})$. Curve equation is slightly modified to be $y^2=x^3+4(1+i)$.\n\n#### Subgroups\n\nBLS12-381 is popular because it's a pairing friendly curve. For pairings, we need two points from **distinct** groups, each of order $r$. The first curve only has one subgroup of order $r$, and thus we can't just use that one curve. That's why we require a second different curve which has a distinct subgroup of same order. Fortunately, this is exhibited by the curve defined over the extension field and one of these groups only contains points having a trace of zero. This is where $k=12$, embedding degree comes in. Thus, we have group $G_1$ of order $r$ in $E(\\mathbb{F}_q)$, and a distinct group $G_2$ of same order in $E(\\mathbb{F}_{q^12})$. This enables pairings.\n\n\u003e Note: `Trace of Zero`\n\n#### Twists\n\nAs explained earlier, that arithmetic on field $\\mathbb{F}_{q^12}$ is fairly complex and inefficient. And all the curve operations like `add`, `sub`, `mul`, etc. requires a lot of arithmetic. Thus, we need to transform the $E(\\mathbb{F}_{q^12})$ curve into a curve defined over a lower degree field that still has an order $r$ subgroup. Why we didn't take this lower order field in the first place? Because we require subgroup having trace of zero points.\n\n\u003e Quoting section 3 of [this](\u003chttps://eprint.iacr.org/2005/133.pdf)\u003e :\n\u003e\n\u003e The basic idea for point compression is not only to restrict the first pairing argument to $E(\\mathbb{F}_{p})$, but also to take the second argument $Q \\in E(\\mathbb{F}_{p^{12}})$ as the image $\\psi(Q')$ of a point on a sextic twist $E'(\\mathbb{F}_{p^2})$, where $\\psi : E'(\\mathbb{F}_{p^2}) \\rightarrow E(\\mathbb{F}_{p^{12}})$ is an injective group homomorphism. This way one would work only with $E(\\mathbb{F}_{p})$ and $E'(\\mathbb{F}_{p^2})$ for non-pairing operations like key generation, and map from $E'(\\mathbb{F}_{p^2})$ to $E(\\mathbb{F}_{p^{12}})$ only when actually computing pairing values.\n\nBLS12-381 uses a `sextic twist`. This means reducing the extension field degree by a factor of `6`.  We find a $u$ such that $u^{6}=(1+i)^{-1}$, then we define a twisting transformation as $(x,y) \\rightarrow (\\frac{x}{u^2},\\frac{y}{u^3})$. This transforms the original curve from $E:y^2=x^3+4$ to $E':y^2=x^3+4/u^6$. $E$ and $E'$ looks different but actually are same objected with coefficients in different base fields.\n\nThis twist gives curve $E'$ that has a subgroup of order $r$ that maps to our $G_2$ group. So, we can work over more efficient $E'(\\mathbb{F}_{q^2})$ and map $G_2$ back to $E(\\mathbb{F}_{q^{12}})$, when required.\n\nNow, we have two groups:\n\n- $G_{1} \\subset E(F_q)$ where $E:y^2=x^3+4$\n- $G_{2} \\subset E'(F_{q^2})$ where $E':y^2=x^3+4(1+i)$\n\nSince, point in G_2 are complex numbers, it takes twice the amount of storage and are more expensive to perform arithmetic operations.\n\n#### Embedding Degree\n\nSmallest $k$ such that $q^{k} \\equiv 1 \\pmod{r}$. Embedding Degree is the smallest positive integer required to extend the field to satisfy conditions required for pairings.\n\n1. $F_{q^k}$ contains more than one subgroup of order $r$ for constructing $G_{2}$.\n2. $F_{q^k}$ contains all the $r^{th}$ roots of unity for constructing $G_{T}$.\n\nEmbedding Degree should be chosen such that it doesn't compromise security and efficiency. Basically, a higher embedding degree makes it harder to to solve DLP in $G_T$. But a higher embedding degree also make it harder to operations in higher field like $F_{q^{12}}$. Maximum available twist is degree six, so best we can do is reduce the field extension degree by six.\n\n### Cofactor\n\nIt's the ratio of order of the curve group and order of the subgroup $hr = n$. Usually, cofactor should be very small in order to avoid subgroup attacks on discrete logarithms. But in pairing-based cryptography, the cofactors of $G_1$, $G_2$ and $G_{T}$ can be very large.\n\nBy multiplying by the cofactor, a point on the curve is mapped to the appropriate group known as **cofactor clearing**.\n\n### Roots Of Unity\n\n[Roots of Unity](https://brilliant.org/wiki/roots-of-unity/) are complex solutions to the equation: $x^n=1$. Every nonzero element of a finite field is a root of unity, as $x^{q-1} = 1$ for every nonzero element of $\\mathbb{F}_{q}$.[^1]\n\n*Primitive root of unity* is when a number is solution to $x^n=1$ but not for $x^m=1$ for any positive integer $m\u003cn$. So, if $a$ is a $n$th primitive root of unity in a field $\\mathbb{F}$, then $\\mathbb{F}$ contains all the roots of unity, $1, a, a^{2}, \\ldots, a^{n-1}$.\n\nEffect of the pairing is to map a point from $G_1$ and $G_2$ onto an $r$th root of unity in $F_{q^{12}}$. These $r$th roots of unity form a subgroup in $F_{q^{12}}$ of order $r$, which is the group $G_T$.\n\n### Extension Towers\n\nFor BLS12-381, $F_{q^{12}}$ is constructed as a 2-3-2 extension tower, i.e. quadratic extension -\u003e cubic extension -\u003e quadratic extension.\n\n1. $F_{q^{2}}:F_{q}(u)/(u^2-\\beta)$ where $\\beta=-1$.\n   Point in $F_{q^{2}}$ looks like $a_0+a_1u$ where $a_{j} \\in F_{q}$.\n   Reduction rule is $u^{2}+1=0$ which is irreducible in $F_{q}$.\n2. $F_{q^{6}}:F_{q^{2}}(v)/v^3-\\xi$ where $\\xi=u+1$.\n   Point in $F_{q^{6}}$ looks like $b_0+b_{1}v+b_{2}v^{2}$ where $b_{j} \\in F_{q^{2}}$.\n   Reduction rule: $v^3-(u+1)=0$ which is irreducible in $F_{q^2}$.\n3. $F_{q^{12}}:F_{q^{6}}(w)/(w^2-\\gamma)$ where $\\gamma=v$.\n   Point in $F_{q^{12}}$ looks like $c_0+c_{1}w$ where $c_{j} \\in F_{q^{6}}$.\n   Reduction rule: $w^2-v=0$ which is irreducible in $F_{q^{6}}$.\n\n## Coordinate System\n\n### Affine Coordinates\n\n Traditional representation of the coordinates, i.e. just an $(x,y)$ where $x$ and $y$ satisfy curve equation. Normally this representation is used for storing and transmitting points.\n\n### Standard Projective Coordinates\n\nPoint in standatd projective coordinates $(X,Y,Z)$ represents $(\\frac{X}{Z},\\frac{Y}{Z})$ in Affine coordinate system. Also called homogenous projective coordinates as the curve equation takes on the homogeneous form $Y^{2}Z=X^{3}+4Z^{3}$.\n\n![standard-projective-coordinates](thoughts/images/standard-projective-coordinates.png)\n\nPoints become straight line through the origin in $(X,Y,Z)$ space, with the affine point being the interesection of the line with the plane $Z=1$.\n\n### Jacobian Coordinates\n\nJacobian Point $(X,Y,Z)$ -\u003e $\\frac{X}{Z^{2}},\\frac{Y}{Z^{3}}$. Curve equation becomes $Y^2=X^3+4Z^6$.\n\n## BLS Signatures\n\n## Resources\n\n- [ethereumbook's elliptic curve section](\u003chttps://github.com/ethereumbook/ethereumbook/blob/develop/04keys-addresses.asciidoc#elliptic_curve)\u003e\n- [An Introduction to Elliptic Curves](\u003chttps://www.math.brown.edu/johsilve/Presentations/WyomingEllipticCurve.pdf)\u003e\n- [Exploring Elliptic Curve Pairings](\u003chttps://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627)\u003e\n- [BLS12-381 for the rest of us](\u003chttps://hackmd.io/@benjaminion/bls12-381)\u003e\n- [Pairings over BLS12-381](\u003chttps://research.nccgroup.com/2020/07/13/pairing-over-bls12-381-part-2-curves/)\u003e\n\n---\n\n[^1]: Check Fermat's Little theorem.\n","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/finite-fields":{"title":"Finite Fields","content":"\n# Fields\n\nAlgebraic structure on which basic math can be performed.\n\n## Properties\n\n- Abelian group under addition\n- Non-zero numbers form abelian group under multiplication\n- multiplicative distribution over addition\n\n# Finite Fields (Galois Fields)\n\nFields defined on a number and the result is in the set.\n\n1. Closed\n2. Associative: $a*(b*c) = (a*b)*c$\n3. Identity: $a*1 = 1$\n4. Inverse: $a*a^{-1} = 1$\n5. Commutative\n\nIt has $p^m$ elements in it, where p = prime, and m is an integer.\n\n$GF(p^m)$ are defined as the finite fields with notation:\n\n1. m = 1: prime fields\n2. m \u003e 1: extension fields\n\n![Image.jpeg](https://miro.medium.com/max/1400/0*lp3NrLwr0fMyrBZc)\n\n\u003e Note: $\\mathbb{F}^*$ are defined as finite fields without 0 as it doesn't have an inverse.\n\n**Prime fields** can be defined on any integer and include number from 0 till that number. All of the operations are done modulo that number.\n\nExample: $\\mathbb{F}_{5} = \\left\\{0, 1, 2, 3, 4\\right\\}$ is a field defined on prime number 5 and include these numbers.\n\n**Extension fields** are polynomials and take the form: $\\mathbb{F(m \u003e 1)} = a_{m-1}X^{m-1} + \\cdots + a_1X^1 + a_0$\n\n![1*AScsGBdzW-L2JQq4hXgxMA.png](https://miro.medium.com/max/1054/1*AScsGBdzW-L2JQq4hXgxMA.png)\n\nExample: $GF(2^3)$'s elements are\n\n```other\n(a2, a1, a0)\n(0, 0, 0) = 0\n(0, 0, 1) = 1\n(0, 1, 0) = x\n(1, 0, 0) = x²\n(0, 1, 1) = x+1\n(1, 1, 0) = x²+x\n(1, 0, 1) = x²+1\n(1, 1, 1) = x²+x+1\n```\n\n## Properties\n\n- Every element of a GF(q) are roots of the polynomial $x^q-x=0$. Generally, every element in GF(p^n) satisfies the polynomial equation $x^{p^{n}}-x=0$.\n- every field has a **characteristic** $p$ which is p for which np=0 and is used to find identity of the field.\n- Non-zero elements of a finite field form a **multiplicative cyclic subgroup**. This means, all non-zero elements can be expressed as powers of a single element. This element is called **primitive element**.\n- All fields generated in GF(q) are isomorphic.\n\n## Roots of Unity in Finite Fields\n\n1. Every non-zero element of $\\mathbb{F}_{q}$ is a root of unity.\n2. Def: Primitive RoU: $x^{n}=1$, but $x^{m}\\neq{1} \\forall m\u003cn$.\n3. $\\mathbb{F}_{q}$ has a nth primitive RoU iff n divides q-1.\n\n# References\n\n- [Learning Cryptography, Part 1: Finite Fields](https://medium.loopring.io/learning-cryptography-finite-fields-ced3574a53fe)\n- [Finite Fields: Theory and Application](https://www.cantorsparadise.com/the-theory-and-applications-of-finite-fields-e78844896eaa)\n- [Finite Fields - Wikipedia](https://en.wikipedia.org/wiki/Finite_field)\n- [Primitive nth root of unity](https://www.csd.uwo.ca/~mmorenom/CS874/Lectures/Newton2Hensel.html/node9.html)\n- [Introduction to Finite Fields](https://web.stanford.edu/~marykw/classes/CS250_W19/readings/Forney_Introduction_to_Finite_Fields.pdf)\n- [Finite field arithmetic](https://cryptojedi.org/peter/data/eccss-20130911a.pdf)\n- [Scalar multiplication algorithms](https://cryptojedi.org/peter/data/eccss-20130911b.pdf)\n- [field](https://research.swtch.com/field)\n- [Galois field course](https://mathweb.ucsd.edu/~jmckerna/Teaching/16-17/Winter/200B/)\n- \n","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/fixed-rate-protocols":{"title":"Fixed Income Protocols","content":"\nDeFi's next wave of protocols has come through fixed income protocols. I will go deep into yield curves based protocols i.e. [Yield](https://yield.is), [Notional](https://notional.finance).\n\n## Yield Protocol\n\nYield introduced *yTokens* i.e. `Yield Tokens` similar to compound's *cTokens* which essentially behaves as zero-coupon bonds expiring at a future date and can be redeemed 1-on-1 for the underlying asset.\n\n*yTokens* can be building blocks that can be used to make many other interesting products. Market price of *yTokens* can be used as interest rate oracle. Each yToken has its own interest rate over the period to expiration date which can be used by many other protocols to settle on-chain interest rate derivatives.\n\n*yTokens* differ from each other in four aspects:\n\n1. Underlying Asset\n2. Collateral Asset\n3. Collateralization requirement\n4. Expiration Time[[]()]()\n\n## Actors\n\n### Borrowers\n\nBorrowers is when actor opens a vault, takes out *yToken* and sells it. It is essentially shorting the underlying asset or longing the collateral asset.\n\n### Lenders\n\nBuying yTokens are similar to lending the underlying asset in which the holder of yToken is earning an interest on the underlying asset in the form of the discount which it gets when buying the yTokens.\n\n## Settlement\n\nyTokens can be construcuted using 3 main principals.\n\n### Cash settlement\n\nCash Settlement is paid in the collateral asset, which implies that it depends on a dependent price oracle that determines the price of underlying asset in terms of collateral asset.\n\nAt the moment of maturity, anyone can call the contract to trigger *settlement*, that redeems the *yTokens* for its equivalent value in collateral asset. After the moment of settlement, *yTokens* begin to track the price of collateral asset rather than the price of underlying asset but can only be redeemed at the price of settlement.\n\nThis mechanism has an advantage that it can support any asset and not just ERC20 assets as it just needs a price oracle to compare the price of yTokens with the collateral asset.\n\n### Physical settlement through auctions\n\nAt the time of minting a yToken, it is backed using the collateral asset. Minting the tokens adds to the vault's owner debt which shouldn't be less than the value of the collateral asset plus some required margin.\n\nWhen a target asset is also an ERC20 token, its settlement can be triggered physically i.e. holders get paid in underlying asset rather than collateral assets through auctions.\n\nGradual dutch auctions are held to sell the collateral for the underlying asset. Remaining collateral is returned back to the vault owner and underlying tokens earned during the auction is ditributed among *yToken* holders.\nIf auction is not completed successfully, collateral asset is distributed amont the holders along with physical assets.\n\nAdvantage over cash settlement is that after the auction is successfully done, each *yToken* is backed equally with the underlying asset rather than some collateral asset. Holders can redeem the underlying asset (but doesn't earn yield on it).\n\n### Synthetics Settlement\n\nWhen the target asset itself is a collateralized synthetic asset like DAI, *yToken* uses token's own issuance mechanism as settlement.\n\nIn case of DAI, if yDAI backed with ETH as collateral matures, the protocol creates a vault in MakerDAO with ETH as collateral. When yDAI holders come to redeem, it borrows DAI from Maker and pay to the yDAI holders. Essentialy fixed rate position in yTokens turns into variable rate debt position at the time of maturity for these synthetic assets. yDAI holders need to pay the interest for their debt position and can earn DAI savings rate as well.\n\nAdvantage is that borrowers' and lenders' position is not settled and have the option to keep it open with the synthetic token's own mechanisms.\n\n## Interest Rate oracle\n\n*yTokens's* price in itself throughout the period until maturity can be treated as an interest rate oracle as the *yTokens* price floats freely depending on the supply and demand.\n\n$$Y = (\\frac{F}{P})^{\\frac{1}{T}} - 1$$\n\n---\n\n## YieldSpace AMM\n\nA new invariant based AMM introduced to trade *fyTokens* introduced in yield paper which incorporates time into the AMM equation.\n\n$$x^{1-t}+y^{1-t} = k$$\n\n$y$ = reserves of *fyToken*,\n\n$x$ = reserves of underlying token.\n\n$t$ = time to maturity\n\n---\n\n![yieldspace curve](thoughts/images/yieldspace_curve.png)\n\nThis formula works as constant sum protocol when $t-\u003e0$, and constant product formula when $t-\u003e1$.\n\nThis formula is defined in the yield space rather than the price space as designed in previous AMM formulas such that marginal interest rate of fyTokens at any time is equal to ratio of fyToken reserve to underlying token reserve minus 1.\n\n$$r = \\frac{y}{x} - 1$$\n\nThis formula does not have any time component, thus ensures that marginal interest rate remains proportional to fyToken and underlying token reserves at any point in time. This implies that as the allocation of fyToken in the pool increases or underlying token decreases so does the interest rate and buying pressure arises, and vice versa.\n\n## Why not other invariants\n\n1. Constant sum invariant only works for assets of similar value, and fyToken generally is priced at a discount until maturity date.\n2. Constant product formula includes liquidity at whole price spectrum but when the fyToken approaches maturity, its price tend to be similar to underlying token and thus the liquidity at other price points are wasted and larger trades have significant impact on interest rates.\n3. Curve's stableswap equation doesn't let it modify $\\chi$ to account for variation in interest rates due to time to maturity.\n\n## Properties\n\n$$x^{1-t}+y^{1-t} = x_{start}^{1-t} + y_{start}^{1-t}$$\n\nMarginal price for a given $x_{start}$, $y_{start}$, and $t$ is given by the formula:\n\n$$(\\frac{y}{x})^t = (\\frac{(x_{start}^{1-t} + y_{start}^{1-t} - x^{1-t})^\\frac{1}{1-t}}{x})^t$$\n\n![token price vs reserves](thoughts/images/dai_price_vs_dai_reserves.png)\n\nLooking at interest rates,\n\n![interest rate vs dai reserves](thoughts/images/interest_rate_vs_dai_reserves.png)\n$$\\frac{y}{x} - 1 = \\frac{(x_{start}^{1-t} + y_{start}^{1-t} - x^{1-t})^\\frac{1}{1-t}}{x} - 1$$\n\n## Fees\n\nLP are incentivised to provide liquidity using the fees that they earn. Since, constant sum power formula is defined in yield space and not price space, it's not meaningful to impose fees on price and rather on interest rates i.e. any buyer of fyToken should get lesser interest rates or higher buy price.\n\nThus, the fee formula modifies the interest rate by adding a variable $g \u003c 1$ to change interest rates.\n\n$$r = (\\frac{y}{x})^g - 1$$\n\n\u003e Note that this formula is used for buying fyTokens, $\\frac{1}{g}$ is used when selling fyTokens.\n\nThus, the new AMM formula becomes\n\n$$x^{1-gt}+y^{1-gt} = k$$\n\n## Capital Efficiency\n\nOriginal protocol allows user to mint 1 fyToken in exchange of 1 underlying token and there is no real incentive to buy a fyToken above the underlying token price. Thus, the pool always checks at the end of every trade that price of 1 fyToken is not greater than 1 underlying token or reserves of fyToken is greater underlying. Thus, Some portion of fyToken reserves in the pool is always inaccessible. Example can be when pool is first initialized, the equal fyToken in the pool is never utilised as the remaining fyToken can't be sold.\n\nSo, the capital efficieny of the pool is improved by making the excess fyToken's reserves `virtual`. LPs don't need to contribute these access reserves. Pool uses liquidity tokens `s` as the virtual fyTokens reserves. Whenever a trade occurs, `virtual` tokens are added to actual reserves to calculate the appropriate amount but whenver liquidity is added, only the real reserves are used to calculate fyTokens in proportion to the actual fyToken in pool.\n\n## Resources\n\n- [Yield Paper](https://research.paradigm.xyz/Yield.pdf)\n- [Yieldspace paper](https://yield.is/YieldSpace.pdf)\n- [Element finance paper](https://paper.element.fi//)\n- [Sense](sense-finance.md)\n- [Messari's Fixed Income Protocol](https://messari.io/article/fixed-income-protocols-the-next-wave-of-defi-innovation)\n- [Designing Yield Tokens](https://medium.com/sensefinance/designing-yield-tokens-d20c34d96f56)\n- [Swivel's cash flow instruments Pt.1](https://swivel.substack.com/p/cash-flow-instruments-pt-1-history?s=r)\n- [Defization of fixed income products](https://medium.com/coinmonks/the-defization-of-fixed-income-products-7e72ed4f57b1)\n- [Defixed income](https://medium.com/@exactly_finance/defixed-income-101-948976c0e2c6)\n- [Notional](https://medium.com/coinmonks/notional-the-alpha-of-fixed-income-defi-products-a5637d2092b5)\n- [](https://medium.com/finoa-banking/turning-proof-of-stake-yield-into-fixed-income-products-7de8a73097ac)\n- [Fixed Income Protocols](https://medium.com/gamma-point-capital/fixed-income-protocols-the-next-wave-of-defi-innovation-69215be82b4e)\n\n\n## Questions\n\n- What are fixed yield rate protocols?\n- different types of protocols currently\n\nmainly three types: tranches, zero-coupon and stripping. Sense follows stripping architecture\n\n- what are tranches based and zero-coupon based?\n\ntranches are where users provide their assets and protocols invests in different strategies on the basis of risks of the tranche. More risk assosciated. Each protocol has their own set of safety backstops to stop protocol from being insolvent.\n\nZero coupon based protocols essentialy turn the asset into a bond which is traded at a discount and is exchangeable 1-1 at maturity.\n\nSense follows stripping architecture that allows it to strip target into PT and YT. Note that target in Sense can be yield bearing assets. This yield is given then to YTs.\n\n- What are yield or notional doing?\n\nyield issues zero-coupon bonds taking collateral and giving fyTokens. susceptible to liquidations in volatile markets.\n\n- what is 88mph or barnbridge doing?\n\n88mph/barnbridge uses lending market protocols like compound, aave to provide fixed yields to deposited assets. These fixed interest rate models are determined by governance. These protocols are prone to drop in interest rate offered by the variable markets after the deposit. They offset this by issuing Floating rate bonds to users for the extra yield. This model is hard to scale, requires governance at every step, not risk-free for users as they can't exit at will because the firb are issued at fixed terms.\n\n- Why yield stripping and not others?\n\ntranches not safe, and zero-coupon bonds mainly use underlying assets which does not take into account the yield. Stripping applications takes a yield bearing assets and gives the user security against their principal in PTs and yields in YTs. So, its just more safer and transparent for users, better abstractability and flexibilty for devs and users as they can plug these PT or YT further.\n\n- Sense space?\n- yieldspace pools\n\nmakes sense to account for implied rate and liquidity to be spread around interest rate rather than the price as price there is function of rate itself. space better as target is deposited, no IL, yield goes to LPs.\n\n- How are PTs and YTs priced?\n\nStripping protocols follows the invariant that PTs + YTs at any point = Target. At the time of series creation, YTs are the claim to the yield and PTs trade at discount which is Target - YTs. As maturity approaches, PTs tend towards the price of target 1 on 1.\n\nPTs are priced according to formula $(\\frac{y}{x})^{\\frac{1}{t}}$, where x and y are reserves in pool.\n\nYTs are priced according to the yield payments that they're going to receive till maturity. So, if 4 yield payments of $0.1, then $0.4 is their price.\n\n- What is a series?\n\nseries is a specific set of PTs and YTs of a target with specific maturity and specific adapter.\n\n- list all common sc attack patterns?\n\nunverified calls, dos, delegatecalls, signature malleability, re-entrancy, arithmetic over/underflows, randomisation in evm, tx.origin, selfdestruct\n\n- compound, aave, fuse?\n\n- what is your thinking when designing some project?\n\ndepends on the project,\n\n1. swap\n2. lend\n\nthen thinking about actors and their actions, then modules, external interactions\n\nuniswap like structure i.e. core and periphery. Anybody can directly build on core and periphery is used for normal user interaction with the protocol.\n\n- How do you guys take an idea from different phases to mainnet?\n- do you use other tools to test your smart contracts like slither, echidna ?\n-","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/fourier-transform":{"title":"","content":"\nErasure Coding is a fundamental part of many algorithms and have a variety of use cases like securing data availability in blockchains and STARKs. And one of the key building blocks of Erasure Coding is **Fast Fourier Transforms**. Life is a full circle, right? I ran away from the course teaching Fourier Transforms as far as possible and here I am, trying to learn it again. But this time it's different as *curiosity* has appeared wildly.\n\n\u003e *But you're a dirty little liar with a message of obsession to come*\n\u003e \n\u003e *You got your head in the clouds*\n\u003e \n\u003e *And your world's upside down*\n\u003e \n\u003e *Get away from the life you're living*\n\nSong time baby!! This time it's [Dirty Little Thing](https://open.spotify.com/track/1PlTjGYNKGsU7Rqi5dZZCk?si=3f7cb22c42c64ee0) by Velvet Revolver. Oooooh, it's a wild song. I couldn't get the tune out of my head when I first listened to it.\n\nDFT: Coefficient representation -\u003e point-value\nInverse DFT: Point-value -\u003e coefficient\n\n## Working with Polynomials\n\nAn $n$-degree polynomial is represented in coefficient form:\n\n$$\nP(x) = \\sum_{j=0}^{n-1}a_{j}x^{j}\n$$\n\nAlgorithms and Time Complexity:\n\n1. Addition/Subtraction: $O(n)$ in coefficient and point-value\n2. Multiplication: $O(n^2)$ in coefficient and $O(n)$ in point-value and converting between the two can be done in $O(n\\log n)$\n\n## Interpolation\n\n### Vandermode\n\nVandermode matrix: $O(n^3)$\n\n$$\n\\begin{pmatrix}\n1 \u0026x_0 \u0026x_0^2 \u0026\\ldots \u0026x_{0}^{n-1} \\\\\n1 \u0026x_1 \u0026x_1^2 \u0026\\ldots \u0026x_1^{n-1} \\\\ \n\\vdots \u0026\\vdots \u0026\\vdots \u0026\\ldots \u0026\\vdots \\\\\n1 \u0026x_{n-1} \u0026x_{n-1}^2 \u0026\\ldots \u0026x_{n-1}^{n-1} \\\\ \n\\end{pmatrix}\n\\begin{pmatrix}\na_0 \\\\\na_1 \\\\ \n\\vdots \\\\ \na_{n-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\ \n\\vdots \\\\ \ny_{n-1}\n\\end{pmatrix}\n$$\n\n### Lagrange\n\n$$\nP(x) = \\sum_{k=0}^{n-1}y_k\\frac{\\prod_{j\\neq k}(x-x_{j})}{\\prod_{j \\neq k}x_{k}-x_{j}}\n$$\n\n## DFT and FFT\n\nDomain should be power of 2, and complex roots of unity, $\\omega_n^i$.\n\nLemma's that help in understanding FFT:\n\n1. Cancellation Lemma: $\\omega_{dn}^{dk}=\\omega_n^k$\n2. Halving Lemma: squares of nth complex root of unity are n/2 complex n/2th root of unity. $(\\omega_n^{k+n/2})^{2}=(\\omega_n^{k})^2$\n3. Summation Lemma: $\\sum_{j=0}^{n-1}(\\omega_{n}^{k})^j=0$\n\n![fft](thoughts/images/fft.png)\n\n## Barrentenberg\n\n### polynomial\n\n#### `polynomial_arithmetic`\n\n- `fft_inner_serial`: \n- `compute_multiplicative_subgroup`: compute `(g.X)^n`\n- `compute_linear_polynomial_product`\n- `compute_kate_coefficients`\n- `fft_inner_parallel`:\n- `compute_interpolation`\n- `compute_efficient_interpolation`\n- \n- `compute_lagrange_polynomial_fft`:\n\n#### `evaluation_domain`\n\n```\n    size_t size;        // n, always a power of 2\n    size_t num_threads; // num_threads * thread_size = size\n    size_t thread_size;\n    size_t log2_size;\n    size_t log2_thread_size;\n    size_t log2_num_threads;\n    size_t generator_size;\n\n    Fr root;           // omega; the nth root of unity\n    Fr root_inverse;   // omega^{-1}\n    Fr domain;         // n; same as size\n    Fr domain_inverse; // n^{-1}\n    Fr generator;\n    Fr generator_inverse;\n    Fr four_inverse;\n\n  private:\n    std::vector\u003cFr*\u003e round_roots; // An entry for each of the log(n) rounds: each entry is a pointer to\n                                  // the subset of the roots of unity required for that fft round.\n                                  // E.g. round_roots[0] = [1, ω^(n/2 - 1)],\n                                  //      round_roots[1] = [1, ω^(n/4 - 1), ω^(n/2 - 1), ω^(3n/4 - 1)]\n                                  //      ...\n    std::vector\u003cFr*\u003e inverse_round_roots;\n\n    std::shared_ptr\u003cFr[]\u003e roots;\n```\n\n- `invert_with_external_scratch`: acc: 1/a1..a5, scratch_i = prod(a_i-1)\n\nimplementing runtime states, can't figure out right now, how to share vector between the threads.\nIt's done using pointer arithmetic in cpp, but how to do this in rust? `Rc` maybe?\nBut read that `Rc\u003cVec\u003e` doesn't make any sense? Why? it doesn't sit right to reference count a vector.\n\n\n### common\n\n\n\n\n## Resources\n\n- CLRS, Introduction to Algorithms, Chapter 30\n- [FFT](https://vanhunteradams.com/FFT/FFT.html)","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/fraud-proofs":{"title":"Fraud Proofs","content":"\n## Readings\n\n- [Fraud Proofs Explainer](https://medium.com/infinitism/optimistic-time-travel-6680567f1864)","lastmodified":"2023-07-24T05:57:52.920603701Z","tags":null},"/thoughts/intro-to-cryptography":{"title":"Intro To Cryptography","content":"\nThis, by no means, is a thorough post covering all the topics. This is just my notes that I like to make while learning any new topic. I have tried attaching any good resource I found along the way.\n\n# ECC\n\n- [ECDSA: Elliptic Curve Signatures - Practical Cryptography for Developers](https://cryptobook.nakov.com/digital-signatures/ecdsa-sign-verify-messages)\n- [Elliptic Curve Cryptography: a gentle introduction](https://andrea.corbellini.name/2015/05/17/elliptic-curve-cryptography-a-gentle-introduction/ \"Elliptic Curve Cryptography: a gentle introduction\")\n\nnonce: $r$\n\ntemp key: $R = r*G$\n\nSignature in ECDSA: $s = k^{-1} * (h + r*x) (mod n)$\n\n# EdDSA Algorithm\n\nUsed to sign arbitrary messages with private keys\n\n- Public key, $PK: xG$\n- Create temp key $R$ from random nonce, $r$: $R = rG$\n- $e = Hash(R||m)$\n- $s = r + ex$\n\nPublic key is known, then send $m, R, s$ to other party\n\n# EdDSA Verification\n\nKnown: PK, m, r, s\n\n$$s.G = r.G + ex.G = R + e.PK$$\n\nOther party calculates $e, s.G$ If $LHS = RHS$, then signature is correct\n\n# ECDH (Elliptic Curve Diffie-Hellman)\n\nWays for two parties to exchange information without revealing their private keys\n\nLet there be two parties Alex and Alice,\n\n- Alex has $pk$: $x$, Alice has $pk$: $y$\n- Generate public keys: $P_{alex} = xG$, $P_{alice} = yG$\n- Send each other public keys\n- Calculate new public key $P_{s} = P_{alex} * y$ OR $P_{alice} * x$\n\n# Threshold Signature Schemes\n\nContrary to multi-sigs, these signature employ MPCs to create signatures using interactive multiple parties, thus not giving ownership of the asset to only one party while not blowing up computation needs.\n\n# Shamir Secret Sharing\n\n- [Shamir's Secret Sharing Algorithm | Cryptography - GeeksforGeeks](https://www.geeksforgeeks.org/shamirs-secret-sharing-algorithm-cryptography/)\n- [Shamir's Secret Sharing Scheme](https://www.zkdocs.com/docs/zkdocs/protocol-primitives/shamir/)\n\nSecret $S$ is divided into $n$ *pieces* or *shares* $s_{1} + s_{2} + \\cdots + s_{n}$ such than any  combination of any $k$ pieces is enough to recover the secret, but any $k-1$ pieces cannot recover $S$.\n\nThis is based upon polynomials such that any $k-degree$ polynomial can be uniquely identified using $k+1$ distinct points.\n\n## Splitting $S$\n\n- Generating polynomial $f(x)$ of degree $k-1$ over a finite field $\\mathbb{F}_{p}$\n- generating distinct shares:  $s_{i} = x_{i}, f(x_i)$\n\nPolynomial is of the form:\n\n$$f(x) = S + r_{1}x+\\cdots+r_{k-1}x^{k-1}$$\n\nThese $r_i$ coefficients are randomly sampled and the points are generated which are the shares.\n\n## Recovering $S$\n\n$S$ is recovered using Lagrange polynomial.\n\n![math4.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/doc/70038E5D-C6DE-4FC6-9A42-D6D734E55270/6EE820CD-DA8D-43F9-A472-7C5B9B18A23A_2/YRLa7DIbI5gyAIkWykkHWsTRbs6fkOFNdqt2yOvXfi4z/math4.png)\n\nPutting $x = 0$, gives us $f(0) = S$\n\n## Pitfalls\n\n- **Zero-share**: Common pitfalls occur during share generation as any shareholder who gets $x_{i} = 0$ has its share as $0, S$, thus revealing the secret.\n- **Non-unique shares:** when generating secret back, the protocol needs to evaluate $\\frac{x_m}{x_m-x_j}$ , and if $x_m = x_{j} \\space (mod \\space q)$, then $x_{m}-x_{j}$ doesn't exist and protocol fails.\n\n# Schnorr Identification Protocol\n\n[Schnorr identification protocol](https://ebrary.net/134583/computer_science/schnorr_identification_protocol)\n\n[Schnorr Identification Scheme - GeeksforGeeks](https://www.geeksforgeeks.org/schnorr-identification-scheme/)\n\n[Ring Signatures (Part 1): Schnorr Identity Protocol](https://medium.com/@jkendzicky16/ring-signatures-part-1-schnorr-identity-protocol-320bd0fe7bf0)\n\n[Schnorr's identification protocol](https://www.zkdocs.com/docs/zkdocs/zero-knowledge-protocols/schnorr/)\n\nIdentification protocol is different than signature scheme as identification protocol is used to identify that prover holds the private while signature scheme is used to verify that the holder indeed used the private key to sign the message and generate the signature.\n\nSteps followed in schnorr identification protocol:\n\nLet there be two parties $Alex (Prover)$ and $Alice (Verifier)$,\n\n1. Alex has private key $x$, public key = $Gx (mod N)$\n2. Generates random value, $Y = Gy (mod N)$\n3. $Y$ is sent to Alice\n4. Alice generates random challenge $c$, and send to Alex\n5. Alex sends back: $z = y + xc$\n6. Alice verifies: $z.G = y.G + c.x.G = Y + c.PK$\n\nNot feasible, requires interaction from *verifier*. Also not publicly verifiable as both parties have to be involved.\n\nNon-Interactive protocols like Fiat-Shamir which transforms any interactive to non-interactive.\n\n# Schnorr Digital Signature\n\n- [Introduction to Schnorr Signatures](https://tlu.tarilabs.com/cryptography/introduction-schnorr-signatures)\n- [Schnorr Digital Signature - GeeksforGeeks](https://www.geeksforgeeks.org/schnorr-digital-signature/)\n- [How Schnorr signatures may improve Bitcoin](https://medium.com/@snigirev.stepan/how-schnorr-signatures-may-improve-bitcoin-91655bcb4744)\n\nUsed to implement “*Proof of Knowledge*”. Used in cryptography to prove to verifier that prover knows something $x$. Verifier gets convinced that they are communicating with the prover without verifier’s knowledge of private key and prover is in fact, right about his private key.\n\nFeatures:\n\n1. faster than ECDSA as doesn’t have to calculate $1/s$\n2. linear\n\nBut this also proposes other disadvantages that this signature scheme can’t be used for aggregation and multi-sigs due to its linearity.\n\n\u003e Non-interactive Schnorr identification protocol also exists using Fiat-Shamir transformation where challenge $c$ is not generated by verifier but $c = Hash(g, q, h, u)$ where g: generator, q: prime number, h: public key, u: public key of random nonce\n\n# Fiat-Shamir Heuristic\n\n- [Feige-Fiat-Shamir and Zero Knowledge Proof](https://medium.com/asecuritysite-when-bob-met-alice/feige-fiat-shamir-and-zero-knowledge-proof-cdd2a972237c)\n- [Non-Interactive Zero Knowledge Proof - GeeksforGeeks](https://www.geeksforgeeks.org/non-interactive-zero-knowledge-proof/)\n- [Ring Signatures (Part 1): Schnorr Identity Protocol](https://medium.com/@jkendzicky16/ring-signatures-part-1-schnorr-identity-protocol-320bd0fe7bf0)\n- [Fiat-Shamir transformation](https://www.zkdocs.com/docs/zkdocs/protocol-primitives/fiat-shamir/)\n\n![Feige-Fiat-Shamir.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/7C67DBCA-3360-4109-833A-03B26CCD00D7_2/dQqjhDktmjoMAfrbxT4zZkhqEjbfxapng3dGPIobNL0z/Feige-Fiat-Shamir.png)\n\nMethod to convert interactive ZKP system into non-interactive system such that public verifiability is feasible and both prover and verifier doesn’t need to be online at all times.\n\nLet *prover* prove to *verifier* that it possess $s1 \\cdots sk $secrets co-prime to $N$, without revealing any of the number to verifier. Takes advantage of difficulty of calculating modular square roots as verifier can’t derive $s1$ from $v1$\n\n# BLS Signatures\n\n- [Math \u0026 Engineering](https://xn--2-umb.com/22/bls-signatures/)\n- [BLS signatures: better than Schnorr](https://medium.com/@snigirev.stepan/bls-signatures-better-than-schnorr-5a7fe30ea716)\n-  [BLS12-381](https://hackmd.io/@benjaminion/bls12-381)\n\nComes under elliptic curve pairing based schemes which makes aggregation easier than other signature schemes like ECDSA, schnorr.\n\nTwo main things used in BLS signatures are:\n\n## Hash to Curve\n\nFor BLS signature to work, message hashing has to be on the curve i.e. $H(m)$ has to be a point on the curve.\n\nTo do this, the method is to concatenate message incrementally with counter to try to find a point.\n\n![BLS_hashing_to_curve.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/85F2FC7E-BC19-4024-9381-CBE498DB7A0F_2/xD1oFGhTuMLUHyu3gpZ5IoTdUo4x1qcfVtjbiXxZrwIz/BLS_hashing_to_curve.png)\n\n# Pairings\n\nSignature/Proof of possession: $S_X = x.H_{G_1}(X)$\n\n$$e(S_X, G_2) = e(H’_{G_1}(X), X)$$\n\n# Commitment Schemes\n\nProver commits to a message and later opens it to reveal the content of the message without revealing it beforehand.\n\n# Zero Knowledge Proof System\n\n- Completeness/Liveness: If *prover* is correct, it will eventually convince *verifier*\n- Soundness/Safety: *prover* can only convince *verifier* if it’s correct\n- Zero Knowledge\n\n## Proving Schnorr Signatures Are Zero-knowledge\n\nProving **completeness** is the easiest part in protocol, i.e. if the prover performs protocol honestly, verifier is able to verify it by just substituting g in schnorr signatures.\n\n**Soundness** is done through another algorithm called ***extractor*** which is able to extract secret of the prover by tricking it. Note: it doesn’t actually reveal the secret but only proving that extractor can extract the secret by duping prover. Done through *replay attack* on schnorr protocol.\n\n**Zero-Knowledgness** is proven through a *simulator* which has no knowledge of the the secret and yet it is able to convince every verifier into believing that the statement is true. This has an **assumption** that *verifier* is **honest.** Done through rewinding a verifier so that prover knows challenge $$c$$ and forging false signature on the basis of it.\n\n# References\n\n- [CS-355: Topics in Cryptography; Lecture 5: Proofs of Knowledge, Schnorr’s Protocol, NIZK](https://crypto.stanford.edu/cs355/19sp/lec5.pdf)\n- [GitHub - jlogelin/fiat-shamir: Zero Knowledge Proofs with Fiat-Shamir Heuristic in Solidity](https://github.com/jlogelin/fiat-shamir)\n- Lecture 1: Honest Verifier ZK and Fiat-Shamir: [https://www.cs.jhu.edu/~susan/600.641/scribes/lecture11.pdf](https://www.cs.jhu.edu/~susan/600.641/scribes/lecture11.pdf)\n- [0xPARC ZK Learning Group](https://0xparc.notion.site/ZK-Learning-Group-Topics-f53933eecc2f41438c6c2bdd5b42ee2d)\n- [eth-research ZK learning starter pack](https://ethresear.ch/t/zero-knowledge-proofs-starter-pack/4519)\n- [Road to ZK](https://plum-lightning-36c.notion.site/Road-to-ZK-2e85993b316b4c7c831bcdc866005e1b)\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/meromorphic-functions":{"title":"Meromorphic Functions","content":"\nHolomorhpic function (also known as Analytic function) usually refer to functions that are inifinitely differentiable.\n\nFormal Definition:\n\nLet G be an open set in $\\mathbb{C}$. A function $f: G \\to C$ is called holomorphic if, at every point $z \\in G$, the complex derivative\n\n$$f'(z) = \\lim\\limits_{h \\to 0} \\frac{f(z+h) - f(z)}{h}$$\n\nexists as a complex number where $\\mathbb{C}$  = complex realm.\n\n## Meromorphic Functions\n\nMeromorphic Function is the ratio of two analytic function which are analytic except for [[zeroes-and-poles|isolated singularities]].\n\n## What it means to be complex differentiable\nDifference between regular differentiable vs complex differentiable.","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/partial-synchrony":{"title":"","content":"GST stands for Global Stabilization time that is a finite time found in partial synchronous models. System behaves asynchronously till GST and synchronously after GST. Note that the adversary can delay GST for a finite amount of time and no protocol can explicitly detect that GST event has occured.\n\nDesign protocols for system that are usually synchronous in normal settings, but the protocol is designed to behave asynchronously. This guarantees safety and only after a finite amount of time (GST) when the synchrony assumptions are violated, does liveness and termination guarantees are provided.\n\n# Resources\n\n- [Synchrony, Asynchrony and Partial synchrony](https://decentralizedthoughts.github.io/2019-06-01-2019-5-31-models/)\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/pedersen-commitments":{"title":"Pedersen Commitments","content":"\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/polynomial-commitments":{"title":"Polynomial Commitments","content":"\nThis is a really informal post of my understanding of polynomial commitments. I am in no way a professional in cryptography and is just learning this for fun. Most of the excerpts in these notes are borrowed from the amazing article by [Dankrad](https://dankradfeist.de/ethereum/2020/06/16/kate-polynomial-commitments.html) and [Alinush](https://alinush.github.io/2020/05/06/kzg-polynomial-commitments.html).\n\n\u003e *My pain*\n\u003e\n\u003e *Is self-chosen*\n\u003e\n\u003e *At least*\n\u003e\n\u003e *So The Prophet says*\n\nNow, another song for you to listen along for this ride that you're embarking on with me. [River of Deceit](https://open.spotify.com/track/5EUsI3LIV042IV5ydksV9y?si=7e97529079ca4e70) by Mad Season. This was a total random song found while listening to my Daily Mix playlist. Anyways, I like it.\n\n# Commitments\n\nContains two algorithms:\n\n1. $commit(m,r) \\rightarrow com$\n2. $verify(m,com,r) \\rightarrow 0/1$\n\nProperties:\n\n1. Binding: can't produce two valid openings for one commitment\n2. Hiding: com doesn't reveal about committed data.\n\n## Functional Commitments\n\ncommitting to a function $f$ in family $F={f:X\\rightarrow Y}$.\n\nSteps followed:\n\n1. $setup()$ -\u003e pp\n2. $commit(f,pp,r)$-\u003e$com$\n3. eval(Prover P, Verifier V): $x\\in X$, $y \\in Y$ s.t. $f(x)=y$\n\t1. $P(f,pp,x,y,r)$ -\u003e succinct proof $pi$\n\t2. $V(pp,com,x,y,\\pi)$ -\u003e 0/1\n\nThree main types of functional commitments possible:\n\n1. Polynomial commitments: Univariate polynomial of at most degree d\n2. Multilinear commitments: Multivariate polynomial\n3. Linear commitments: Linear functions $f_\\vec{v}(\\vec{u}) = \u003c\\vec{u},\\vec{v}\u003e=\\sum^{i=1}_{n}u_{i}v_{i}$\n\n\u003e [!info] different types of PCS:\n\u003e 1. Basic Elliptic curves: Bulletproofs\n\u003e 2. Bilinear groups: KZG\n\u003e 3. Groups of unknown order: DARK\n\u003e 4. Hash functions: FRI\n\u003e\n\u003e More info about comparison between these schemes [here](https://hackernoon.com/kzg10-ipa-fri-and-darks-analysis-of-polynomial-commitment-schemes).\n\n## Polynomial Commitment Scheme (PCS)\n\n- what properties are desirable for an efficient PCS? :: commitment time is fast and proof is small, verify/eval time is fast\n- what do you mean by small and fast? :: small means few bytes, for ex: in KZG, it's just one group element and BN254 is just 32 bytes. By fast, i mean in quasilinear time. for ex: verification in KZG takes $O(logd)$, where d is degree of polynomial. \n\n- see, commitment to a polynomial is just evaluation of that polynomial at a point and brute force method takes $O(d)$ time, where d is the degree of the polynomial and if you want to commit to d polynomials, it amounts to $O(d^2)$ time.\n\t- there's a better method than that, i.e. NTT Number theory transform which can do this in O(d logd).\n\t\t- implement this\n\t- even better than this -\u003e use lagrange interpolation to find $f(\\tau)$ which is linear in $d$.\n - then comes evaluation proofs $\\pi_{a}$ -\u003e naively each evaluation proof takes $O(d)$ time, and for $d$ polynomials this goes to $O(d^2)$\n\t - comes Feist-Khovartovich algorithm, which takes it in $O(d logd)$ if $\\Upomega$ domain is multiplicative subgroup.\n\n## KZG Commitments\n\nA **polynomial commitment scheme** allows anyone to commit to a polynomial $p(X)$ with the properties that this commitment $C$ (an [[elliptic-curves|elliptic curve]] point) can later be verified by anyone at any position $z$ on that polynomial. The prover proves at a position the value of the polynomial known as *proof* $\\pi$ versus the claimed value known as *evaluation* $p(z)$ and this is only possible when prover actually has provided proof for that one polynomial that it committed to.\n\n![kzg-steps](thoughts/images/kzg-steps.png)\n\n### Notation\n\n- Let $\\mathbb{G_1}$ and $\\mathbb{G_2}$ be two elliptic curves with a pairing $e: \\mathbb{G}_1 \\times \\mathbb{G}_2 \\rightarrow \\mathbb{G}_T$.\n- Let $p$ be the order of $\\mathbb{G}_1$ and $\\mathbb{G}_2$ and $G$ is generator of $\\mathbb{G}_1$ and $H$ is generator of $\\mathbb{G}_2$.\n- $\\[x]_1 = xG \\in \\mathbb{G}_1$ and $\\[x]_2 = xH \\in \\mathbb{G}_2$ where $x \\in \\mathbb{F}_p$.\n\n### Trusted Setup\n\nYou might've read multiple times in twitter threads that there is a trusted ceremony that is associated with DAS, SNARKs or words like KZG Ceremony. They are usually talking about this trusted setup. Also known as [Powers of tau](https://vitalik.ca/general/2022/03/14/trustedsetup.html#what-does-a-powers-of-tau-setup-look-like).\n\nIn this ceremony, a random secret $\\tau \\in \\mathbb{F}_p$ is created, using which its powers $[\\tau^i]_1$ and $[\\tau^i]_2$ for $i = 0, \\ldots, n-1$  are created. In additive notation, it is represented as:\n\n$$[\\tau^i]_1 = (G, \\tau G, \\tau^2 G, \\ldots, \\tau^{n-1} G) \\in \\mathbb{G}_1$$\n$$[\\tau^i]_2 = (H, \\tau H, \\tau^2 H, \\ldots, \\tau^{n-1} H) \\in \\mathbb{G}_2$$\n\nThese elements which are just elliptic curve points are made public but the secret has to be destroyed otherwise this ceremony has no meaning as anyone will be able to forge invalid proofs later. In practice this is usually implemented via a secure multiparty computation (MPC), which uses multiple participants to derive these elements and require all participants together to know secret $\\tau$. Thus, this has a nice 1-to-N security assumption, basically only 1 honest participant is required which is easy to obtain.\n\nNow, you must be thinking what are they used for? We know that a polynomial is represented as:\n\n$$Polynomial: p(X) = \\sum_{i=0}^{n}p_iX^i$$\n\nNow, the prover can compute the commitment to the polynomial as:\n\n$$Commitment: C = [p(\\tau)]_{1}$$\n\n$$= \\left[\\sum_{i=0}^{n} p_{i} {\\tau}^{i}\\right] = \\sum_{i=0}^{n}p_i[\\tau^i]$$\n\nThis is a really interesting output, as prover is able to evaluate polynomial at point $\\tau$, even when he doesn't know the secret and this becomes the commitment of prover of the polynomial.\n\nNow, you might be wondering if a malicious prover find another polynomial $q(X) \\neq p(X)$ such that $[q(\\tau)]_1 = [p(\\tau)]_1$. But the prover doesn't know $\\tau$, so the best bet for prover to achieve $p(\\tau) = q(\\tau)$ is to make the polynomial pass through as many points $n$ as possible. But it's computably inefficient to find the exact polynomial that passes through $n$ points where curve order $p \\approx 2^{256}$ as $n \u003c\u003c\u003c p$.\n\n### Proof Evaluation\n\nWe need a mechanism so that prover could verify its commitment on the polynomial and this is done through *evaluations* on the polynomial $p(X)$. Evaluation is done at a point $z$ and the claimed value $p(z)$ is verified by the verifier.\n\nNow, a value $z$ is chosen such that $p(z) = y$. This means that $p(X)-y$ should have a factor $X-z$. So, a quotient polynomial $q(X) = \\frac{p(X) - y}{X-z}$ is computed and the evaluation proof becomes,\n\n$$\\pi = [q(\\tau)]_1$$\n\n$$ = \\sum_{i=0}^{n}[\\tau^i] \\cdot q_i$$\n\nProver sends this proof to verifier which verifiers this proof.\n\n### Verifying Evaluation\n\nNow, verifier has following things:\n\n- Polynomial: $p(X) = \\sum_{i=0}^{n}p_iX^i$\n- Commitment: $C = \\sum_{i=0}^{n}p_i[\\tau^i]$\n- Evaluation point: $z$\n- Claimed value: $y$\n- Proof: $\\pi = [q(\\tau)]_1$\n\nNow verifier wants to verify that claimed value is indeed correct using the proof. That's where [[bilinear-pairings|pairings]] come into play. Pairings allow us to multiply two polynomials in different curves and get the output in a target curve. This unfortunately is not possible in a single curve which is a property called Fully Homomorphic Encryption for elliptic curves. ECs are only homomorphic additively.\n\nVerifier checks the following equation:\n\n$$e(\\pi, [\\tau]_2-[z]_2) = e(C-[y]_1, H)$$\n\n\u003e Note: verifier has $[\\tau]_2$ from the trusted setup and $[\\tau - z]_2$ is computed through EC arithmetic.\n\nThis convinces verifier that claimed value is indeed the evaluation of the committed polynomial at the point $z$. How? Let's unroll the equation,\n\n$$e(\\pi, [\\tau]_2-[z]_2) = e(C-[y]_1, H)$$\n$$e(\\pi, [\\tau-z]_2) = e(p[\\tau]_1-[y]_1, H)$$\n$$[q(\\tau) \\cdot (\\tau-z)]_T = [p(\\tau)-y]_T$$\n\nWe need two properties from this equation:\n\n- Correctness: If prover followed right steps, then proof is correct.\n- Soundness: if they produce incorrect proof, verifier won't be convinced.\n\nCorrectness checks out as this equation is similar to what we defined in the quotient polynomial $q(X)(X-z) = p(X)-y$.\n\nFor soundness, prover would have to compute incorrect proof,\n\n$$\\pi' = (C - [y']_1)^{\\frac{1}{\\tau-z}}$$\n\nBut it's not possible to do this as $\\tau$ is not known to prover and if they can forge this proof, then they can convince verifier for anything.\n\n### Comparison to Merkle Trees\n\nMerkle trees are a form of *vector commitments*, i.e. Using a merkle tree of depth $d$ will have $2^{d}-1$ elements, anyone can compute a commitment to a vector $(a_0, a_1, a_2, \\ldots, a_{2^d-1})$. Thus, using Merkle trees anyone can prove that element a_i is a member of a vector at position i using d hashes.\n\n![merkle-tree](thoughts/images/merkle-tree.png)\n\nYou can even make polynomial commitment out of Merkle trees. A polynomial is represented as $p(X) = \\sum_{i=0}^{n} p_i X^i$, using $a_i = p_i$, we can construct a polynomial of degree $n = 2^d-1$ and computing merkle root of the coefficients. Now, the prover can prove the evaluation by sending all the $p_i$ and verifier verifying the evaluation by computation $p(z) = y$.\n\nThis is a very simple polynomial commitment that is inefficient to its core but will help to understand the marvel KZG commitment is.\n\n- Prover has to send all the $p_i$ to verifier in order to commence verification. So, proof size is linear with the degree of the polynomial. While in Kate Commitments, commitment and proof size is just one group element of an elliptic group. Commonly used pairing curve is BLS12-381, that would make proof size to be 48 bytes.\n- The verifier need to do linear work to compute $z$. While in KZG, verification is constant as only two multiplications and pairings are required to verify proof.\n- commitments using merkle trees makes the polynomial public and doesn't hide anything. While it's mostly hidden in Kate commitments.\n\nNow, the best part about Kate proofs are it can create one proof for multiple evaluation, i.e. only one group element for multiple proofs. [^1]\n\n### Multiproofs\n\nLet's say we have a list of $k$ points that we want to prove, $(z_0, y_0), (z_1, y_1), \\ldots, (z_k, y_k)$. We find a polynomial $I(X)$ that goes through these points using Lagrange Interpolation:\n\n$$I(X) = \\sum_{i=0}^{k-1} y_i \\prod_{j=0 \\atop j \\neq i}^{k-1} \\frac{X-z_j}{z_i-z_j}$$\n\nFor the evaluation proof, while in the single proof we compute $q(x) = \\frac{p(x)-y}{x-z}$, we will replace $y$ as single point with $I(x)$, and $x-z$ will be replaced with $Z(x) = \\prod_{i=0}^{k}x-z_i$. Now, we replace our values and $q(x)$ becomes\n\n$$q(x) = \\frac{p(x) - I(x)}{Z(x)}$$\n\nThis is possible due to $p(x)-I(x)$ being divisible by all linear factors of $Z(x)$ and being divisible by whole $Z(x)$. The multiproof for evaluation $(z_0, y_0), (z_1, y_1), \\ldots, (z_k, y_k)$: $\\pi = [q(s)]_1$. Now, our prover equation becomes\n\n$$e(\\pi,[Z(s)]_{2})=e(C-[I(s)]_{1},H)$$\n\n$$[q(s) \\cdot Z(s)]_{T} = [p(s)-I(s)]_T$$\n\nThis really blew my mind when I was first studying them. You can have a million proofs batch together in one single 48 bytes proof which anyone can verify by just computing $I(x)$ and $Z(x)$.  Cryptography really is cool.\n\n## Resources\n\n- [KZG Commitments By Dankrad](https://dankradfeist.de/ethereum/2020/06/16/kate-polynomial-commitments.html)\n- [Kate Commitments in ETH](https://hackmd.io/yqfI6OPlRZizv9yPaD-8IQ?view)\n- [ethresear.ch post about commitments](https://ethresear.ch/t/open-problem-ideal-vector-commitment/7421/27)\n- [Using polynomial commitments to replace state roots](https://ethresear.ch/t/using-polynomial-commitments-to-replace-state-roots/7095)]\n- [Kate Commitments: A Primer](https://hackmd.io/@tompocock/Hk2A7BD6U)\n- [Understanding KZG10 Polynomial Commitments](https://taoa.io/posts/Understanding-KZG10-Polynomial-Commitments)\n- [Polynomials in bit reversal permutation](https://github.com/ethereum/consensus-specs/pull/3006)\n- [Formulas for Polynomial Commitments](https://hackmd.io/@Evaldas/SJ9KHoDJF)\n- [Uncovering KZG](https://scroll.io/blog/kzg)\n\n[^1]: This is the first footnote.\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/rollups":{"title":"Making Sense of Rollups","content":"\n\u003e The butterfly sailed on the breeze\n\u003e \n\u003e Past a field of barbed wire trees\n\u003e \n\u003e Where golden dragons chased around\n\u003e \n\u003e Pampered poppies on the ground\n\nFirst of all, let's give you a banger to listen along this wild ride that I had while going deep and deep in this amazing rabbit hole of Rollups. The song is [Nine Cats](https://open.spotify.com/track/6M6S6DhGZZzmtTjP1iPvxb?si=d35d13e8c74e40ae) from Porcupine Tree. I listen to them quite often as Prog Rock is one of my favorite genres to listen, it's pretty wild yet sounds soothing to my ears.\n\n## Let's Dive in!\n\nModular Stack divided the monolithic state of a blockchain into four main parts:\n\n1. Data Availability (DA): making state and transaction data available to consumers cheaply and quickly\n2. Consensus: agreement over the transactions included in a block and their ordering\n3. Settlement: can vary between different implementations, but mainly refers to settling/validating of transactions on the chain through verifying/arbitrating proofs.\n4. Execution: computation of previous state → transaction → new state\n\n**Economic security** refers to large amount of monetary value locked into the layer that is being secured by the network. In the end, the most economical secure layer will accrue the most value as that's where most of the premium and profit resides.\n\n## Different Rollups\n\n![1_Modular-Stacks](thoughts/images/1_Modular-Stacks.jpeg)\n\nThere are [four](https://twitter.com/apolynya/status/1511623759786307586) main kinds into which Rollups can be categorised:\n\n1. Smart Contract Rollups: they use already decentralised, economic secure L1 for settling through the use of smart contracts for arbitrating proofs.\n2. Enshrined Rollups: in-protocol rollup which doesn't rely on smart contracts and is built into the L1 spec itself.\n3. [[thoughts/sovereign-rollups|Sovereign Rollups]]: doesn't use another L1 for settling, and only use another DA layer for data and ordering. These type of rollups have full control over their stack and can outperform other alt-L1s.\n4. Validium / Celestiums: these use off-chain DA solution for cheap DA and settle proofs on other external chain.\n\n## Why Rollups?\n\nBecause Ethereum in it's older monolithic form was not designed to scale for the demand of thousands of transactions in a second and thus, rollups provide a better environment for apps to exist, with better transaction pricing and scalability. And also provide better grounds for research and technological advancements to take place which can't happen in a monolithic system.\n\nAny system that wants to house many high valued assets has to capture the monetary premium in the form of fees or MEV to guarantee the economic security. Many believe that to be the settlement layer as that's where a transaction validity gets finalised.\n\nRollups use proofs to verify their transactions on the base layer and these come in two different forms:\n\n1. [[thoughts/fraud-proofs|Fraud Proofs]]: in case of ORUs\n2. Validity Proofs: in case of ZKRs\n\n## Rollup fees\n\n![14_Smart-Contract-Rollups.jpeg](thoughts/images/14_Smart-Contract-Rollups.jpeg)\n\nIn its current form i.e. pre *EIP-4844* and *Danksharding*, each rollup is essentially a *Smart Contract Rollup,* which posts its transaction data (in case of ORUs, ZKRs don't need to post complete transaction data), state roots along with proof data to L1 in the form of calldata to smart contracts. These SCRs have fixed cost in terms of state commitments and proofs that they submit and variable costs in the transaction data along with proposers signatures in case of ORs.\n\n\u003e Note: ZKRs don't need to submit transaction data and ORUs does is in case of fraud in ORUs, transaction data is needed to check the fraud but ZKRs along with validity proofs prove that the state diffs are valid. Thus, ZKRs post validity proofs every time to L1.\n\nOptimism currently uses two smart contracts at L1 that sequencer and proposer post to:\n\n1. Canonical Transaction Chain contract: append-only logs of transactions submitted by sequencer.\n2. State Commitment Chain contract: state roots proposed by the proposers for each transactions in CTC.\n\nPosting to both of these contracts incur a cost to L2s. Although most of these contracts will be archived sooner than later as Ethereum eyes its bigger upgrades and Rollups also upgrades to better infra in the form of ***bedrock*** for Optimism and ***Nitro*** for Arbitrum. Better DA layer, separate EIP-1559 fee market for rollup data will get rid of the smart contracts.\n\n![8_Value-Flows.jpeg](thoughts/images/8_Value-Flows.jpeg)\n\nBut with EIP-4844 and Danksharding looming, DA supply will overshoot and better compression from rollups as well will scale the TPS metric to an absurd amount. This brings to the question of value accrual as the value acquired as a DA layer won't be much if the DA supply isn't fully exhausted.\n\n|              | Target DA Bandwidth | Target Useful Data per block |\n| ------------ | ------------------- | ---------------------------- |\n| EIP-4844     | 83.33 KB/s          | 1 MB                         |\n| Danksharding | 1.33 MB/s           | 16 MB                        |\n\nWith Danksharding, Ethereum as a DA layer will be able to provide 1.33 MB/s data bandwidth with maximum economic security. Current Eth blocks average around 90KB with calldata being 10 KB of this. Rollups eventually want to optimise for 14 bytes/tx which with 1.33 MB/s bandwidth amounts for 100k sweet sweet TPS. Although, this should be kept in mind that these are best case numbers in a hyper-optimised rollup environment that will be implemented in a very long time horizon.\n\n## MEV in a modular stack\n\nWith MEV-Boost running in production with several relayers and in-protocol PBS as part of roadmap, this makes most of the MEV value to accrue to L1 base layer. Similarly, in a rollup environment, sequencers try to bid for block with maximum value from searchers which accrues most of the value to rollup layer. This is still a research area, with how much of the value that will get accrued to L1 DA/settlement layer or L2 stack. MEV value can get leaked to DA layer if it censors or delays settlement layer blocks.\n\nCelestia wants a small part of a bigger pie here due to optimising for the DA layer.\n\n## Attacks on DA Layer\n\n### 33% Attack\n\nThis leads to liveness failure in tendermint consensus and is a slashable event in Gasper consensus which leads to inactivity leak until the protocol can finalise again.\n\n### 67% Attack\n\nThere are certain validity condition in the consensus protocols such that even if 100% of the validators are dishonest, they can't just print native tokens out of nothing as the honest nodes won't accept the transactions and protocol can be forked using social consensus.\n\nAttacks that can happen if 67% of the protocol is malicious are:\n\n1. Double signing\n2. Data withholding\n3. Fraud proofs censorship\n\nThus, both DA and settlement layer is susceptible to frauds. Eventually, only that stack wins which accrues the most value i.e. most economically secure. Every layer has to be designed such that participants can capture a value of the transaction for self-sustaining environment otherwise, it leads to more centralisation. And, as it is put now, DA layer doesn't accrue much of the transactional value and it will be the case for a long time in the future.\n\n## Rollup Stack\n\n### SC Settlement Rollup \u0026 SC Recursive rollup\n\nOne question that comes to mind is why does Rollup has to use Ethereum as its Settlement layer? Because it's an economic secure network which behaves as an apt settlement layer due to its isolated execution environment that can arbitrate proofs submitted by the rollup. The smart contracts on Ethereum also serves as trust-minimised two way bridges between the rollup and the L1. It's trust minimised as *rollups operators* (proposers, sequencers, provers, challengers) due to the means of smart contracts doesn't need any third party to submit batches to the L1 and it's two way because L1 smart contracts receive block headers along with proofs and thus behaves as a light client.\n\nStarkware wants to create recursive smart contract rollups on top of Starkware L2, where L2 behaves as a settlement layer for L3s and more use-specific applications can be deployed as L3s on top of L2s. L2s verifier contract receives many validity proofs from L3s and recursively combines it into one proof and submit to L1.\n\n### Enshrined Rollups\n\nERs are what rollups would have looked like if an L1 supports them from the start, i.e. in-protocol support for verifying state changes. Currently, to verify if a state is valid, full nodes have to run all the transactions. Perfect game would be when the blocks or rollup batches have proofs attached and state validity could be proven. A single zkEVM which could verify the SNARK submitted with each block for state validity.\n\nAs the name mentions, ERs are currently a theoretical concept, meant to be part of the L1 itself in the spec rather deployed on top of smart contracts. So,\n\n- How does rollups settle to L1?\n\n   Full Nodes doesn't have to run all the transactions unless there's a fraud proof in case of ORUs, much better case in case of ZKRs where no re-execution happens due to provers providing validity proof with each batch.\n\n- Other thing that comes to mind is how will the in-protocol proofs would look like?\n\n   The simplest way for an L1 to support proofs within consensus layer is to re-execute the transactions with pre and post state roots. A zkEVM would get SNARK with every block as a sidecar or on-chain depending on the implementation.\n\n- What performance upgradation does in-house proof proving could provide?\n\n   With weak statelessness already part of the roadmap, full nodes wouldn't have to execute every transaction to check state validity due to SNARK coming with every block leading to simpler consensus logic. Removing compute bottleneck and statelessness reducing disk I/O also helps in raising the block gas limit. And bandwidth resource increases according to the Nielsen's Law. Also light clients can filter invalid state roots due to SNARKs much more quickly than fraud proofs.\n\n**Step 2** would be to deploy parallel zkEVMs ERs. These parallel ERs can verify separate SNARKs and then can settle to one main settlement rollup, performing like execution shards but better.\n\n![24.-Enshrined-Rollups_00288-1.jpeg](thoughts/images/24.-Enshrined-Rollups_00288-1.jpg)\n\nzkEVMs ERs have several benefits:\n\n1. **Social Alignment**: Follows social consensus\n2. **Secure**: No upgrade keys required\n3. **Economic**: maximum value accrual to ETH\n4. **Gas Efficient**: less gas cost as directly arbitrating proofs instead via SCs\n\nDisadvantages:\n\n1. Slow: has to go through vigorous protocol update phases\n2. Pre-Confirmations: Harder pre-confirmations as no centralised sequencers\n3. Less VM innovation\n4. Increased builder cost: more specialised builders that are able to verify the SNARK.\n\nThere are many arguments against zkEVM shards ERs potentially harming SCRs and innovation happening on it. Users wouldn't feel as comfortable using a private SCR and would instead choose a more embedded, easily verifiable ER, thus weakening these SCRs.\n\nThis could potentially lead to a rollup leaving L1 for their own to gain a bigger piece of the pie and gain more control over its users and stack. In my opinion, rollups doing this are just scared of competition, looking for profit and maximalism rather than what blockchains as a tool stands for and, that is, social coordination. ERs and SCRs could easily go hand-in-hand, with SCR housing the innovations in the ecosystem and ERs becoming the efficient, trust-minimised settlement layer.\n\nDA on its own doesn't accrue much value to the base layer, high-value transactions and massive liquidity is needed to attract more premium and value to the participants and that is achieved by being the best settlement layer. This could be achieved using zkEVM ERs. A successful settlement layer would only benefit the SCRs building on top, as more monetary premium leads to economic security and more trust a user has on the rollup. Rollups breaking away from the base layer would have to again fight the same battle of network effects, decentralisation, retaining users with fragmented liquidity and much lesser scaling options which is the whole point of rollup economics.\n\nEthereum as a base layer shouldn't just focus on rollups, but housing as many participants as possible with apps like Uniswap, Aave running and doing massive defi transactions on L1, SCRs, ERs.\n\nValidiums/Celestiums that use other chains as DA layers suffers from trust assumptions as there are two options for DA, off-chain and on-chain. Off-chain options could be fast and cheap but lacks sufficient decentralisation and On-chain options like Celestia could be a better option but if it goes down so does your chain relying on it for DA. Thus, rollups settling on same chain for DA and settlement avoid these issues as if Ethereum gets 51% attacked and reverts, so does the rollup settling to it. There are benefits that validiums provide that depends on the use case of data availability modes.\n\n\u003e Settlement alone then largely provides network effects, while DA adds on full security. These network effects are safely optimised by sharing DA.\n\n### Sovereign Rollups\n\nThese are another class of [[thoughts/sovereign-rollups|Rollups]] that doesn't settle to any other external layer and only use them for DA and transaction ordering.\n\n## Questions\n\n- What do we really mean by a settlement layer?\n- Does DA layer need economic security or its function is just to provide data cheaply and quickly?\n- Atomic cross-chain MEV?\n- Danksharding and EIP-4844 specs\n- IVG\n- ZK Bridging\n- Cevmos\n- statelessness\n\n## Readings\n\n- [[posts/Optimistic Rollups|ORUs]]\n- [The Complete Guide to Rollups - Delphi Digital](https://members.delphidigital.io/reports/the-complete-guide-to-rollups)\n- [Understanding rollup economics from first principles](https://barnabe.substack.com/p/understanding-rollup-economics-from)\n- [l2Beat](https://l2beat.com/scaling/tvl/)\n- [Modular war on Twitter](https://twitter.com/dystopiabreaker/status/1531102983894597632￼)\n- [EF Reddit AMA Enshrined Rollups](https://www.reddit.com/r/ethereum/comments/vrx9xe/comment/if11ljm/?utm_source=share\u0026utm_medium=web2x\u0026context=3)\n- [Sovereign Rollup Chains](https://blog.celestia.org/sovereign-rollup-chains/)\n- [Fraud Proofs Explainer](https://medium.com/infinitism/optimistic-time-travel-6680567f1864)","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/snark":{"title":"SNARKs","content":"\nProperties that any zero knowledge based proving system needs to entertain:\n\n1. Completeness: Any honest prover can convince verifier of the statement and witness.\n2. Soundness: Any malicious prover can never convince a verifier of the statement that it's trying to prove. In other words, if a prover doesn't know witness, it can never convince the verifier.\n\nAny proving system defines an n-variate polynomial with an evaluation recipe. Two parties involved in the process are **Prover** and **Verifier**.\n\n\u003e [!note] [What is the difference between proof and argument of knowledge](https://crypto.stackexchange.com/questions/34757/what-is-the-difference-between-proofs-and-arguments-of-knowledge)? SNARKS are succinct argument of knowledge: Mostly used interchangeably but in proofs the soundness holds against a computationally unbounded adversary while in argument of knowledge soundness only holds against polynomially unbounded adversary. Arguments are thus called *computationally sound proofs*.\n\n# SNARK\n\n![snark](thoughts/images/easy-snark.png)\n\nArgument system that generate **succinct** and **fast** proofs for public statement $x$ in $\\mathbb{F}^n$ and secret witness $w$ in $\\mathbb{F}^m$. The statement and witness need to be encoded in an arithmetic circuit $C$.\n\nArithmetic circuits: $C(x,w) \\rightarrow \\mathbb{F}$\n\nRequirements:\n\n1. Completeness: $\\forall x,w:C(x,w)=0 \\rightarrow Pr[V(S_{v},x,P(S_{p},x,w))]=1$\n2. Knowledge Sound: P doesn't know $w \\rightarrow Pr[V(S_v,x,\\pi)]=$ negligible\n\nMore formal definition:\n\n## Soundness\n\n\u003e [!tldr] For dummies like me: for every polynomial time adversary, there exists an extractor that uses adversary to find out about $w$, even though adversary doesn't know about $w$, because if it would, then adversary can generate arbitrary proofs.\n\n(S,P,V) is **knowledge sound** for a circuit if for every ***poly. time adversary A*** = (A_0, A_1) such that:\n\n$$S(C)\\rightarrow(S_{p},S_{v})$$\n$$(x,state)\\leftarrow A_{0}(S_{p})$$\n$$w\\leftarrow A_{1}(x,S_{p},state)$$\n\nthere is an efficient ***extractor*** $E$, that uses $A_{1}$ s.t.\n\n$$S(C)\\rightarrow (S_{p},S_{v})$$\n$$(x,state)\\leftarrow A_{0}(S_{p})$$\n$$w\\leftarrow E^{A_{1}(x,S_{p},state)}(S_{p},x)$$\n\n## Zero Knowledge\n\n\u003e [!tldr] For any normal proof, there exists a simulator which can generate proofs without knowledge of $w$.\n\n(S,P,V) is zero knowledge for circuit C if there is an efficient **Sim** s.t. $\\forall x\\in\\mathbb{F}^{n} \\rightarrow \\exists w:C(x,w)=0$, the distribution:\n\n$(C,S_{p},S_{v},x,\\pi):$ where $(S_{p},S_{v}):S(C)$, $\\pi:P(S_{p},x,w)$\n\nis indistinguishable from the distribution:\n\n$(C,S_{p},S_{v},x,\\pi):$ where $(S_p,S_{v},\\pi)\\leftarrow Sim(C,x)$\n\n## Proofs\n\nTwo ways to prove statement:\n\n- Interactive: prover and verifier goes back and forth to agree on some random values and parameters which verifier can then verify.\n- Non-interactive: Pre-processing done to generate a proof $\\pi$ that allows verifier to verify circuit.\n\nPreprocess argument system: prover preprocesses $C$ to generate public parameters $S_{p},S_{v}$.\n\nConsists of $(S,P,V)$:\n\n- $S(C)$: generate $S_{p},S_{v}$\n- $P(S_p,x,w)$: generate proof $\\pi$\n- $V(S_v,x,\\pi)$: verify proof\n\n\u003e [!question] How does $S_{p},S_{v}$ gets generated? Is it done for every circuit or some other method? Wouldn't it be very bad UX if we have to run $S$ every time?: Each circuit needs some summary that can be used to verify the proof as no verifier will run the whole computation again, otherwise there's no benefits of these systems.\n\nThere has to be some randomisation involved that prover can't forge by itself to generate these parameters.\n\n- First method is to generate random $r$ for every circuit outside prover.\n- Second is to generate a random value once $S_{init}(\\lambda,r) \\rightarrow pp$, create some public parameters based on that, use those parameters to generate $S_{index}(pp,C) \\rightarrow S_{p},S_{v}$\n- Totally transparent: no random secretness needed to generate circuit parameters.\n\n---\n\nPipeline followed by any ZKP system:\n\nDSL -\u003e circuit -\u003e prover -\u003e proving system -\u003e proof -\u003e verifer -\u003e yes/no\n\nAny circuit need to be proven to verifier, so we need to encode and evaluate our polynomial. Thus, most SNARK system consist of two components:\n\n1. [[polynomial-commitments|Functional commitment scheme]] (FCS)\n2. Interactive oracle proofs (IOP)\n\n## IOP\n\nReference: \u003chttps://nmohnblatt.github.io/zk-jargon-decoder/definitions/polynomial_interactive_oracle_proof.html\u003e\n\n![polynomial-iop](thoughts/images/polynomial-iop.png)\n\nMethod to convince verifer that the proof generated by prover is correct.\n\n$S(C): S_{p},S_{v}$ :- setup procedure of circuit\n\n$S_{v}=[f_{0}],[f_{-1}],[f_{-2}]\\cdots[f_{-s}]$ :- S_v consists of commitments to s+1 polys.\n\n\u003e - Why does S_v contains commitments to different polynomial?::maybe due to having different sets of polynomial for each part of circuit.\n\u003e - What if prover commits to a zero polynomial? i.e. what if $f(x) = 0 \\forall x\\in\\mathbb{F}_{p}$ :: then we can do zero test on polynomial. i.e. take a random point $r$ in $\\mathbb{F}_{p}$ and test whether it's zero or not. and prob. of being zero is negligible with value $d/p$ where d is the degree of polynomial. example: $p=2^{256}, d=2^{40}$\n\u003e - what if prover commits to same polynomial? :: do equality test, take $r \\in \\mathbb{F}_{p}$ if f(r)=g(r), then f=g with very high prob. **Not straightaway understandable to me tho.**\n\n- Prover sends t commitments to verifier\n- verifier sends t random values to prover\n- verifier runs a verify algorithm taking all the commitments and the random values, and previously generated setup summary S_v, i.e. s+1 commitments to polynomials.\n\t- It can ask prover to open any of the commitment at any point.\n\t- Verifier accepts/reject the proof.\n\n(t,q) Poly-IOP:\n\n- t: no. of commitments to polynomials\n- q: no. of eval queries in verify\n\n1. Prover sends t commitments\n2. Verifier verifies at q evaluations by running PCS eval.\n\n\u003e [!info]\n\u003e Length of SNARK: $t$ commitments + $q$ eval proofs\n\u003e\n\u003e Verifier time: q$*$O(eval)+O(IOP-verify)\n\u003e\n\u003e Prover time: t$*$O(commit)+q$*$O(prove)+O(IOP-prove)\n\n### PLONK: poly-IOP for General Circuit\n\nPlonk IOP can be used with different PCS to generate different proving systems. For example:\n\n1. Aztec's proving system: KZG+Plonk\n2. Halo2: Bulletproofs + Plonk, used by zcash\n3. Plonky2: FRI + Plonk, used by polygon zkevm\n\n#### Step 1: Compile Circuit for a Computation Trace\n\nSuppose you have a circuit with inputs $I: I_{x},I_{w}$, and gates C where $x$ is the statement inputs and $w$ is the witness input. Our goal is to prove to verifier that $C(w)=0$.\n\n![circuit](./images/example-circuit.png)\n\nWe put inputs $x_1=5, x_2=6, w_1=1$, and compute the computation trace through the circuit. The **computation trace** comes out to be:\n\n| inputs: |  5  |  6  |        1         |\n|:-------:|:---:|:---:|:----------------:|\n| Gate 0: |  5  |  6  |        11         |\n| Gate 1: |  6  |  1  |        7         |\n| Gate 2: | 11  |  7  | ==77== \u003c- Output |\n\n#### Step 2: Encode Trace in Polynomial $P$\n\n- Calculate $d$ of $P$ = $3|C|+|I|$ where |C|: no. of gates and |I|: no. of inputs\n- Aim: encode in a polynomial of form: $F_{p}^{(\u003c=d)}(X)$\n- Encode inputs: $P(w^{-i}) = I_{i} \\forall i \\in I$\n- Encode gate $l\\in C$ wirings:\n\t- $P(w^{3l})=$ left input to gate l\n\t- $P(w^{3l+1})=$ right input to gate l\n\t- $P(w^{3l+2})=$ output of gate l\n- Prover has $d$ evaluations of $P$, can use FFT to interpolate coefficients in $O(log_{2}d)$\n\n#### Step 3: Prove Validity of P\n\nWhat does prover needs to prove to verifier in order to convince it?\n\n- correct output: to prove that output of computation was according to the statements and witness committed.\n- correct inputs: proves\n- correct intermediate gate evaluations:\n- wiring between gates is according to the statement specified by prover\n\n### Proof Gadgets for IOP\n\n- **equality test** -\u003e to test if polynomial $f$ and $g$ are equal and note that verifer only has the commitment, verifier queries the polynomial at point $r$ or opens the commitment and test if values providied by prover are equal. But this generates soundness error.\n- soundness error -\u003e two polynomials in $\\mathbb{F}_{p}$ can be zero at atmost d roots, if f(r)-g(r)=0 =\u003e f-g=0 =\u003e f=g v.h.p\n\t- this assumption has error d/p such that if suppose g is not same as f, then g can have at most d different roots. thus, error -\u003e d/p\n\t- this is derived from [Schwartz-Zippel Lemma](https://courses.cs.washington.edu/courses/cse521/17wi/521-lecture-7.pdf)\n\t![kzg-proof-system](thoughts/images/kzg-proof-system.png)\n\nlet $\\Upomega$ be some subset of $\\mathbb{F}_{p}$ of size $k$. Let $f \\in \\mathbb{F}_{p}^{\u003c=d}[X]$ be the polynomial that prover wants to prove. Verifer has commitment to this polynomial $Com(f)$.\n\nWe can construct efficient poly-IOPs for the following tasks:\n\n1. Zero Test: Prove that $f$ is identically zero on $\\Upomega$\n2. Sumcheck: Prove that $\\sum_{a \\in \\Upomega}f(a)=0$\n3. Product check: Prove that $\\prod_{a \\in \\Upomega}f(a)=1$\n4. Permutation check: Prove that $f(a)=g(W(a))$ where $W$ is a permutation polynomial.\n\n\u003e [!question]\n\u003e - Why do we do these checks only? What is the significance of these checks? What other checks can be done? Can we remove some checks or combine some checks?\n\u003e - How does the permutation polynomial is calculated?\n\u003e - Find the reasons behind these checks? Could you have come up with these yourselves?\n\n\u003e Vanishing Polynomial of $\\Upomega$ is $Z_{\\Upomega}(X) := \\prod_{a\\in\\Upomega}(X-a)$. $Deg(Z_{\\Upomega})=k$\n\u003e If $\\Upomega$ becomes the multiplicative subgroup formed using primitive kth root of unity, then VP becomes $X^{k}-1$. Thus, VP can be calculated in logarithmic time.\n\n#### Zero Check\n\nLet $\\Upomega = 1, \\omega, \\cdots, \\omega^{k-1}$. Calculate $q(X) = f(X)/X^k-1$.\n\nSend $Com_{q}$ to verifier, and verifier opens commitment at point $r$, and check $f(r)=q(r)(r^k-1)$. This proves that f(x) is divisible by X^k-1, hence, f has roots in $\\Upomega$.\n\n#### Sum Check\n\n#### Product Check on $\\Upomega$\n\nWe want to prove $\\prod_{a\\in\\Upomega}f(a)=1$. Naively, we can send all evaluations of $f$ in $\\Upomega$ but that will be quadratic in degree d. Instead, we can create a polynomial $t\\in\\mathbb{F}_{p}^{\u003c=k}(X)$ that evaluates to 1 at $\\omega^k-1$.\n\nLet's try a toy example in sage taken from [here](https://crypto.stackexchange.com/questions/105983/prod-check-gadget-in-plonk-any-polynomial-which-satisfies-the-prod-check-seems):\n\n```sage\n// verifying that f(x)=c in GF(17) for w = 4\nF17 = GF(17)\nR17.\u003cx\u003e = PolynomialRing(F17)\nw = F17(4)\nc0 = F17(2)\nc1 = F17(3)\nc2 = F17(4)\nc3 = F17(5)\nc = c0*c1*c2*c3\npoints = [(w^0, c0), (w, c1), (w^2, c2), (w^3, c3)]\nR17.lagrange_polynomial(points)\n```\n\nSo, we can define polynomial $t$ s.t. $t(1)=1, \\enspace t(\\omega^{s})=\\prod_{i=0}^{s}f(\\omega^{i})$ for s=1,…,k-1. Now in order to prove product check, we'll prove:\n\n1. $t(\\omega^{k-1})=1$\n2. $t(\\omega.x)=t(x).f(\\omega.x)$ for all $x\\in\\Upomega$. This will be used to a zero test on t(X) so that verifier can get convinced that prover actually computed $t(x)$. This is proven using: $t(\\omega r)-t(\\omega)f(\\omega r)=q(r)(r^k-1)$.\n\n\u003e [!Note]\n\u003e The same product check holds for rational function as well, i.e. $f/g$. Prove this using: $t(\\omega x)g(\\omega x)=t(x)f(\\omega x)$\n\n#### Permutation Check\n\nIt is used to check that two polynomials $f, g$ are permutation of each other. Mainly, prover wants to prove that: $(f(1),f(\\omega),\\cdots,f(\\omega^{k-1}))\\in\\mathbb{F}_{p}^{k}$ is a permutation of $(g(1),g(\\omega),\\cdots,g(\\omega^{k-1}))\\in\\mathbb{F}_{p}^{k}$.\n\nCan be done by creating two polynomials $\\hat{F}=\\prod_{a\\in\\Upomega}(X-f(a))$ and $\\hat{G}=\\prod_{a\\in\\Upomega}(X-g(a))$ and doing product check on these two polynomials. How will the product check work? It's simply testing that $\\frac{\\hat{f}(r)}{\\hat{g}(r)} = \\prod_{a\\in\\Upomega}\\left(\\frac{r-f(a)}{r-g(a)}\\right)=1$.\n\n\u003e [!question]\n\u003e Why can't we do zero test to check that these polynomials are equal? ::\n\n#### Prescribed Permutation Check\n\nInstead of just proving the permutataion, prover wants to prove that $f(x)=g(W(x))$, where $W:\\Upomega\\rightarrow\\Upomega$ is a permutation if $\\forall i\\in[k]:W(\\omega^{i})=\\omega^{j}$ is a bijection.\n\n\u003e [!question]\n\u003e Why can't you use zero check here to check the two polynomials are equal? :: because g(W(y)) where y in $\\Upomega$ will become O(d^2) in prover time. We don't want quadratic time prover.\n\nObservation: If $(W(a),f(a))_{a\\in\\Upomega}$ is a permutation of $(a, g(a))$ then $f(y)=g(W(y))$. We prove this by defining bivariate polynomial:\n\n\u003e [!question]\n\u003e didn't understand why this $(W(a),f(a))$ and $a, g(a)$, if the thing that we're trying to prove is f=g(W).\n\n$$\n\\begin{aligned}\n\\hat{f}(X,Y)=\\prod_{a\\in\\Upomega}(X-Y.W(a)-f(a)) \\\\\n\\hat{g}(X,Y)=\\prod_{a\\in\\Upomega}(X-Y.a-g(a))\n\\end{aligned}\n$$\n\nWe do product check on these two polynomials such that $\\prod_{a\\in\\Upomega}\\left(\\frac{r-s.W(a)-f(a)}{r-s.a-g(a)}\\right)=1$, where $r,s$ is the input queried by verifier.\n\n#### Proving Validity of T(x).\n\nT(x) is the polynomial that encodes the computation trace of the circuit.\n\n#### Prove Inputs Were Correct.\n\nCreate polynomial v(y) on $\\Upomega_{inp}:=\\{\\omega^{-1},\\omega^{-2},\\cdots,\\omega^{-|Ix|}\\}\\subseteq\\Upomega$ and do zero test on:\n\n$$\nT(y)-v(y)=0 \\qquad \\forall y \\in \\Upomega_{inp}\n$$\n\n#### Prove Gate Evaluations Are Correct\n\nDefine a selector polynomial S(X) which is:\n\n1. $S(\\omega^{3l})=1$ if gate #l is an addition gate.\n2. $S(\\omega^{3l})=0$ if gate #l is a multiplication gate.\n\nThen, you define the polynomial:\n\n$$S(y)[T(y)+T(wy)]+(1-S(y))[T(y).T(wy)]=T(w^2y)$$\n\nthen it's just the zero check.\n\n#### Combining gate constraints into one equation\n\nYou can very easily combine gate constraints into one equation:\n\n$$\n\\begin{equation}\nQ_{L}(x)a(x)+Q_{R}(x)b(x)+Q_{O}(x)c(x)+Q_{M}(x)(a(x).b(x))+Q_{C}(x)=0\n\\end{equation}\n$$\n\nwhere, \n1. $Q_{L},Q_{R}$: selector polynomial created for evaluation for left, right of a gate respectively.\n2. $Q_{O}$: selector polynomial for addition gate. \n3. $Q_{M}$: selector polynomial for multiplication gate.\n4. $Q_{C}$: polynomial for constant in a gate, ex: public inputs of the circuit.\n5. $a(x),b(x),c(x)$: polynomial for left, input and output values of the gates.\n\n#### Prove Gate Wirings Are Correct.\n\nencode the wires as permutation polynomial and prove through permutation check that permutation is correct.\n\n$$T(y)=T(W(y)) \\qquad \\forall y \\in \\Upomega$$\n\nBut we have three different n-degree poly, namely $a(X),b(X),c(X)$, this \n\n## Recap\n\n![plonk-iop](thoughts/images/plonk-iop.png)\n\n## Improvements\n\n- Turboplonk: Custom gates\n- ultraplonk: turboplonk+ Plookup\n- hyperplonk\n- goblinplonk\n\n## References\n\n- [ZK Whiteboard sessions](https://youtu.be/h-94UhJLeck)\n- [Why and How ZK-SNARKS work?](https://medium.com/@imolfar/why-and-how-zk-snark-works-2-proving-knowledge-of-a-polynomial-f817760e2805)\n- [PLONK whitepaper](https://eprint.iacr.org/2019/953.pdf)\n- [Awesome PLONK](https://github.com/fluidex/awesome-plonk)\n- [ZKP MOOC L5](https://www.youtube.com/watch?v=vxyoPM2m7Yg)\n- [Vitalik's Understanding PLONK](https://vitalik.ca/general/2019/09/22/plonk.html)\n- [David Wong's Understand PLONK](https://www.cryptologie.net/article/527/understanding-plonk/)\n- [Plonk by hand](https://research.metastate.dev/plonk-by-hand-part-1/)\n- [Notes on Plonk Prover’s and Verifier’s Algorithm](https://hackmd.io/@aztec-network/ByiUK_Plt)\n- [Plonk toy implementation](https://github.com/fabrizio-m/TyPLONK)\n- [Plonk with Zac](https://www.youtube.com/watch?v=NqrVcDuQ8hM\u0026t=1669s): Might have the questions that I'm looking for, i.e. the reasons behind decision in the protocol.\n- [FAQs about PLONK prover and verifier algorithm](https://hackmd.io/@aztec-network/ByiUK_Plt)\n- [ZK Proving systems by Jonathan Wang](https://hackmd.io/@jpw/BJAf6spB9)\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/sovereign-rollups":{"title":"Sovereign Rollups","content":"\n![28_Sovereign-Rollups.jpeg](thoughts/images/28_Sovereign-Rollups.jpg)\n\nSRs are other type of rollups that doesn't use other L1 as settlement layer but just use external DA and consensus layer. Settlement is handled within the rollup with their own clients. Full Nodes and Light Clients are used to sample DA and fraud/validity proofs for verification. Celestia is a solution in this direction which wants to be the go-to layer for cheap DA, giving sovereignty to chains. A rollup that uses another chain as settlement layer is bounded by the consensus rules of that chain, and doesn't have complete control over its changes. SR nodes verify the correct canonical chain for themselves, relying just for data on the external layer. The trust minimised two-way bridge converts to a one-way bridge for SRs as no verification is happening from the base layer.\n\nOne major drawback of having a chain that can fork endlessly even with minority is that it hurts composability. Apps that rely on other apps for their functionality would also have to fork and follow the new rollup chain which is not feasible in a bigger ecosystem and is quite pointless if the apps prefer the older chain, then you are just left with a new fork which no one is using. Thus, SRs make more sense for specific and single purpose DApps. DeFi is the most significant use case for decentralised applications at the moment and most of it relies on other apps for composability and yields, it wouldn't make much sense for apps to just spin up their own rollup chain to have sovereignty away from the base layer.\n\nOne advantage that SRs flex is the flexibility of having their own execution environments tailored for the specific use case rollup chain is designed for. This is not the case with ERs or SCRs that only supports base layer native execution environment for arbitrating proofs.\n\n# Fraud Proofs in SRs\n\n![30_IVG.jpeg](thoughts/images/30_IVG.jpg)\n\n- Non-Interactive: simple flavour of proofs where the challenger submits the claim and the light clients execute the transactions completely to check if the claim is true.\n- Interactive: Challenger submits claim and responder responds to defend themselves. These entities play a **Interactive Verification Game** (IVG) with a referee.\n\nThere are two methods to distribute FPs over the network:\n\n- P2P layer: distributed via nodes to other nodes, faster light client finality, no censoring by L1 miners/validators.\n- On-chain inclusion\n\nCelestia is a dirty ledger that means it doesn't check for validity and there's no concept of finality and just focuses on putting the data out there. Now, if a challenger wants to prove a Tx is an invalid transaction, it needs to have all the related transactions that could prove the malicious transaction is invalid and thus, this again brings us to the same place of archival nodes having to store all of the transactions. To mitigate this, weak subjectivity assumption is introduced. SCRs challengers has to keep the transaction data for the challenging period while SRs challengers now need to keep the data for the weak subjectivity period.\n\nIVG also introduces synchrony assumptions where all the light clients have to be connected to the honest challengers in order to distribute proofs. Thus, SRs are likely to use single round FPs as light clients can then participate in the distribution of proofs.\n\n# Bridging in SRs\n\nSRs do add additional design considerations over bridging as the settlement is happening on the rollup side and not on the base layer.\n\nThere can be two types of bridges:\n\n- Committee-based: A validator set of source chain attest to the validity of a block that is being used by another chain. Not trust-minimised as committee can steal funds. IBC is an example of this.\n- Proof-based: Trust-minimised bridging where SRs can verify each other's state using proofs, much more complex than other types.\n\n   Two types of bridging settlement here:\n\n   - P2P settlement: where light clients are embedded in the chain and receive proofs over P2P network. Both SRs have bridging contract allowing for lock-and-mint mechanism.\n   - On-chain settlement: both rollups run light clients as smart contracts. proofs and transaction data stored on-chain in these contracts.\n\n   Two types of upgrade mechanisms possible:\n\n   - Static Bridging: In which the rollup $SR_A$ doesn't support $SR_B$'s execution environment and has to support it node software upgrades. Now, if $SR_B$ forks, $SR_A$ has to fork itself to support $SR_A$ execution environment again. Thus, social consensus or governance is needed to add bridge.\n   - Dynamic bridging: rollup's support each other execution environment, thus not needed to fork and can support other's proofs directly.\n\n   Two types of bridging possible in SRs:\n\n   - Pairwise Bridging: every rollup has a bridge to every other rollup which means $N^2$ bridges for N rollups.\n   - Hub-and-Spoke model: a central hub SR which aggregates all the liquidity and communication from all the rollups. Similar to what Cosmos Hub wants to be.\n\n# Aggregated ZK Bridging\n\nProblem: $SR_1 - SR_2$ through $SR_N$\n\n1. An aggregate prover would receive proofs off-chain $SR_2 - SR_N$, runs a light client for each chain.\n2. Combines proofs into one aggregated proof and includes it in a smart contract on-chain.\n3. $SR_1$ verifies proof in the same amount of time it would take to verify one proof.\n\n# ETH Sovereign Rollups\n\nWould be in effect when Ethereum implements data blobs carrying transactions which makes ETH to be used as a DA layer.\n\nTradeoffs:\n\n- Better liveness: gasper can retain liveness in 33% attack\n- Better economic security\n- slower finality\n- More overhead: no DAS in ethereum yet, thus can't run light clients and need to run full nodes.\n\n# Celestia Sovereign Settlement Rollup \u0026 SC Recursive Rollup\n\n![32_SSRSCR.jpeg](thoughts/images/32_SSRSCR.jpeg)\n\nSRs are specially designed for recursive rollups to live on top of. These recursive rollups use SRs for shared settlement establishing a two-way trust minimised bridge between the layers. Submits proofs, state updates and transaction data to SRs, which then batches these rollup blocks and post them to Celestia.\n\nThis reduces a lot of bridging overhead found in legacy SRs where $N_2$ bridges exist for each pair of rollups. Thus, this creates $N$ trust minimised bridges to communicate with a shared settlement layer. One could argue a restrictive more specific settlement layer that only allows specific operations to perform on top of or a general purpose settlement layers. A specific settlement layer would be cheap and easier to set up with less complexities but also comes at a UX degrade of not having many use-cases like DeFi pooling, dAMMs, etc.\n\nBut a separate settlement layer could pose more challenges for censorship and force transaction inclusion which is supported in SCRs. Participants can leave rollup to post their transaction to L1 anytime just trading off the cheap fees, or sequencers are forced to include transactions within a certain time period which will get decentralised with more sequencers joining eventually.\n\nBut how do you do this with shared SRs? If settlement layer is censoring a rollup, then there needs to be a way to submit transactions directly to DA layer and skip settlement layer altogether.\n\nLiveness risks are still associated as liveness failure of settlement layer will render the rollup useless, but not in the case of sequencers failure of rollup itself, as then you can settle to settlement layer directly.\n\n# Readings\n\n- [Rollups as Sovereign Chains](https://blog.celestia.org/sovereign-rollup-chains/)\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/tee":{"title":"TEE","content":"\n## Why?\n\nData is a really precious commodity in an environment where most of the decisions are taken based on information. Trusted Execution Environments give us the capability of executing application code with secrets in an environment where not even the host system can access the secrets and determine the access patterns. Most of the data processing has been moved from local systems to cloud providers in a remote physical location. Any security breach at these remote systems compromises users' secrets and can thus, confidential computing is required which reduces TCB and attack surfaces to a minimum.\n\n## Use cases \n\n1. Any personal data like Medical Records, Biometrics\n2. Password Manager\n3. Encryption keys\n\nThese data items in vocabulary of trusted execution environment are called ***secrets***. Even though there are ways of encrypting the data when it's getting transported, or static data in the server, but any processing on the data needs to happen on raw and unencrypted one. There are countless incidents where local systems or remote servers have been compromised, exposing the data to an attacker.\n\n\u003e [!info] \n\u003e A term that we'll come back to often is **Trusted Computing Base (TCB)**. It refers to the hardware and software components of a system that are required to run a particular program/software securely. It comprises of the OS, BIOS, firmware, drivers, software dependencies, etc. Any breach of security in TCB can compromise the executing program and reveal the secrets. \n\u003e\n\u003e Larger TCB means larger surface of area for attacks and more trust in the underlying components.\n\n\n## SGX\n\nSGX(Software Guard Extensions) is a technology which models the Trusted Execution Environment capabilities on hardware. Hence, making it easier to run programs in an encrypted hardware memory region on RAM. This includes protection from adversarial entity even with control of OS, BIOS. \n\n- Production level enclaves doesn't allow debuggers.\n- The memory region created for enclave is encrypted and unadressable from outside of trusted part of the application. No cpu instructions like calls, jump, register manipulation or stack manipulation cannot be used to enter the memory inside the enclave region.\n- Even if any malicious party is successful in tapping into the DRAM modules would only gain encrypted garbage. \n- Memory encryption is randomly changed periodically, i.e. every power cycle.\n- Any external attempt at accessing the memory inside the enclave is denied. \n\nThis gives protection from a variety of threats:\n\n1. Compromised Operating Systems\n2. Kernel Exploits\n3. Untrustworthy cloud services\n\n### Enclave\n\nProgram is run inside an encrypted region called **Enclave**. This is made possible by extending x86 architecture with new instruction sets. Enclaves provide confidentiality and integrity of the data inside the enclave through the use of **Enclave Page Cache** (EPC), which the system reserves at boot time. [^1]\n\n![memory-structure-enclave](thoughts/images/memory-structure-enclave.png)\n\nEnclaves are secure compartments but have boundaries to determine trusted and untrusted part of the application. Trusted Part contains the sensitive piece or secrets of the enclave's code and untrusted part containing other parts of the program. The enclave memory is a volatile memory that gets removed whenever system goes to sleep, machine is destroyed, or application exits. \n\nSGX has two calls used to communicate between the two parts:\n\n1. OCalls: Trusted -\u003e Untrusted\n2. ECalls: Untrusted -\u003e Trusted\n\n\u003e [!hint]\n\u003e what if we need secrets of our applications between successive builds of our enclave, generating the secrets every time would be a waste of the resources. That's why we need a way to securely transfer secrets between enclaves. This is done using **Data sealing**.\n\n### Data sealing\n\nThe main aim of SGX is to not let raw secrets spill out of the enclave, and many misunderstand enclaves to be blackboxes where nothing is accessible. Rather, it's possible to encrypt data in the enclave using the encryption keys with one of the two policies defined in SGX:\n\n1. ***MRENCLAVE***: sealing happens on the enclave using the key derived specific to that particular enclave, and can only be decrypted on the same enclave, on the same machine.\n2. ***MRSIGNER***: sealing happens using key specific to the developer signing key on the system, and thus unsealing of data occurs when accessed by other enclave started by the same developer signing key.\n\n\u003e [!hint]\n\u003e how I as a user can be sure that there is no adversary manipulating the untrusted system and data integrity is intact. Attestation helps any entity to verify the integrity of the enclave and untrusted party to gain trusted party's trust.  \n\n### Remote attestation\n\nRemote attestation is an independent and untrustworthy party, namely prover(enclave) verifying to a trusted entity, namely verifier, the current state of the enclave to gain the relying party's trust. Can be done in two ways, Hardware-based and Software-based. A simple software-based example could include, prover sending memory hash after the execution of an application and the verifier verifying that the execution has indeed happened correctly and that no tampering has been done with the data.[^2]\n\n1. Software-based RA is when the prover provides proof of its correct state\n2. Hardware-based RA is the use of specialised memory regions inside the hardware that makes it difficult to tamper the secrets.\n\nAfter the prover has attested to the remote party that it is indeed running the correct application on a SGX enabled processor in an enclave, both parties can then continue with exchanging information and secrets with each other. This is how secrets are supplied inside the enclave using secure channels.\n\nIntel provides a pseudo-filesystem[^3] inside enclave as `/dev/attestation` which exposes all the necessary files related to attestation for local or remote verifiers.\n\nLet's understand attestation flow used in SGX:\n\n![epid-based-attesation](thoughts/images/epid-based-attestation.png)[^4]\n\n1. Application requests report data from `/dev/attestation/user_report_data` which internally calls `EREPORT` hardware-instruction call to write enclave report.\n2. Calls `/dev/attestation/quote`, reads `/dev/attestation/user_report_data`, sends to Quoting Enclave.[^]\n3. Quoting enclave reads the EPID key provided by Provisioning Enclave at the time of deployment.\n4. Provisioning Enclave fetches EPID key from Intel Provisioning Service, which is a remote trusted server by Intel.\n5. Quoting enclave generates quote along with report from the `/dev/attestation/report`, and sends to enclave.\n6. Enclave sends the quote to the remote verifier on request.\n7. Verifier sends the quote to Intel Attestation service that checks whether the quote was generated by the enclave or not and sends the result back to verifier.\n8. Verifier then verifies the quote metadata and SGX enclave measurements against local measurements like the policies MRENCLAVE and MRSIGNER are the ones that the user knows, are the architectural enclaves up to date, the quoting enclave's identity is correct. \n\nData Center Attestation Primitives are attestation services that doesn't utilise Intel's attestation service instead have their own ECDSA attestation certificates in a remote data center. Also the Quoting Enclave doesn't talk with Provisioning Enclave but with Provisioning Certificate Enclave which in turn calls Intel Provisioning Certificate service to get the attestation collateral.[^]\n\n![dcap-based-attestation](thoughts/images/dcap-based-attestation.png)\n\nThese certificates are cached at remote verifier and the remote user doesn't need to check with Provisioning service each time a quote arrives, and then periodically fetched to update.\n\n### Example\n\nPassword Manager\n\nSecrets:\n\n1. Vault file\n2. account passwords\n3. account info\n4. primary key\n5. encryption key\n\n1. Encrypted vault file using an encryption key derived \n2. User's master key is derived from passphrase which is generated using KDF that uses SHA256.\n3. Primary key used to encrypt passwords is generated randomly using RDSEED instruction\n\n1. \n\n### Tradeoffs\n\n1. Enclave Transitions: ECalls and OCalls b/w the different parts of the program inflict severe overhead on the performance due to security checks done at the boundary. Not all system calls are allowed inside enclave, thus OCalls are executed.\n2. EPC limit: Each enclave has a fixed size, and is divided in pages. Any enclave that exceeds the EPC's size limit has to be swapped out to DRAM and is encrypted in doing so. This induces an overhead of approx 3x.\n3. Due to the confidentiality of the data involved, the program has to be designed very carefully. Unoptimised designs lead to poor performance and prone to many attacks.[^5]\n\n## LibOS\n\nThe program needs to be modified in order to make it compatible to run inside SGX environment. LibOSes help here as they allow to run unmodified programs inside the SGX environments.\n\n\u003e [!note] Common things to keep in mind:\n\u003e 1. It's all about secrets and efficient communication b/w enclaves\n\u003e 2. \n\n## Resources\n\n- [https://01.org/sites/default/files/documentation/intel_sgx_developer_guide_pdf.pdf](https://01.org/sites/default/files/documentation/intel_sgx_developer_guide_pdf.pdf)\n- [https://sgx101.gitbook.io/sgx101/sgx-bootstrap/sealing](https://sgx101.gitbook.io/sgx101/sgx-bootstrap/sealing)\n- [https://blog.quarkslab.com/overview-of-intel-sgx-part-2-sgx-externals.html](https://blog.quarkslab.com/overview-of-intel-sgx-part-2-sgx-externals.html)\n- [Remote Attestation](https://encyclopedia.pub/entry/7912)\n- [sgx.fail](https://sgx.fail)\n- [SGXoMeter](https://www.ibr.cs.tu-bs.de/users/mahhouk/papers/eurosec2021.pdf)\n- [Quote verification with Intel SGX DCAP](https://www.intel.com/content/www/us/en/developer/articles/technical/quote-verification-attestation-with-intel-sgx-dcap.html)\n- [establish an SGX enclave](https://medium.com/magicofc/establish-an-intel-sgx-enclave-c6208f820ff9)\n- [difference b/w trusted computing and confidential computing](https://stackoverflow.com/questions/63335341/what-is-the-difference-between-trusted-computing-and-confidential-computing)\n- [Intel SGX Demystified](https://medium.com/obscuro-labs/intel-sgx-demystified-757a242682a3)\n\n[^1]: How does the Enclave provide integrity of data? What does it mean to provide integrity?\n[^2]: What are the attacks that are possible due to poor design?\n[^3]: pseudofs is a virtual file system where files are generated virtually on the fly when it's used by system calls like open, read, write, close, etc.\n[^4]: How can ZK Proofs help here? Or does it not even require zk proofs?\n[^5]: Source: [Gramine docs](https://gramine.readthedocs.io/en/stable/attestation.html)\n[^6]: Don't understand what these provisioning certificates are.","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/tendermint":{"title":"Tendermint","content":"\nTendermint is the consenus protocol used by Cosmos to reach consensus in a distributed system.\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/uncovering-celestia":{"title":"Uncovering Celestia","content":"\n\u003e Privileged.\n\u003e\n\u003e A Chosen Few.\n\u003e\n\u003e Blessed With our time in Hell.\n\nAnother song today would be [Hourglass](https://open.spotify.com/track/2pd1Lm8Jsslf2VdWQv0Je8?si=5b90339ee6804f32) from Lamb of God. I discovered this band when I just started listening to Metal and was stealthily going around the woods but listening to this song just instils a strange sense of rage in you.\n\nAnywas, these are my thoughts towards an amazing piece of tech, redesigning the [[thoughts/rollups|Rollups]] architecture from ground up, with several challenges in front of them.\n\n# Let's Dive In\n\nCurrent problems are Monolithic chains. Celestia answers what minimum a blockchain can do to provide shared security to other blockchains (rollups)?\n\n\u003e Celestia combines the best of both worlds: Ethereum’s Rollups with its shared security and Cosmos’s sovereign interoperable app-chains.\n\nIn a series of transactions, a blockchain cares about two things, *consensus* and *validity*.\n\n**Validity** rules determine which transactions are considered valid to be included in the blockchain while **Consensus** rules determine the order of the valid transactions to be included.\n\nCelestia uses [[thoughts/tendermint|Tendermint]] as its consensus protocol and is only responsible to enforce consensus rules. It doesn’t care about transaction validity nor is it responsible to execute them. Rollups on top of celestia monitor for transactions important for them, download and execute them accordingly.\n\n![9448d3db-ignore.png](https://storage.googleapis.com/members-portal-bucket/uploads/2022/02/9448d3db-ignore.png)\n\nQn: If execution doesn’t happen on celestia core, then does every app running on top of celestia has to execute each transaction and how does that help solve scalability problems found in other monolithic chains?\n\n# Scalability Bottleneck\n\nMost common issue suffered by major blockchains is **state bloat,** i.e. growth of nodes storage requirements with each added transactions to the history. Solution to this problem is existence of light clients, which doesn't download complete block data but only the block header and represents 99% of the users.\n\nLight clients, however are not able to tell if a tx is invalid or not as it doesn’t have the data and works under the assumption that consensus nodes are honest. As the state continues to increase, full nodes start decreasing and light nodes start increasing leading to more centralization around full nodes.\n\nFraud/Validity proofs negate the occurrence of this phenomenon as any light client can run these succinct proof to verify whether the contents of the blocks are valid or not.\n\n# How Fraud/Validity Proofs Work\n\n![02571c37-simplified-fraud-proof.png](https://storage.googleapis.com/members-portal-bucket/uploads/2022/02/02571c37-simplified-fraud-proof.png)\n\nFull nodes can provide light nodes with just enough data for them to verify that a particular transaction in included in block or not. This happens using merkle trees, which can efficiently prove that a particular tx is included w/o requiring them to download the whole block. So, these full nodes can provide just a subset of data and these light clients can verify against that.\n\nBut for full nodes to generate *fraud/validity proofs*, they need access to complete block data to run these transactions. If a malicious miner withholds block data, then light client won’t notice and continue to follow the chain as full nodes won’t be able to provide any proofs for the invalidity of that block. Thus, **Data Availability** is a necessary condition for fraud/validity proofs to exist.\n\nCelestia solved this problem by making data availability a necessary rule such that this can be enforced even by light nodes through `Data Availability Sampling (DAS)` .\n\n# Data Availability Sampling\n\nDAS is implemented in celestia using a technique called `Erasure Coding` , which is a data protection technique used at various places. Erasure coding to a data extends the data such that complete data can be recovered using just a fraction of the extended data.\n\nLight nodes randomly sample small fixed-sized chunks of block data from the block and have probabilistic assurances that other chunks have been made available to the network.\n\n![a4e60aab-eithercase.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/CD732BFD-616B-4A45-BDB8-132508E3914F_2/OKzZK6rzyYtwYyNf0SI2pK0Bpv03SG6uzxxRTPczC78z/a4e60aab-eithercase.png)\n\nWhen light clients start sampling the chunks of data along with the merkle proofs of inclusion of data in the block, from the block producer, it has two options:\n\n1. To make the data available, in which case enough light clients can request a significant amount of chunks such that full nodes can recover the whole block with that data.\n2. Data is not available, in which case full nodes are not able to recover the block data and light clients stop following the chain as the sampling process was not completed.\n\nThis results in liveness failure of the blockchain and the nodes stop following the chain. DAS guarantees that data availability is the necessary condition for blockchain to resume block production with both full and light nodes under same security assumptions.\n\n# How Much Can Celestia Scale\n\nSome tradeoffs exist in DAS light nodes, i.e. the block header grow in proportion to the square root of block’s size. For any blockchain to scale, it means processing higher throughput → processing more transactions → either more blocks or larger blocks. Size of a block depends upon\n\n- Amount of data that can be collectively sampled\n- Target block header size a light node can process.\n\nNow, the aha moment of light clients comes in play here. Running a light client is as easy as running an app on a mobile phone and thus, as the demand of the blockspace rises, more and more nodes join the network and network bandwidth to sample the data rises and thus block size rises with it. This means, unlike monolithic blockchains which starts to congest as demand rises, modular blockchain flourish and can easily support the rise in demand along with stable low fees.\n\nThe second part where the size of the block header rises with rise in block size can be compensated with the rise in bandwidth which makes it easier for celestia throughput to grow.\n\n# Benefits\n\n1. **Self-sovereign blockchains**\n\nSelf-sovereignity represents entity which has complete control of his/her data.\n\nCurrent Ethereum rollups post tx data along with proofs to L1 in the form of smart contracts, thus there has to be a way of on-chain governance mechanisms. This is not fully secure and decentralised to operate a system trying to support loads of high amount of throughput and handling significant amount of value. Celestia differs here in the sense that it doesn’t make any sense of the data and simply stores valid transactions, its the rollups that has to make sense of the required data and thus, has to make its own canonical chain.\n\nThis makes it significantly easier for any rollup to perform hard/soft forks which are regarded as a threat to any current monolithic blockchains. These forks dilute the security of the network and reduces it’s users faith on the underlying. But Celestia rollups can freely change the algorithm it uses in its nodes to make sense of the data without any threat of security failure because the DA and consensus layer remains the same with all the valid transactions.\n\n1. **Flexibility**\n\nSince celestia can freely take the roles of DA and consensus layer for rollups, it is much easier for any rollup to have its own execution environment with native VMs specially made for their use case. Thus, the VM market becomes open for any players to spin up flexible execution environments.\n\n1. **Effortless deployment**\n\nDeploying a blockchain has become easier and easier with the passage of time and significant improvements in chain architectures. Cosmos’ SDK is the prime example of how much easier it is to spin up their own app chain without writing millions of lines of node codes.\n\nCelestia takes it a step further with rollups not having to worry about the validators it needs to bootstrap a blockchain. New rollups can be deployed with the click of a button having same security assumptions as the oldest rollups on the network.\n\n1. ### Efficient Resource pricing\n\nIn ethereum scenario, rollups post data on to L1 which charges gas for each byte that is stored on-chain. This is then susceptible to L1 gas variations and cause fee spikes in L2s as well. Celestia, however only charges for historical state data that is stored as blockspace in bytes per sec and active state execution is handled by each rollup environment. Thus, spikes in one environment doesn’t affect another.\n\n1. **Trust minimised bridges**\n\nL1 can’t communicate with each other as there are no ways for one chain to verify state of another chain as they can’t execute the fraud/validity proofs of each other. Whereas rollups can form trust minimised bridges with each other as they all share same security. Ethereum has access to all rollup data and can execute their proofs. This allows them to create trust minimised bridges or trusted bridges depending on the requirements of rollups.\n\n1. **Faster and minimal governance**\n\nSeparation of stacks also separates the amount of required to execute any improvements proposals as each layer can focus on themselves and iterate faster and much more effectively without worrying about the DA or consensus layer.\n\n1. **Decentralised block verification, not production**\n\nIt doesn’t matter if in the long run various external factors tend to make block production centralised if you have sufficient amount of verifiers that can easily verify validity of a chain and continue to uphold the consensus rules to make them an efficient trustless machines.\n\n1. **Simplicity**\n\nCelestia made simpler architectural decisions that let them reduce any unnecessary tech burden and let them iterate and develop faster.\n\n# Challenges\n\n1. Bootstrapping execution environments: celestia as DA layer has no use if there are no execution environments on top of celestia to take advantage of all the DA throughput.\n2. Determining appropriate block sizes, although celestia has the property of scaling data sampling as more number of nodes join the network, but there are no mechanisms to determine number of light clients in the network. Also, light clients cannot be rewarded for data sampling processes, thus, has no incentives.\n\n![cevmos-celestiums.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/EF825D6C-48E0-46D5-85B6-F4FE7DBDD949_2/CfHFGvj6vjieQD4EScjkPaAlunvQm3o1bbkJDOGB3JQz/cevmos-celestiums.png)\n\n1. Third, is the limitation of Celestia native token utility because DA layer is not where value accrual in a blockchain takes place. Value accrual in a blockchain occurs mostly at settlement layer as no that’t where state execution takes place. Celestia’s native token will have to be moved to settlement layer using trusted bridges.\n\n# Questions\n\n1. Details about erasure coding.\n2. sparse merkle trees\n3. light clients\n\n# Readings\n\n- [Pay Attention To Celestia - Delphi Digital](https://members.delphidigital.io/reports/pay-attention-to-celestia)\n- [A note on data availability and erasure coding · ethereum/research Wiki](https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding)\n- [https://arxiv.org/pdf/1809.09044.pdf](https://arxiv.org/pdf/1809.09044.pdf)\n- [A note on data availability and erasure coding · ethereum/research Wiki](https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding)\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/verkle":{"title":"Verkle Trees","content":"\n![verkle_trie](thoughts/images/verkle_trie.svg)\n*Credits: Dankrad's post on verkle tries*\n\nVerkle Trees is one of the big change that is going to be included in the Ethereum protocol towards its push to weak statelessness. There are much better [resources](https://dankradfeist.de/ethereum/2021/02/14/why-stateless.html) to understand the reasoning behind going for statelessness.\n\nIn a Verkle trie, inner nodes are $d$-ary vector commitments to their children instead of hashes like a merkle trie. The root of a leaf is hash of the (key, value) pair, and root of an inner node is hash of the vector commitment. So, naively to prove value of a leaf, you only require $\\log_{d}n - 1$ values.\n\n- tree of some depth $d$\n- to prove leaf node $x \\rightarrow y$\n- need $(d-1)\\log_{d}n$ proofs/hashes to prove a leaf in case of merkle trees\n- But for verkle tries, proofs are in terms of vector commitments i.e. $f(z_{i})=y_{i}$. Polynomial commitments most efficient and simple vector commitments\n- thus require, $\\log_{d}n-1$ commitments and proofs\n- each commitments would me in terms of a polynomial, $f_{i}(X)$ where commitment $C_{i}=[f_i(s)]_{1}$\n- domain in terms of $\\omega$, i.e. $d$th root of unity.\n\nBut fortunately in case of polynomial commitments, instead of giving commitment and proof for each level, we can combine the proof and give a multiproof that convinces the verifier that value of a leaf node is indeed what we want to prove. Following is the steps of a scheme that generate multiproof using random evaluation.\n\n# Proof\n\nGiven $m$ commitments $C_i=[f_{i}(s)]_1$, prove evaluations $f_{i}(z_{i})=y_{i}$ where $z_{i} \\in \\lbrace{\\omega^i\\rbrace}$ and $\\omega$ is a $d$-th root of unity.\n\n1. Generate $r\\leftarrow H(C_{0},\\ldots,C_{m-1},y_0,\\ldots,y_{m-1},z_0,\\ldots,z_{m-1})$\n2. Prover encode all proofs in a polynomial $g(X)=r^{0}\\frac{f_0(X)-y_0}{X-z_0}+r^1\\frac{f_{1}(X)-y_{1}}{X-z_{1}}+\\ldots+r^{m-1}\\frac{f_{m-1}(X)-y_{m-1}}{X-z_{m-1}}$\n\n   \u003e Now, we just have to prove that $g(X)$ is indeed a valid polynomial (not a rational function). This can be done by committing to this polynomial $D$, evaluating it at a random point $t$ and verifier verifying it.\n\n3. Prover create a commitment $D=[g(s)]_1$\n4. evaluate $g(X)$ at point $t$, where $t \\leftarrow H(r,D)$\n   $$g(t) = \\underbrace{\\sum_{i=0}^{m-1} r^i \\frac{f_i(t)}{t-z_i}}_{g_1(t)} -  \\underbrace{\\sum_{i=0}^{m-1} r^i \\frac{y_i}{t-z_i}}_{g_2(t)}$$\n5. $y=g_{2}(t)$ can be computed by verifier as all the inputs are known\n6. $h(X) = \\sum_{i=0}^{m-1}r^{i} \\frac{f_{i}(X)}{t-z_{i}}$ =\u003e $E = [h(s)]_{1} = \\sum^{m-1}_{i=0} \\frac{r^{i}}{t-z_{i}} C_i$ can also be computed by verifier\n7. Prover gives the proof $\\pi=[(h(s)-g(s)-y)/(s-t)]_1$\n8. Verifier verifies via $e(E-D-[y]_{1},[1]_{2})=e(\\pi,(s-t)_{2})$\n\nThis allows us to prove an arbitrary number of evaluations.\n\n- Proof is also constant size (one commitment $D$, one number $t$, two proofs).\n- $x_{i}$, $z_{i}$ values do not need to be explicitly provided\n- Only leaves keys and values, and corresponding commitment to each level are required\n- For $n = 2^{30}$ and $d=2^{10}$, the average depth comes out to be $3$.\n\n## Design Goals\n\n- Cheaper access to neighbouring code chunks and storage slots to prevent thrashing\n- distribute data as evenly as possible in the tree\n- Fast in SNARKs\n- Forward compatible, i.e. interface should be pure (key, value) pair of *32 bytes*.\n\n## Reasoning for Pedersen Commitments\n\nNow, there are two commitment schemes that can be used for vector commitments.\n\n1. [[polynomial-commitments|KZG Polynomial commitments]]\n2. [[thoughts/pedersen-commitments|Pedersen Vector commitments]] + IPA (Inner Product Arguments)\n\nEthereum has decided to go with Pedersen commitments citing following advantages:\n\n1. No trusted setup\n2. Everything can be done inside a SNARK as well\n3. Developed new curve (Bandersnatch) that “fits inside” BLS12_381 (outer field = curve order)\n4. Future proof for witness compression and zk-rollups\n\n**Disadvantage**: Not as efficient for proving a single witness. But this is not very important to us.\n\n## Changes in Tree Structure\n\n![verkle_tree_structure](thoughts/images/verkle-tree-structure.png)\n\nVerkle Trees introduces a number of changes in the tree structure.\n\n1. Shifting from 20 bytes keys to 32 bytes\n2. tree width from hexary to 256\n3. vector commitments instead of hashes\n4. merge of account and storage tries\n5. gas changes\n\n# ETH Verkle Explainer\n\n![verkle_tree](thoughts/images/VerkleTree.jpg)\n\n# Resources\n\n- [Vitalik's explainer on Verkle Trees](https://vitalik.ca/general/2021/06/18/verkle.html)\n- [verkle.dev](https://verkle.dev/docs/intro)\n- [Verkle Tree EIP](https://notes.ethereum.org/@vbuterin/verkle_tree_eip)\n- [Dankrad's Peep an EIP](https://www.youtube.com/watch?v=RGJOQHzg3UQ)\n- [Verkle Trie reference implementation](https://github.com/ethereum/research/tree/master/verkle_trie_eip)\n- [Verkle tree structure](https://blog.ethereum.org/2021/12/02/verkle-tree-structure)\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null},"/thoughts/zeroes-and-poles":{"title":"Zeroes And Poles","content":"\nIn complex analysis, functions are studied that are differentiable (in the complex space) at almost all points. Zeroes and poles are very important concept.\n\n**Zeros** are termed as points at which a function $f, f(z) = 0$.\n\nA **Pole** (also called an *isolated singularity*) is a point where the limit of a complex function inflates dramatically with polynomial growth.\n\nMore specifically, a point $z_0$ is a pole of a complex valued function f if the function value $f(z)$ tends to infinity as $z$ gets closer to $z_0$. If the limit is finite, then $z_0$ is not a pole.\n\n$z_0$ is a pole of order $n$ if:\n\n$$\\lim\\limits_{z \\to z_0} (z-z_0) \\cdot f(z) \\neq 0$$\n\nWhat is done here is $f(z)$ is multiplied by $(z-z_0)^n$ and then taking limit as $z$ approaches $z_0$. If the result is not equal to $0$, then $z_0$ is a pole.\n\n# Order of Zeros\n\n# Resources\n- \u003chttps://klwu.co/knowledge/ec-basics-3-divisors/\u003e\n","lastmodified":"2023-07-24T05:57:53.004605184Z","tags":null}}