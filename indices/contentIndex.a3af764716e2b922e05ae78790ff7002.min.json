{"/":{"title":"ðŸ—¡ lonerapier.xyz","content":"\nI always wanted a corner of my own on this wild zoo that we call internet, and finally made one. I'm Sambhav and I try to write as much as I can to slow down my brain. It's mostly my thoughts and notes which I occassionally write after listening to a podcast.\n\nPresently, I work as a Software Engineer at an [cred.club](https://cred.club) startup and have discovered my love for complex engineering and writing. I am a big fan of OSS and try to contribute as much as I can to some mind-blowing projects made and maintanined by truly amazing folks.\n\nMy curiosity generally takes me to some unseen and uninviting places. Currently, it's all about crypto and web3.\n\nI'll try to mold this website into something of my own.\n","lastmodified":"2022-10-06T10:43:51.428591814Z","tags":null},"/posts/ECC":{"title":"Elliptic Curve Cryptography","content":"\n\n## Elliptic Curve Cryptography\n\n$$ y^2 = x^3 + ax^2 + bx + c $$\n\nsecp256k1: used by Bitcoin and Ethereum to implement public key cryptography. Elliptic curve over a field $z_p$ where $p$ is a 256-bit prime.\n\nECDSA: Elliptic Curve Digital Signature Algorithm\n\nPublic key cryptography uses this method to calculate public keys which is a point on ECC curve.\n\n$$ K = (k * G) \\% p $$\n\n- K = 512-bit public key\n- k = 256-bit randomly generated private key\n- G = base point on the curve\n- p = prime number\n\nTake a base point $G$, add it $n$ (private key) times to make $nG (\\% p)$ (public key).\n\n\u003e **Note**: addition here means addition in elliptic curve and not addition in field of integers mod p.\n\nOrder of a base point is when keys generated using this point starts to form a cycle. Max number of points on the curve.\n\nThus, choosing a good base point is necessary in any public key generation curves.\n\nsecp256k1:\n\n```other\nx = 0x79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798\ny = 0x483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8\np = 0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F\n```\n\nThe order is:\n\n```other\nN = FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141\n```\n\nEthereum public keys are serialization of 130 hex characters\n\n```other\n04 + x-coord (64) + y-coord (64)\n```\n\n\u003e 04 is prefixed as it is used to define uncompressed point on the ECC.\n\nEthereum addresses are hexadecimal numbers, identifiers derived from the last 20 bytes of the Keccak256 hash of the public key.\n\n## EIP55\n\nMixed capitalisation of letters in the address\n\nTake keccak256 hash of the address, capitalize character if hex digit of hash is greater than 8.\n\n## Discrete Log Problem (DLP)\n\ndescribes that there are currently no known method for calculating point division on an elliptic curve.\n\n### Why Discrete logarithm?\n\nECC is significant because solving $k * G$ is trivial but obtaining $k$ from product $k * G$ is not.\n\nk*G can be obtained using Fast-Exponentiation algorithm but solving for k requires computing discrete logarithms.\n\n## Security\n\nBig-O Notation of discrete logarithm problem is $O(\\sqrt{n})$.\n\nBase point G, is chosen to be closer to $2^{256}$ and thus is in the order of 256.\n\nSo, $\\sqrt{256} = 128$ bits level of security is provided by curves like secp256k1.\n\n## secp256k1 v/s secp256r1\n\nsecp256k1 is a Koblitz curve defined in a characteristic 2 finite field while secp256r1 is a prime field curve.\n\nNot going into details as to what a characteristic 2 finite field is, we can specify secp256r1 as a pseudo-randomised curve and secp256k1 as completely random curve which canâ€™t be solved using discrete logarithm problem **yet**.\n\n## BLS Signatures\n\n\n## To-Read\n\n1. [https://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627](https://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627)\n2. [https://eth2book.info/altair/part2/building_blocks/signatures](https://eth2book.info/altair/part2/building_blocks/signatures)\n\n## Resources\n\n- [Group Theory | Brilliant Math \u0026 Science Wiki](https://brilliant.org/wiki/group-theory-introduction/)\n\n- [Picking a Base Point in ECC](https://medium.com/asecuritysite-when-bob-met-alice/picking-a-base-point-in-ecc-8d7b852b88a6)\n\n- [Whatâ€™s The Order in ECC?](https://medium.com/asecuritysite-when-bob-met-alice/whats-the-order-in-ecc-ac8a8d5439e8)\n\n- [https://github.com/ethereumbook/ethereumbook/blob/develop/04keys-addresses.asciidoc#elliptic_curve](https://github.com/ethereumbook/ethereumbook/blob/develop/04keys-addresses.asciidoc#elliptic_curve)\n\n- [Secp256k1 | River Glossary](https://river.com/learn/terms/s/secp256k1/)\n\n- [Discrete Log Problem (DLP) | River Glossary](https://river.com/learn/terms/d/discrete-log-problem-dlp/)\n\n- [Elliptic curves secp256k1 and secp256r1](https://www.johndcook.com/blog/2018/08/21/a-tale-of-two-elliptic-curves/)\n\n- [Why did Satoshi decide to use secp256k1 instead of secp256r1? - DappWorks](https://dappworks.com/why-did-satoshi-decide-to-use-secp256k1-instead-of-secp256r1/)\n\n- [Exploring Elliptic Curve Pairings](https://medium.com/@VitalikButerin/exploring-elliptic-curve-pairings-c73c1864e627)","lastmodified":"2022-10-06T10:43:51.428591814Z","tags":null},"/posts/Optimistic-Rollups":{"title":"Optimistic Rollups","content":"\n## Asserter/Checker Problem\n\n- Asserter makes a claim $A$, checker checks the claim for cost $C$ and gets R as reward, if successful.\n- If Asserter cheats without getting caught, Checker loses $L$ in the form of loss of items of value.\n- Two main threats to worry about: Laziness and Bribery\n- Bribery: Asserter bribes Checker more than the reward $R$. Prevent this by large bond by Asserter so that bribe canâ€™t be bigger than the reward $R$.\n- Laziness: Checker does not check intentionally.\n- If we assume, asserter cheats with prob. X, then\ncheckerâ€™s utility comes as:\n\n\u003e $-X*L$, if checker doesnâ€™t\n\u003e\n\u003e $R*X-C$, if checker checks\n\n- So, checking is only worthwhile, if utility of checking \u003e not checking.\n\n\u003e $X \u003e C/(R+L)$\n\n- Thus, *asserter* can cheat with random prob \u003c required and not get caught.\n- This doesnâ€™t depend on how much *asserter* gains from cheating, as long as itâ€™s non-zero which is a bad result.\n- Even adding more *checkers* doesnâ€™t help, as the reward gets distributed which only reduces *checkerâ€™s* incentives.\n\nReason this is a problem:\n\n- *Asserter* controls the behaviour of the checker, as the utility of checker depends on asserterâ€™s prob.\n- Need to add an attention parameter in *checkerâ€™s* incentives where checker pre-computes *asserterâ€™s* claim off-chain beforehand and has to verify it on-chain time to time.\n- Two new parameters:\n\n\u003e $P$, fraction of time *checker's* will post response\n\u003e\n\u003e $A$, penalty in case *checker* gives wrong answer\n\n- New equation becomes,\n\n$$R*X-C$$\n\n$$-L*X-P*A$$\n\n- If $P*A \u003e C$, then checking is better than not checking\n- cost of checking is low\n\n\u003e Assume 1 assertion / 5 mins and $C = $0.001$ If P = 0.3%, checker will deposit $3.\ncost per assertion = $0.0003 =\u003e $0.01*0.3%\ninterest cost of locking = $0.0003\ntotal cost = $0.0006\n\n- Multiple checkers need to submit proofs differently thus, scaling to multiple checkers efficient.\n\n## Technical Details\n\nChecker: private key $k$, public key $g^k$\n\nHash fn: $H$\n\nComputation to solve: $f(x)$\n\nAsserter challenge: $(x, g^r)$\n\nChecker post on-chain iff $H(g^{rk}, f(x)) \u003c T$\n\n\u003e Note: Only checker and asserter knows $g^k$ and $r$, and $T$ requires $f(x)$\n\nChecker can guess $f(x)$ with prob. G, then multiply deposit with $1/1-G$\n\n- Asserter publishes f(x), can challenge checker's response while publishing r\n- Check the accusation and penalise checker, half the deposit to asserter\n- If asserter $f(x)$ incorrect, accusation reverted.\n- Each checker will have different prob of posting on-chain due to use of private key, thus canâ€™t copy others computation.\n- Asserter now instead of bribing checker, will try to mislead him into giving false information on-chain.\n\n## Links\n\n[(Almost) Everything you need to know about Optimistic Rollup](https://research.paradigm.xyz/rollups)\n\n[The Cheater Checking Problem: Why the Verifierâ€™s Dilemma is Harder Than You Think](https://medium.com/offchainlabs/the-cheater-checking-problem-why-the-verifiers-dilemma-is-harder-than-you-think-9c7156505ca1)\n","lastmodified":"2022-10-06T10:43:51.428591814Z","tags":null},"/posts/SoK":{"title":"Communication Across Distributed Ledgers","content":"\n[Communication Across Distributed\nLedgers](https://eprint.iacr.org/2019/1128.pdf)\n\nAims to develop a guide for designing protocols bridging different types of blockchains (distributed ledgers).\n\nShows that CCC is impossible without **_third party._**\n\nPresents a framework keeping these trust assumptions in mind. Classifies current CCC protocols on the basis of framework.\n\n## Introduction\n\n- NB-AC (_Non-Blocking Atomic Commit_) is used in distributed databases to ensure that correct processes don't have to wait for crashed processes to recover.\n- Can be extrapolated to distributed ledgers by handling _byzantine failures._\n\n## Distributed Ledger Model\n\n- $X, Y$: Blockchains\n- $Lx$, $Ly$: ledgers with _states_ as dynamically evolving sequences of _transactions_\n- state of ledgers progresses in round _r._\n- $L^P[r]$: state of ledger _L_ at round _r_ after all txs till _r-1_, according to some party _P._\n- Consistency is defined by the system\n- ($TX$, $Lx^P[r]$): tx _TX_ is valid for _Lx_ at round _r_ according to _P._\n- $TX$ âˆˆ $L^P[r]$: TX is included in _L_ as position _r._\n- **Time** $L^P[t]$**:** ledger state at round r or time t.\n\n**Persistence**: $L^P[t] \u003c= L^Q[tâ€™]$, $L^P$ at time $t$ is prefix of $L^Q$ at time $t$â€™.\n\n**Liveness**: if tx $TX$ is included in ledger $L$ at time $t$, then it will appear in ledger at time $t$â€™.\n\n### CCC System Model\n\n- $P: TX_P, Q: TX_Q$: separate processes running on two different ledgers with txs\n- $P$ possesses a description $d_Q$ which characterises the transaction $TX_Q$, while $Q$ possesses $d_P$ which characterises $TX_P$\n- Thus, $P$ wants $Q$ to be written to $Ly$ and vice-versa.\n- $m_P, m_Q$: boolean error variables for malicious processes\n\n## Formalisation of correct CCC\n\nGoal: sync of P and Q such that Q is included iff P is included. For example, they can constitute an exchange of assets which must be completed atomically.\n\n**Effectiveness:** if both correct, then both will be included, otherwise none\n\n**Atomicity**: no outcome in which $TX_P$ included but $TX_Q$ not at time $t$â€™ or vice versa.\n\n**Timeliness**: If a process behaves correctly, $TX_P$ will be included and $Q$ will verify. It is a liveness property.\n\n## Generic CCC Protocol\n\n\u003e $u_x$: liveness delay\n\u003e\n\u003e $k_x$: depth parameter\n\n1. **Setup**: inherently done by both blockchains due to the properties defined above\n2. **Pre-Commit on X**: $P$ writes $TX_P$ to $L^P_X$ at time $t$ in round $r$. Due to persistence and liveness, all honest parties report TX_P as valid in $r+u_x+k_x$.\n3. **Verify**: Q verifies $TX_P$.\n4. **Commit on Y:** $Q$ writes $TX_Q$ to $L^Q_Y$ at time $t$â€™ in round $r$â€™.\n5. **Abort**: revert $TX_P$ on $Lx$ in case of verification failure or $Q$ fails\n\n![Image.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/doc/2875A4D6-F00A-45A2-96EE-7222C31E634F/490D063A-EBE6-49DA-A5C7-D53342042837_2/q21xMyyBAkdRygr7FHp2PQr7J452ctL6JuxUysHHwccz/Image.png)\n\nCCC protocols follow two-phase commit design.\n\nPre-commit and commit on Y is executed in parallel following verification and abortion, if required.\n\n## Impossibility of CCC without TTP (Trusted Third Party)\n\nAnalogous to **_Fair Exchange_** Problem.\n\nTTP is basically any entity, be it individual or a committee that either confirms a tx has been successfully included or enforce correct behaviour of $Q$ on $Ly$.\n\nLemma 1: Let $M$ be a system model. Let $C$ be a protocol which solves $CCC$ in $M$. Then there exists a protocol $S$ which solves _Fair Exchange_ in $M$.\n\nSketch: to complete exchange, $TX_Q \\in  Ly$ and $TX_P \\in Lx$.\n\n- _effectiveness_ enforces correct transfer for correct behaviour.\n- Persistence and liveness enforce both txs to be eventually written to respective ledgers.\n- Atomicity $\u003c-\u003e$ Strong Fairness in Fair exchange\n\nSmart contracts or code based solutions can be used to write $TX_Q$ to $Y$, in this case consensus becomes TTP to execute this smart contract.\n\nTTP either becomes the process $P$ itself or another party which submits proof of $P$ inclusion to $Q$.\n\nMany other frameworks for designing a CCC protocol:\n\n- Incentivizing third party\n- Slashing the rewards\n- Optimistic\n\n## CCC Design Framework\n\nThree main types of trust model:\n\n- TTP\n- Synchrony\n- Hybrid\n\n### Pre-Commit Phase\n\n#### Model 1: TTP (Coordinators)\n\nCan participate in two ways:\n\n- Custody of Assets: taking control of protocol participant funds to enforce rules\n- Involvement in consensus: in case of smart contracts, when consensus participants are TTP\n\nCoordinator Implementations\n\n- External Custodians: Committee\n- Consensus Level Custodians (Consensus Committee)\n- External Escrows (Multisig Contracts)\n\n#### Model 2: Synchrony (Locking)\n\n- Locks based on hashes\n- Locks based on signatures\n- Timelock puzzles \u0026 Verifiable delay fns\n- Smart Contracts\n\n#### Model 3: Hybrid\n\nWatchtowers (Other external parties) to be used as fallback if one of the service fails or crashes\n\n### Verification Phase\n\nSame models but applied on verification part\n\n1. External Validators/Smart Contracts\n2. Direct Observation/Relay SCs (Using light clients)\n3. Hybrid using watchtowers\n\n### Abort Phase\n\n## Classification of Existing Protocols\n\n### Exchange Protocols\n\nAtomic exchange of digital goods: $x$ on Chain $X$ again $y$ on $Y$. Both parties pre-commit, then verify and abort in case of failure.\n\n#### Pre-Commit\n\nDone through atomic swaps\n\n- Both parties lock assets on-chain with identical release conditions. _Hashed Timelock contracts_ are the closest implementation of symmetric locks. Signature locks using _ECDSA_ are also used.\n- On turing-complete blockchains, atomic swaps can be handled through smart contracts which can verify the state of chain $Y$ (_chain relay_).\n- Hybrid: symmetric with TTP is used to solve usability challenges in atomic swaps.\n\n#### Verify\n\nDone through external validators in symmetric swaps or through chain relays in SPV based atomic swaps.\n\n#### Abort\n\nTimelocks are set up on assets for a pre-defined duration to prevent indefinite lock up in case of failures.\n\n### Migration Protocols\n\nMigrate the asset $x$ from chain $X$ using write locks on $x$ preventing further use on $X$ and creating a wrapped version of same asset on $Y$.\n\nFour main use cases of these protocols:\n\n- Wrapped version of assets between chains\n- communication b/w shards\n- sidechains\n- bootstrapping a new chain\n\n#### Pre-commit\n\nRelies on a single/committee based external custodian for TTP or through multisigs.\n\n**Sidechains**: same approach of depositing on chain $x$ controlled via multisigs which approve asset $y$ on chain $Y$.\n\n**Shards**: utilises the same security and consensus model as the main chain is same for all shards.\n\n_Bi-directional chain relays_ can also be used if both chains support smart contracts and thus, locking/minting of assets can be handled through these contracts.\n\n**Proof of Burn**: used for uni-directional flow as asset $x$ is burned on chain $X$.\n\n#### Verify\n\n- Chain relay contracts\n- Consensus committees to sign to verify pre-commit step.\n\n#### Abort\n\nMigration protocol doesnâ€™t have explicit abort phase.\n\n## CCC Challenges\n\n### Heterogeneous Models and Parameters Across Chains\n\n- Different parameters used by different chains\n- security models\n- consensus differences: consensus execution, finality\n\n### Cryptographic Primitives\n\ndifferent cryptographic algorithms for hash locks or signatures\n\nZK proofs may provide a workaround but increases complexity, communication costs.\n\n### Collateralization and Exchange Rates\n\nUsing collaterals to prevent malicious behaviour among custodians or TTPs, incentivising correct behaviour but different types and rates of collateral b/w different chains.\n\nDynamic Collateralization based on exchange rates among different blockchains\n\n#### Lack of Formal Security Analysis\n\n- Replay Attacks on state verification: if proofs are submitted multiple times either on the same chain or on different chains can lead to multiple spendings of assets.\n- Data availability: timely requirements of proofs and data, if not reached in time, leads to incorrect behaviour of process.\n\nNeed more research on this topic as current solution increases complexity and decreases efficiency.\n\n#### Lack of Formal Privacy Analysis\n\ndidnâ€™t understand perfectly\n\n### Upcoming Research\n\n- Interoperability chains: Cosmos and polkadot Layer 0 based ecosystems.\n- Light Clients: for better verification\n- Off-Chain Protocols\n  - Communication across off-chain channels\n  - Communication b/w on-chain and off-chain networks\n","lastmodified":"2022-10-06T10:43:51.428591814Z","tags":null},"/posts/eth-basics":{"title":"Ethereum Yellow Paper","content":"\nTransaction based state-machine\n\nstate of the blockchain changes after executing transactions.\n\nstores states in ***state root***\n\n## Accounts\n\n20-byte hex addresses whose state is stored on the blockchain\n\nTwo types of accounts:\n\n1. EOA: externally owned account\n2. Contract Accounts\n\n## Each accounts has four parts:\n\n1. Nonce: for EOA, number of transactions sent from the address. For Contracts, number of contracts creations.\n2. Balance: amount of ether owned by the account.\n3. storageRoot: Merkle-Patricia tree that stores data related to the account. Stored in the top-level state root tree.\n4. codeHash: For EOA, hash of empty string. For contracts, hash of the init code.\n\n## Transactions\n\nPiece of data signed by an external actor.\n\nTwo types of txs:\n\n1. Transaction which result in message calls\nMessage Calls: Done by contract account, when executing `CALL` opcode.\n2. Contract creation\n\nFields in a tx:\n\n1. nonce\n2. gasLimit\n3. gasPrice\n4. to\n5. value\n6. v, r, s: signature identifying sender\n7. init: in case of contract creating tx, returns code of the contract without constructor\n8. data: in case of message call tx, data being passed in call\n\n## Blocks\n\naggregate transactions and include in the blockchain\n\nContains:\n\n1. parentHash\n2. ommerHash\n3. beneficiary\n4. stateRoot\n5. transactionsRoot\n6. receiptsRoot\n7. timestamp\n8. number\n9. difficulty\n10. gasLimit\n11. gasUsed\n12. extraData\n13. mixHash\n14. nonce\n\n## GHOST\n\n`GHOST` protocol is used by ethereum to prevent mining centralization and enhance protocol security.\n\nLongest chain isnâ€™t just the chain with more blocks as ancestor, but it also includes other stale(uncle) blocks in the calculation.\n\nUncle blocks are child of the ancestor of the block and not directly related to the block.\n\n## RLP\n\nIt is the data encoding used by the protocol to store data in tries.\nThere are several rules for encoding mentioned below.\n\n[Data structure in Ethereum | Episode 1: Recursive Length Prefix (RLP) Encoding/Decoding.](https://medium.com/coinmonks/data-structure-in-ethereum-episode-1-recursive-length-prefix-rlp-encoding-decoding-d1016832f919)\n\n## HP Encoding\n\nThis encoding is used for trie paths.\n\n[Data structure in Ethereum | Episode 1+: Compact (Hex-prefix) encoding.](https://medium.com/coinmonks/data-structure-in-ethereum-episode-1-compact-hex-prefix-encoding-12558ae02791)\n\n[Ethereum: Tutorials - LayerX Research](https://scrapbox.io/layerx/Ethereum:_Tutorials)\n\nPatricia tree\n\n[patricia-tree](https://eth.wiki/fundamentals/patricia-tree)\n[Understanding trie databases](https://medium.com/shyft-network-media/understanding-trie-databases-in-ethereum-9f03d2c3325d)\n\n","lastmodified":"2022-10-06T10:43:51.516593642Z","tags":null},"/posts/eth-consensus":{"title":"ETH Consensus","content":"\nELI5 understanding of ETH 2.0 specs\n\n**Safety**: guarantees that something bad never happen. Examples: Tendermint from Cosmos that uses BFT style consensus.\n\n**Liveness**: something good eventually occurs. Example: POW, Casper used by Ethereum.\n\n## **Why PoS**\n\nAn individual autonomy should always be greater than the power of any state. Cryptography solved this issue, by using ECC, individuals can now have a pair of keys that only he can access and thus, has the power to defend even state-level attacks.\n\nConsensus in blockchain is what drives the value of the network, and from where the network derives its value. No attacker should have incentives to attack the network for his own gain. PoW style networks are based on rewards where a consensus participant has almost nothing to lose in case the network is attacked. PoS solves this problem by allocating stakes to network particpants and imposing penalties in case of any malicious actors in the system. The ratio of rewards v/s penalties determines the incentives of consensus participants to behave honestly as penalties are directly proportional to number of wrong validations, if more validators behave maliciously, then slashing is higher.\n\n## Different types of PoS\n\n- Chain-Based PoS: pseudo-random validator assigned at each time slot to create new block behind a previous block\n- BFT-style PoS: (partially synchronous) randomly assigned validator proposes a block and canonical chain is assigned using a voting process on which each validator votes for the valid chain\n\n## **Proof Of Stake**\n\n- Stake ETH to become validator\n  - For each 32 ETH staked, a validator is activated. Anyone can stake any number of ETHs and activate and control validators and execute **validator clients**.\n  \u003e Validator clients has the functionality of following and reading the Beacon Chain. A validator client can make calls into the Beacon nodes.\n- Validator, responsible for adding blocks to the chain by verifying txs and policing blocks being added by other validators\n  - Earns by successfully adding blocks\n  - Staked eths slashed if illegal txs added\n- Save Energy as selected randomly and not competing\n- No need to mine, just validate blocks known as **Attesting**\n\n## **Phases**\n\n- Beacon Chain\n- Sharding\n- Execution\n\n## **Sharding**\n\n- Scaling nodes horizontally\n- At least 128 Validators randomly assigned to a shard where a new block will be added in each slot of an epoch i.e.,Â after 32 slots\n- ETH plans for 64 shards\n\n## **Beacon Chain**\n\n- Main functioning body in the blockchain, managing all the shards and the validators\n- Functions as heart of the chain\n- Creates **committee**, which are the validators used to validate a blockchain on which everyone verifies, stores and downloads\n\n## **Slots \u0026 Epochs**\n\n- Chain is divided into **slots** and **epochs**\n- **Slot** â†’ timeframe to propose and validate a new block\n  - In a time period pre-determined in a blockchain, 12 seconds in case of Ethereum, all shard blocks are added into the blockchain\n  - Slots can be empty in case a validator fails to **propose** the block or the committee fails to attest\n  - Genesis blocks added to both Beacon chain and shards at block 0\n- **Epochs** â†’ 32 slots comprises an epoch. \u003e 12 sec * 32 slots = 6.4 mins\n\n## **Crosslinks**\n\n- reference to shard blocks\n- basically the proof that a shard is valid\n- created at beacon chain for every successful block proposed by a shard\n- only after a crosslink does a validator get its reward\n\n## **Committee**\n\n- **Beacon chain** gets its name from the random numbers that it emits to the public\n- uses RANDAO process to randomly select a group of validators for an epoch to a shard to attest transactions in a block\n- crosslinks are made after attestation from validators of that slot\n- Every epoch, validators randomnly assigned to a slot which is then subdivided into committees.\n- Each committee is assigned a particular shard and they attempt to crosslink a shard block to Beacon chain head in order to gain rewards.\n- 64 shards, each assigned 128 validators per committee -\u003e Thus, has atleast 8192 validators\n\n### **Beacon Chain Checkpoints**\n\n![Beacon Chain Checkpoints](posts/assets/Beacon-Chain-Checkpoints.jpeg)\n\nA `checkpoint` or an `epoch boundary` is the first block in an epoch. If no such block, it is the most recent preceding block.\n\n### Votes\n\n- **`LMD GHOST`**: validator vote for beacon chain head, i.e. what they believe beacon chain head is.\n- **`Casper FFG`**: When casting LMD GHOST vote, validators also vote for checkpoint in current epoch, called `target`. This also includes previous epoch checkpoint called `source`.\n\n## Finality\n\n\u003e `Supermajority vote`: which is made by 2/3 of the total **balance** of all validators.\n\n![image](posts/assets/Beacon-Chain-Justification-and-Finalization.png)\n\nWhen an epoch checkpoint gets supermajority vote, it is said to be *justified*.\nAn epoch is *finalised* when it is justified and next epoch checkpoint gets justified.\nWhen a epoch checkpoint, i.e. slot gets finalised, all preceding blocks also gets finalised.\n\n- Finality is important as it gives gurantees to shards and ethereum parties regarding transactions.\n- Reduces complexity with cross-shard communications.\n\n## FLP Impossibility\n\n![image](posts/assets/FLP-Impossibility.png)\n\nIn a distributed system, it is not possible to simultaneously have safetly, liveness and full asynchrony unless some unreasonable assumptions are made.\n\nEthereum uses both `LMD-GHOST` and `Casper FFG` as its protocol to justify and finalise blocks.\n\n**LMD-GHOST** preferes liveness over safety in the form that validator can attest to a chain head and keep producing blocks while **Casper** prefers safety over liveness such that a block is finalised only when it is justified and in a later epoch, majority of validators attest it again to finalise it. Once, a block is finalised, it is added forever in the chain.\n\nDue to **LMD-GHOST** prefering liveness over safety, there is a chance of reorgs. Capser FFG helps here as it prefers safety and decisions made under Casper is considered final. It has phases, in which nodes indicate they'd like to agree on something(justification), then agree that they've seen each other agreeing(finalisation).\n\n## **Questions**\n\n1. MEV in PoS\n2. Reorgs\n\n## **Checkpoints**\n\n1. [https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/](https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/)\n2. [eth2book.info](https://eth2book.info/altair/contents)\n3. [https://github.com/ethereum/annotated-spec/blob/master/phase1/beacon-chain.md#introduction](https://github.com/ethereum/annotated-spec/blob/master/phase1/beacon-chain.md#introduction)\n4. [https://ethos.dev/beacon-chain/](https://ethos.dev/beacon-chain/)\n5. [https://ethresear.ch/t/two-ways-to-do-cross-links/2074](https://ethresear.ch/t/two-ways-to-do-cross-links/2074)\n6. [https://vitalik.ca/general/2017/12/31/pos_faq.html](https://vitalik.ca/general/2017/12/31/pos_faq.html)\n7. [https://medium.com/codechain/safety-and-liveness-blockchain-in-the-point-of-view-of-flp-impossibility-182e33927ce6](https://medium.com/codechain/safety-and-liveness-blockchain-in-the-point-of-view-of-flp-impossibility-182e33927ce6)\n8. [https://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51](https://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51)\n9. [Ben Eddington blog](https://hackmd.io/@benjaminion/eth2_news/https%3A%2F%2Fhackmd.io%2F%40benjaminion%2Fwnie2_220311)\n10. [eth2book](https://eth2.incessant.ink/book/03__eth1/07__clients.html)\n11. \u003chttps://arxiv.org/pdf/2203.01315.pdf\u003e\n12. [LMD GHOST and Casper FFG](https://blog.ethereum.org/2020/02/12/validated-staking-on-eth2-2-two-ghosts-in-a-trench-coat/)\n13. [https://blog.ethereum.org/2019/12/30/eth1x-files-state-of-stateless-ethereum/](https://blog.ethereum.org/2019/12/30/eth1x-files-state-of-stateless-ethereum/)\n14. [Understanding validator effective balance](https://www.attestant.io/posts/understanding-validator-effective-balancehttps://www.attestant.io/posts/understanding-validator-effective-balance/)\n15. [0xfoobar's Proof of Stake](https://0xfoobar.substack.com/p/ethereum-proof-of-stake)\n","lastmodified":"2022-10-06T10:43:51.516593642Z","tags":null},"/thoughts/fixed-rate-protocols":{"title":"Fixed Income Protocols","content":"\nDeFi's next wave of protocols has come through fixed income protocols. I will go deep into yield curves based protocols i.e. [Yield](https://yield.is), [Notional](https://notional.finance).\n\n## Yield Protocol\n\nYield introduced *yTokens* i.e. `Yield Tokens` similar to compound's *cTokens* which essentially behaves as zero-coupon bonds expiring at a future date and can be redeemed 1-on-1 for the underlying asset.\n\n*yTokens* can be building blocks that can be used to make many other interesting products. Market price of *yTokens* can be used as interest rate oracle. Each yToken has its own interest rate over the period to expiration date which can be used by many other protocols to settle on-chain interest rate derivatives.\n\n*yTokens* differ from each other in four aspects:\n\n1. Underlying Asset\n2. Collateral Asset\n3. Collateralization requirement\n4. Expiration Time[[]()]()\n\n## Actors\n\n### Borrowers\n\nBorrowers is when actor opens a vault, takes out *yToken* and sells it. It is essentially shorting the underlying asset or longing the collateral asset.\n\n### Lenders\n\nBuying yTokens are similar to lending the underlying asset in which the holder of yToken is earning an interest on the underlying asset in the form of the discount which it gets when buying the yTokens.\n\n## Settlement\n\nyTokens can be construcuted using 3 main principals.\n\n### Cash settlement\n\nCash Settlement is paid in the collateral asset, which implies that it depends on a dependent price oracle that determines the price of underlying asset in terms of collateral asset.\n\nAt the moment of maturity, anyone can call the contract to trigger *settlement*, that redeems the *yTokens* for its equivalent value in collateral asset. After the moment of settlement, *yTokens* begin to track the price of collateral asset rather than the price of underlying asset but can only be redeemed at the price of settlement.\n\nThis mechanism has an advantage that it can support any asset and not just ERC20 assets as it just needs a price oracle to compare the price of yTokens with the collateral asset.\n\n### Physical settlement through auctions\n\nAt the time of minting a yToken, it is backed using the collateral asset. Minting the tokens adds to the vault's owner debt which shouldn't be less than the value of the collateral asset plus some required margin.\n\nWhen a target asset is also an ERC20 token, its settlement can be triggered physically i.e. holders get paid in underlying asset rather than collateral assets through auctions.\n\nGradual dutch auctions are held to sell the collateral for the underlying asset. Remaining collateral is returned back to the vault owner and underlying tokens earned during the auction is ditributed among *yToken* holders.\nIf auction is not completed successfully, collateral asset is distributed amont the holders along with physical assets.\n\nAdvantage over cash settlement is that after the auction is successfully done, each *yToken* is backed equally with the underlying asset rather than some collateral asset. Holders can redeem the underlying asset (but doesn't earn yield on it).\n\n### Synthetics Settlement\n\nWhen the target asset itself is a collateralized synthetic asset like DAI, *yToken* uses token's own issuance mechanism as settlement.\n\nIn case of DAI, if yDAI backed with ETH as collateral matures, the protocol creates a vault in MakerDAO with ETH as collateral. When yDAI holders come to redeem, it borrows DAI from Maker and pay to the yDAI holders. Essentialy fixed rate position in yTokens turns into variable rate debt position at the time of maturity for these synthetic assets. yDAI holders need to pay the interest for their debt position and can earn DAI savings rate as well.\n\nAdvantage is that borrowers' and lenders' position is not settled and have the option to keep it open with the synthetic token's own mechanisms.\n\n## Interest Rate oracle\n\n*yTokens's* price in itself throughout the period until maturity can be treated as an interest rate oracle as the *yTokens* price floats freely depending on the supply and demand.\n\n$$Y = (\\frac{F}{P})^{\\frac{1}{T}} - 1$$\n\n---\n\n## YieldSpace AMM\n\nA new invariant based AMM introduced to trade *fyTokens* introduced in yield paper which incorporates time into the AMM equation.\n\n$$x^{1-t}+y^{1-t} = k$$\n\n$y$ = reserves of *fyToken*,\n\n$x$ = reserves of underlying token.\n\n$t$ = time to maturity\n\n---\n\n![yieldspace curve](thoughts/images/yieldspace_curve.png)\n\nThis formula works as constant sum protocol when $t-\u003e0$, and constant product formula when $t-\u003e1$.\n\nThis formula is defined in the yield space rather than the price space as designed in previous AMM formulas such that marginal interest rate of fyTokens at any time is equal to ratio of fyToken reserve to underlying token reserve minus 1.\n\n$$r = \\frac{y}{x} - 1$$\n\nThis formula does not have any time component, thus ensures that marginal interest rate remains proportional to fyToken and underlying token reserves at any point in time. This implies that as the allocation of fyToken in the pool increases or underlying token decreases so does the interest rate and buying pressure arises, and vice versa.\n\n## Why not other invariants\n\n1. Constant sum invariant only works for assets of similar value, and fyToken generally is priced at a discount until maturity date.\n2. Constant product formula includes liquidity at whole price spectrum but when the fyToken approaches maturity, its price tend to be similar to underlying token and thus the liquidity at other price points are wasted and larger trades have significant impact on interest rates.\n3. Curve's stableswap equation doesn't let it modify $\\chi$ to account for variation in interest rates due to time to maturity.\n\n## Properties\n\n$$x^{1-t}+y^{1-t} = x_{start}^{1-t} + y_{start}^{1-t}$$\n\nMarginal price for a given $x_{start}$, $y_{start}$, and $t$ is given by the formula:\n\n$$(\\frac{y}{x})^t = (\\frac{(x_{start}^{1-t} + y_{start}^{1-t} - x^{1-t})^\\frac{1}{1-t}}{x})^t$$\n\n![token price vs reserves](thoughts/images/dai_price_vs_dai_reserves.png)\n\nLooking at interest rates,\n\n![interest rate vs dai reserves](thoughts/images/interest_rate_vs_dai_reserves.png)\n$$\\frac{y}{x} - 1 = \\frac{(x_{start}^{1-t} + y_{start}^{1-t} - x^{1-t})^\\frac{1}{1-t}}{x} - 1$$\n\n## Fees\n\nLP are incentivised to provide liquidity using the fees that they earn. Since, constant sum power formula is defined in yield space and not price space, it's not meaningful to impose fees on price and rather on interest rates i.e. any buyer of fyToken should get lesser interest rates or higher buy price.\n\nThus, the fee formula modifies the interest rate by adding a variable $g \u003c 1$ to change interest rates.\n\n$$r = (\\frac{y}{x})^g - 1$$\n\n\u003e Note that this formula is used for buying fyTokens, $\\frac{1}{g}$ is used when selling fyTokens.\n\nThus, the new AMM formula becomes\n\n$$x^{1-gt}+y^{1-gt} = k$$\n\n## Capital Efficiency\n\nOriginal protocol allows user to mint 1 fyToken in exchange of 1 underlying token and there is no real incentive to buy a fyToken above the underlying token price. Thus, the pool always checks at the end of every trade that price of 1 fyToken is not greater than 1 underlying token or reserves of fyToken is greater underlying. Thus, Some portion of fyToken reserves in the pool is always inaccessible. Example can be when pool is first initialized, the equal fyToken in the pool is never utilised as the remaining fyToken can't be sold.\n\nSo, the capital efficieny of the pool is improved by making the excess fyToken's reserves `virtual`. LPs don't need to contribute these access reserves. Pool uses liquidity tokens `s` as the virtual fyTokens reserves. Whenever a trade occurs, `virtual` tokens are added to actual reserves to calculate the appropriate amount but whenver liquidity is added, only the real reserves are used to calculate fyTokens in proportion to the actual fyToken in pool.\n\n## Resources\n\n- [Yield Paper](https://research.paradigm.xyz/Yield.pdf)\n- [Yieldspace paper](https://yield.is/YieldSpace.pdf)\n- [Element finance paper](https://paper.element.fi//)\n- [Sense](sense-finance.md)\n- [Messari's Fixed Income Protocol](https://messari.io/article/fixed-income-protocols-the-next-wave-of-defi-innovation)\n- [Designing Yield Tokens](https://medium.com/sensefinance/designing-yield-tokens-d20c34d96f56)\n- [Swivel's cash flow instruments Pt.1](https://swivel.substack.com/p/cash-flow-instruments-pt-1-history?s=r)\n- [Defization of fixed income products](https://medium.com/coinmonks/the-defization-of-fixed-income-products-7e72ed4f57b1)\n- [Defixed income](https://medium.com/@exactly_finance/defixed-income-101-948976c0e2c6)\n- [Notional](https://medium.com/coinmonks/notional-the-alpha-of-fixed-income-defi-products-a5637d2092b5)\n- [](https://medium.com/finoa-banking/turning-proof-of-stake-yield-into-fixed-income-products-7de8a73097ac)\n- [Fixed Income Protocols](https://medium.com/gamma-point-capital/fixed-income-protocols-the-next-wave-of-defi-innovation-69215be82b4e)\n\n\n## Questions\n\n- What are fixed yield rate protocols?\n- different types of protocols currently\n\nmainly three types: tranches, zero-coupon and stripping. Sense follows stripping architecture\n\n- what are tranches based and zero-coupon based?\n\ntranches are where users provide their assets and protocols invests in different strategies on the basis of risks of the tranche. More risk assosciated. Each protocol has their own set of safety backstops to stop protocol from being insolvent.\n\nZero coupon based protocols essentialy turn the asset into a bond which is traded at a discount and is exchangeable 1-1 at maturity.\n\nSense follows stripping architecture that allows it to strip target into PT and YT. Note that target in Sense can be yield bearing assets. This yield is given then to YTs.\n\n- What are yield or notional doing?\n\nyield issues zero-coupon bonds taking collateral and giving fyTokens. susceptible to liquidations in volatile markets.\n\n- what is 88mph or barnbridge doing?\n\n88mph/barnbridge uses lending market protocols like compound, aave to provide fixed yields to deposited assets. These fixed interest rate models are determined by governance. These protocols are prone to drop in interest rate offered by the variable markets after the deposit. They offset this by issuing Floating rate bonds to users for the extra yield. This model is hard to scale, requires governance at every step, not risk-free for users as they can't exit at will because the firb are issued at fixed terms.\n\n- Why yield stripping and not others?\n\ntranches not safe, and zero-coupon bonds mainly use underlying assets which does not take into account the yield. Stripping applications takes a yield bearing assets and gives the user security against their principal in PTs and yields in YTs. So, its just more safer and transparent for users, better abstractability and flexibilty for devs and users as they can plug these PT or YT further.\n\n- Sense space?\n- yieldspace pools\n\nmakes sense to account for implied rate and liquidity to be spread around interest rate rather than the price as price there is function of rate itself. space better as target is deposited, no IL, yield goes to LPs.\n\n- How are PTs and YTs priced?\n\nStripping protocols follows the invariant that PTs + YTs at any point = Target. At the time of series creation, YTs are the claim to the yield and PTs trade at discount which is Target - YTs. As maturity approaches, PTs tend towards the price of target 1 on 1.\n\nPTs are priced according to formula $(\\frac{y}{x})^{\\frac{1}{t}}$, where x and y are reserves in pool.\n\nYTs are priced according to the yield payments that they're going to receive till maturity. So, if 4 yield payments of $0.1, then $0.4 is their price.\n\n- What is a series?\n\nseries is a specific set of PTs and YTs of a target with specific maturity and specific adapter.\n\n- list all common sc attack patterns?\n\nunverified calls, dos, delegatecalls, signature malleability, re-entrancy, arithmetic over/underflows, randomisation in evm, tx.origin, selfdestruct\n\n- compound, aave, fuse?\n\n- what is your thinking when designing some project?\n\ndepends on the project,\n\n1. swap\n2. lend\n\nthen thinking about actors and their actions, then modules, external interactions\n\nuniswap like structure i.e. core and periphery. Anybody can directly build on core and periphery is used for normal user interaction with the protocol.\n\n- How do you guys take an idea from different phases to mainnet?\n- do you use other tools to test your smart contracts like slither, echidna ?\n-","lastmodified":"2022-10-06T10:43:51.516593642Z","tags":null},"/thoughts/nomad-xyz":{"title":"Nomad","content":"\nProtocol for cross-chain communication. Follows **optimistic** design inspired from _Optimistic Rollups_. Nomad's design aims to solve the interoperability trilemma famously coined by Connext but introduces a new component, `latency`.\n\n## Design\n\nArchitectural design is divided into two main components:\n\n- On-chain components\n  - Home\n  - Replica\n- Off-chain components\n  - Updater\n  - Relayer\n  - Watcher\n  - Processor\n\n## Optimistic Flow\n\nNomad works with both of these components to facilitate communication between any number of chains that support user-defined computations. It takes use of _sparse merkle trees_ as data structure to pass message between the chains.\n\nThere is a source domain with `Home` contract and a destination ~~chain~~ domain with the `Replica` contract. `Home` is reponsible for generating the message on the source domain and `Replica` on the destination domain keeps a copy of the tree which gets updated when new messages are received from the source domain.\n\n\u003e Note: We'll use `domain` here instead of `chain` as Nomad is a base layer that can be used with any chains, rollups, etc.\n\nHow these messages gets passed securely is handled by the off-chain components which helps in ease of implementation and reducing the cost of these transfers by 80%. The `updater` component polls messages from the `Home` contract in the form of a merkle tree and attests it. It is then sent to the destination chain by the `Relayer`. There, it waits for some time before being added in the Replica contract which holds the current root. Since, `updater` can attest fraudulent messages, the timeout gives anyone the chance to prove the fraud which results in bond slashing of updater and all the attestations being marked as invalid.\n\nSince the messages are just a merkle tree being committed to newer messages, it can be easily rolled back. The process of timeout and `Watchers` submitting a fraud proof is done on the receiving chain because a `Relayer` doesn't know about messages at all, it's job is just to relay whatever message it receives from source chain. These fraud can be proven back to the source chain as that is the original `Source of Truth`, which slashes the `Updater` bonds.\n\nNomad rather than handling globally verified fraud proofs on the sending chain, allows fraudulent messages to pass through as then these frauds become public due to the timeout at the receiving chains. These can then easily be proven to the source chain which becomes the source of truth that these fraudulent messages where in fact, passed by the `Updater`.\n\n## Components\n\n### On-Chain Components\n\n#### `Home`\n\n- Used by other contracts to send messages using `send message` API\n- enfore message format\n- updates root by committing messages\n- maintains queue of tree roots\n- slashes `Updater` bonds\n  - Double update proofs\n  - Improper update proofs\n\n#### `Replica`\n\n- maintains queue of pending updates\n- add to new tree root after timeout elapses\n- accepts fraud proofs to validate messages\n- ensure processing of messages in proper order\n- sends messages to end recipients\n\n### Off-Chain Components\n\n#### `Updater`\n\n- polls home for new updates\n- signs or attests to new updates\n- publish to home chain\n\n#### `Relayer`\n\nrelays updates to new\n\n- Observes home to check for new updates\n- forwards the signed updates to one or more replicas\n- Observes replicas for timeout passed updates and updates relayer current root\n\n#### `Watcher`\n\nProvides security to the protocol\n\n- Submits Double/Invalid update proof\n- Observes home, to check the interaction of updater's with the Home contract to check for any malicious attestations\n- Observes replicas, so that updater doesn't directly go to replica\n\n#### `Processor`\n\nprocesses pending updates\n\n- maintains the old tree with all the details\n- creates proof for new updates\n- send proven messages to end recipients\n\n## Frauds\n\nCurrently, there are two types of fraud that can occur in the system.\n\n1. ### Improper Update\n\nOccurs when an `Updater` attests to a fake root that was not in the `Home` root's queue. Updater purpose is to send fake messages to the destination chain.\n\nThis can be easily proven by the `Watcher` by submitting the fake root and Updater sig to Home, eventually slashing Updater's bonds or at worst halting the transfers to destination chain.\n\n2. ### Double Update\n\nAn `Updater` can attest two identical roots, i.e. the roots share same sig in order to double spend the receiving chain. This gets detected by `Watcher` as the duplicate root submitted by Updater is not the part of the queue as it's already sent to the destination chain.\n\n## Token Bridge xApp Example\n\n**Source**: Chain A\n\n**Destination**: Chain B\n\n**Local Contract**: Contract on same chain\n\n**Remote Contract**: Contract on other chain\n\n- **Source**\n\n  - User approve tokens to local `BridgeRouter`\n    - If native token, held in escrow and sent to local `Home`\n    - If non-native, token burned as the contract is deployed by `BridgeRouter`\n  - Message constructed by local `BridgeRouter` to send tokens to `Destination` on `Remote BridgeRouter`\n    - It keeps mapping of bridge routers on other chains, to send message to desired receiver.\n  - enqueues message on `Home`\n\n- Off-Chain\n\n  - Updater attests to the root\n  - relayer forwards it to `Remote Replica`\n  - processor creates proof\n\n- **Destination**\n  - `Replica` processes message after timeout ends and sends message to `BridgeRouter`\n  - `BridgeRouter` verifies that it was sent by `Source BridgeRouter` as it keeps mapping of `BridgeRouter` of other chains\n  - Looks for ERC20 token in registry\n  - Sends token to recipient\n    - If native, send from tokens held in escrow\n    - If non-native, mints new tokens as the representative token contract is deployed by the `BridgeRouter`\n\n## Questions\n\n1. Why is an `Updater` called an `Updater` even tho its job is more like a validator?\n\n2. Can a `Relayer` send fraudulent updates? What if it does, and who gets slashed in that case?\n\nNo, it can't as it's job is just to relay whatever updates it polls to receiving chain.\n\n4. What are the future plans of tackling this optimistic timeout delay?\n\nRight now, this is more of a feature than a disadvantage as it makes system more robust and for implementation ease as well. Need more idea about future plans. Moreover, connext plays out an important role here to partner with Nomad and reduce this to nearly 2 mins.\n\n5. Does this `30 min` delay quantified on a basis? How is it chosen? Can it be variable on the basis of watcher numbers or some other factors? or is it fixed?\n\nResearch: https://medium.com/offchainlabs/fighting-censorship-attacks-on-smart-contracts-c026a7c0ff02\n\n6. How does a bridge handles fork on a sending chain? Does the updates get rolled back?\n\nMaybe here the timeout delay plays out as an advantage, if a fork occurs on a sending chain, the `Relayer` can mark update as invalid and it is no longer processed by `Processor`.\n\n7. Nomad assumes security on the presence of even 1 watcher, doesn't that create centralization risk? Can that 1 watcher be the same updater?\n\n8. Can Nomad leverage ETH consensus mechanisms like RANDAO to pseudo-randomly select updators and watchers?\n\n## Links\n\n- [Nomad Docs](https://docs.nomad.xyz)\n- [Optimistic Brides](https://blog.connext.network/optimistic-bridges-fb800dc7b0e0)\n- [The Cheater Checking Problem: Why the Verifierâ€™s Dilemma is Harder Than You Think](https://medium.com/offchainlabs/the-cheater-checking-problem-why-the-verifiers-dilemma-is-harder-than-you-think-9c7156505ca1)\n","lastmodified":"2022-10-06T10:43:51.524593808Z","tags":null},"/thoughts/rollups":{"title":"Making Sense of Rollups","content":"\nFirst of all, let's give you a banger to listen along this wild ride that I had while going deep and deep in this amazing rabbit hole of Rollups. The song is [Nine Cats](https://open.spotify.com/track/6M6S6DhGZZzmtTjP1iPvxb?si=d35d13e8c74e40ae) from Porcupine Tree. I listen to them quite often as Prog Rock is one of my favorite genres to listen, it's pretty wild yet sounds soothing to my ears.\n\n## Let's Dive in!\n\nModular Stack divided the monolithic state of a blockchain into four main parts:\n\n1. Data Availability (DA): making state and transaction data available to consumers cheaply and quickly\n2. Consensus: agreement over the transactions included in a block and their ordering\n3. Settlement: can vary between different implementations, but mainly refers to settling/validating of transactions on the chain through verifying/arbitrating proofs.\n4. Execution: computation of previous state â†’ transaction â†’ new state\n\n***Economic security*** refers to large amount of monetary value locked into the layer that is being secured by the network. In the end, the most economical secure layer will accrue the most value as that's where most of the premium and profit resides.\n\n## Different Rollups\n\n![1_Modular-Stacks](thoughts/images/1_Modular-Stacks.jpeg)\n\nThere are [four](https://twitter.com/apolynya/status/1511623759786307586) main kinds into which Rollups can be categorised:\n\n1. Smart Contract Rollups: they use already decentralised, economic secure L1 for settling through the use of smart contracts for arbitrating proofs.\n2. Enshrined Rollups: in-protocol rollup which doesn't rely on smart contracts and is built into the L1 spec itself.\n3. Sovereign Rollups: doesn't use another L1 for settling, and only use another DA layer for data and ordering. These type of rollups have full control over their stack and can outperform other alt-L1s.\n4. Validium / Celestiums: these use off-chain DA solution for cheap DA and settle proofs on other external chain.\n\n## Why Rollups?\n\nBecause Ethereum in it's older monolithic form was not designed to scale for the demand of thousands of transactions in a second and thus, rollups provide a better environment for apps to exist, with better transaction pricing and scalability. And also provide better grounds for research and technological advancements to take place which can't happen in a monolithic system.\n\nAny system that wants to house many high valued assets has to capture the monetary premium in the form of fees or MEV to guarantee the economic security. Many believe that to be the settlement layer as that's where a transaction validity gets finalised.\n\n## Rollup fees\n\n![14_Smart-Contract-Rollups.jpeg](thoughts/images/14_Smart-Contract-Rollups.jpeg)\n\nIn its current form i.e. pre *EIP-4844* and *Danksharding*, each rollup is essentially a *Smart Contract Rollup,* which posts its transaction data (in case of ORUs, ZKRs don't need to post complete transaction data), state roots along with proof data to L1 in the form of calldata to smart contracts. These SCRs have fixed cost in terms of state commitments and proofs that they submit and variable costs in the transaction data along with proposers signatures in case of ORs.\n\n\u003e Note: ZKRs don't need to submit transaction data and ORUs does is in case of fraud in ORUs, transaction data is needed to check the fraud but ZKRs along with validity proofs prove that the state diffs are valid. Thus, ZKRs post validity proofs every time to L1.\n\nOptimism currently uses two smart contracts at L1 that sequencer and proposer post to:\n\n1. Canonical Transaction Chain contract: append-only logs of transactions submitted by sequencer.\n2. State Commitment Chain contract: state roots proposed by the proposers for each transactions in CTC.\n\nPosting to both of these contracts incur a cost to L2s. Although most of these contracts will be archived sooner than later as Ethereum eyes its bigger upgrades and Rollups also upgrades to better infra in the form of ***bedrock*** for Optimism and ***Nitro*** for Arbitrum. Better DA layer, separate EIP-1559 fee market for rollup data will get rid of the smart contracts.\n\n![8_Value-Flows.jpeg](thoughts/images/8_Value-Flows.jpeg)\n\nBut with EIP-4844 and Danksharding looming, DA supply will overshoot and better compression from rollups as well will scale the TPS metric to an absurd amount. This brings to the question of value accrual as the value acquired as a DA layer won't be much if the DA supply isn't fully exhausted.\n\n|              | Target DA Bandwidth | Target Useful Data per block |\n| ------------ | ------------------- | ---------------------------- |\n| EIP-4844     | 83.33 KB/s          | 1 MB                         |\n| Danksharding | 1.33 MB/s           | 16 MB                        |\n\nWith Danksharding, Ethereum as a DA layer will be able to provide 1.33 MB/s data bandwidth with maximum economic security. Current Eth blocks average around 90KB with calldata being 10 KB of this. Rollups eventually want to optimise for 14 bytes/tx which with 1.33 MB/s bandwidth amounts for 100k sweet sweet TPS. Although, this should be kept in mind that these are best case numbers in a hyper-optimised rollup environment that will be implemented in a very long time horizon.\n\n## MEV in a modular stack\n\nWith MEV-Boost running in production with several relayers and in-protocol PBS as part of roadmap, this makes most of the MEV value to accrue to L1 base layer. Similarly, in a rollup environment, sequencers try to bid for block with maximum value from searchers which accrues most of the value to rollup layer. This is still a research area, with how much of the value that will get accrued to L1 DA/settlement layer or L2 stack. MEV value can get leaked to DA layer if it censors or delays settlement layer blocks.\n\nCelestia wants a small part of a bigger pie here due to optimising for the DA layer.\n\n## Attacks on DA Layer\n\n### 33% Attack\n\nThis leads to liveness failure in tendermint consensus and is a slashable event in Gasper consensus which leads to inactivity leak until the protocol can finalise again.\n\n### 67% Attack\n\nThere are certain validity condition in the consensus protocols such that even if 100% of the validators are dishonest, they can't just print native tokens out of nothing as the honest nodes won't accept the transactions and protocol can be forked using social consensus.\n\nAttacks that can happen if 67% of the protocol is malicious are:\n\n1. Double signing\n2. Data withholding\n3. Fraud proofs censorship\n\nThus, both DA and settlement layer is susceptible to frauds. Eventually, only that stack wins which accrues the most value i.e. most economically secure. Every layer has to be designed such that participants can capture a value of the transaction for self-sustaining environment otherwise, it leads to more centralisation. And, as it is put now, DA layer doesn't accrue much of the transactional value and it will be the case for a long time in the future.\n\n## Rollup Stack\n\n### SC Settlement Rollup \u0026 SC Recursive rollup\n\nOne question that comes to mind is why does Rollup has to use Ethereum as its Settlement layer? Because it's an economic secure network which behaves as an apt settlement layer due to its isolated execution environment that can arbitrate proofs submitted by the rollups. The smart contracts on Ethereum also serves as trust-minimised two way bridges between the rollup and the L1. It's trust minimised as *rollups operators* (proposers, sequencers, provers, challengers) due to the means of smart contracts doesn't need any third party to submit batches to the L1 and it's two way because L1 smart contracts receive block headers along with proofs and thus behaves as a light client.\n\nStarkware wants to create recursive smart contract rollups on top of Starkware L2, where L2 behaves as a settlement layer for L3s and more use-specific applications can be deployed as L3s on top of L2s. L2s verifier contract receives many validity proofs from L3s and recursively combines it into one proof and submit to L1.\n\n### Enshrined Rollups\n\nERs are what rollups would have looked like if an L1 supports them from the start, i.e. in-protocol support for verifying state changes. Currently, to verify if a state is valid, full nodes have to run all the transactions. Perfect game would be when the blocks or rollup batches have proofs attached and state validity could be proven. A single zkEVM which could verify the SNARK submitted with each block for state validity.\n\nAs the name mentions, ERs are currently a theoretical concept, meant to be part of the L1 itself in the spec rather deployed on top of smart contracts. So,\n\n- How does rollups settle to L1?\n\n   Full Nodes doesn't have to run all the transactions unless there's a fraud proof in case of ORUs, much better case in case of ZKRs where no re-execution happens due to provers providing validity proof with each batch.\n\n- Other thing that comes to mind is how will the in-protocol proofs would look like?\n\n   The simplest way for an L1 to support proofs within consensus layer is to re-execute the transactions with pre and post state roots. A zkEVM would get SNARK with every block as a sidecar or on-chain depending on the implementation.\n\n- What performance upgradation does in-house proof proving could provide?\n\n   With weak statelessness already part of the roadmap, full nodes wouldn't have to execute every transaction to check state validity due to SNARK coming with every block leading to simpler consensus logic. Removing compute bottleneck and statelessness reducing disk I/O also helps in raising the block gas limit. And bandwidth resource increases according to the Nielsen's Law. Also light clients can filter invalid state roots due to SNARKs much more quickly than fraud proofs.\n\n**Step 2** would be to deploy parallel zkEVMs ERs. These parallel ERs can verify separate SNARKs and then can settle to one main settlement rollup, performing like execution shards but better.\n\n![24.-Enshrined-Rollups_00288-1.jpeg](thoughts/images/24.-Enshrined-Rollups_00288-1.jpg)\n\nzkEVMs ERs have several benefits:\n\n1. **Social Alignment**: Follows social consensus\n2. **Secure**: No upgrade keys required\n3. **Economic**: maximum value accrual to ETH\n4. **Gas Efficient**: less gas cost as directly arbitrating proofs instead via SCs\n\nDisadvantages:\n\n1. Slow: has to go through vigorous protocol update phases\n2. Pre-Confirmations: Harder pre-confirmations as no centralised sequencers\n3. Less VM innovation\n4. Increased builder cost: more specialised builders that are able to verify the SNARK.\n\nThere are many arguments against zkEVM shards ERs potentially harming SCRs and innovation happening on it. Users wouldn't feel as comfortable using a private SCR and would instead choose a more embedded, easily verifiable ER, thus weakening these SCRs.\n\nThis could potentially lead to a rollup leaving L1 for their own to gain a bigger piece of the pie and gain more control over its users and stack. In my opinion, rollups doing this are just scared of competition, looking for profit and maximalism rather than what blockchains as a tool stands for and, that is, social coordination. ERs and SCRs could easily go hand-in-hand, with SCR housing the innovations in the ecosystem and ERs becoming the efficient, trust-minimised settlement layer.\n\nDA on its own doesn't accrue much value to the base layer, high-value transactions and massive liquidity is needed to attract more premium and value to the participants and that is achieved by being the best settlement layer. This could be achieved using zkEVM ERs. A successful settlement layer would only benefit the SCRs building on top, as more monetary premium leads to economic security and more trust a user has on the rollup. Rollups breaking away from the base layer would have to again fight the same battle of network effects, decentralisation, retaining users with fragmented liquidity and much lesser scaling options which is the whole point of rollup economics.\n\nEthereum as a base layer shouldn't just focus on rollups, but housing as many participants as possible with apps like Uniswap, Aave running and doing massive defi transactions on L1, SCRs, ERs.\n\nValidiums/Celestiums that use other chains as DA layers suffers from trust assumptions as there are two options for DA, off-chain and on-chain. Off-chain options could be fast and cheap but lacks sufficient decentralisation and On-chain options like Celestia could be a better option but if it goes down so does your chain relying on it for DA. Thus, rollups settling on same chain for DA and settlement avoid these issues as if Ethereum gets 51% attacked and reverts, so does the rollup settling to it. There are benefits that validiums provide that depends on the use case of data availability modes.\n\n\u003e Settlement alone then largely provides network effects, while DA adds on full security. These network effects are safely optimised by sharing DA.\n\n### [Sovereign Rollups](https://blog.celestia.org/sovereign-rollup-chains/)\n\n![28_Sovereign-Rollups.jpeg](thoughts/images/28_Sovereign-Rollups.jpg)\n\nSRs are other type of rollups that doesn't use other L1 as settlement layer but just use external DA and consensus layer. Settlement is handled within the rollup with their own clients. Full Nodes and Light Clients are used to sample DA and fraud/validity proofs for verification. Celestia is a solution in this direction which wants to be the go-to layer for cheap DA, giving sovereignty to chains. A rollup that uses another chain as settlement layer is bounded by the consensus rules of that chain, and doesn't have complete control over its changes. SR nodes verify the correct canonical chain for themselves, relying just for data on the external layer. The trust minimised two-way bridge converts to a one-way bridge for SRs as no verification is happening from the base layer.\n\nOne major drawback of having a chain that can fork endlessly even with minority is that it hurts composability. Apps that rely on other apps for their functionality would also have to fork and follow the new rollup chain which is not feasible in a bigger ecosystem and is quite pointless if the apps prefer the older chain, then you are just left with a new fork which no one is using. Thus, SRs make more sense for specific and single purpose DApps. DeFi is the most significant use case for decentralised applications at the moment and most of it relies on other apps for composability and yields, it wouldn't make much sense for apps to just spin up their own rollup chain to have sovereignty away from the base layer.\n\nOne advantage that SRs flex is the flexibility of having their own execution environments tailored for the specific use case rollup chain is designed for. This is not the case with ERs or SCRs that only supports base layer native execution environment for arbitrating proofs.\n\n#### Fraud Proofs in SRs\n\n![30_IVG.jpeg](thoughts/images/30_IVG.jpg)\n\n- Non-Interactive: simple flavour of proofs where the challenger submits the claim and the light clients execute the transactions completely to check if the claim is true.\n- Interactive: Challenger submits claim and responder responds to defend themselves. These entities play a ***Interactive Verification Game*** (IVG) with a referee.\n\nThere are two methods to distribute FPs over the network:\n\n- P2P layer: distributed via nodes to other nodes, faster light client finality, no censoring by L1 miners/validators.\n- On-chain inclusion\n\nCelestia is a dirty ledger that means it doesn't check for validity and there's no concept of finality and just focuses on putting the data out there. Now, if a challenger wants to prove a Tx is an invalid transaction, it needs to have all the related transactions that could prove the malicious transaction is invalid and thus, this again brings us to the same place of archival nodes having to store all of the transactions. To mitigate this, weak subjectivity assumption is introduced. SCRs challengers has to keep the transaction data for the challenging period while SRs challengers now need to keep the data for the weak subjectivity period.\n\nIVG also introduces synchrony assumptions where all the light clients have to be connected to the honest challengers in order to distribute proofs. Thus, SRs are likely to use single round FPs as light clients can then participate in the distribution of proofs.\n\n### Bridging in SRs\n\nSRs do add additional design considerations over bridging as the settlement is happening on the rollup side and not on the base layer.\n\nThere can be two types of bridges:\n\n- Committee-based: A validator set of source chain attest to the validity of a block that is being used by another chain. Not trust-minimised as committee can steal funds. IBC is an example of this.\n- Proof-based: Trust-minimised bridging where SRs can verify each other's state using proofs, much more complex than other types.\n\n   Two types of bridging settlement here:\n\n   - P2P settlement: where light clients are embedded in the chain and receive proofs over P2P network. Both SRs have bridging contract allowing for lock-and-mint mechanism.\n   - On-chain settlement: both rollups run light clients as smart contracts. proofs and transaction data stored on-chain in these contracts.\n\n   Two types of upgrade mechanisms possible:\n\n   - Static Bridging: In which the rollup $SR_A$ doesn't support $SR_B$'s execution environment and has to support it node software upgrades. Now, if $SR_B$ forks, $SR_A$ has to fork itself to support $SR_A$ execution environment again. Thus, social consensus or governance is needed to add bridge.\n   - Dynamic bridging: rollup's support each other execution environment, thus not needed to fork and can support other's proofs directly.\n\n   Two types of bridging possible in SRs:\n\n   - Pairwise Bridging: every rollup has a bridge to every other rollup which means $N^2$ bridges for N rollups.\n   - Hub-and-Spoke model: a central hub SR which aggregates all the liquidity and communication from all the rollups. Similar to what Cosmos Hub wants to be.\n\n### Aggregated ZK Bridging\n\nProblem: $SR_1 - SR_2$ through $SR_N$\n\n1. An aggregate prover would receive proofs off-chain $SR_2 - SR_N$, runs a light client for each chain.\n2. Combines proofs into one aggregated proof and includes it in a smart contract on-chain.\n3. $SR_1$ verifies proof in the same amount of time it would take to verify one proof.\n\n### ETH Sovereign Rollups\n\nWould be in effect when Ethereum implements data blobs carrying transactions which makes ETH to be used as a DA layer.\n\nTradeoffs:\n\n- Better liveness: gasper can retain liveness in 33% attack\n- Better economic security\n- slower finality\n- More overhead: no DAS in ethereum yet, thus can't run light clients and need to run full nodes.\n\n### Celestia Sovereign Settlement Rollup \u0026 SC Recursive Rollup\n\n![32_SSRSCR.jpeg](thoughts/images/32_SSRSCR.jpeg)\n\nSRs are specially designed for recursive rollups to live on top of. These recursive rollups use SRs for shared settlement establishing a two-way trust minimised bridge between the layers. Submits proofs, state updates and transaction data to SRs, which then batches these rollup blocks and post them to Celestia.\n\nThis reduces a lot of bridging overhead found in legacy SRs where $N_2$ bridges exist for each pair of rollups. Thus, this creates $N$ trust minimised bridges to communicate with a shared settlement layer. One could argue a restrictive more specific settlement layer that only allows specific operations to perform on top of or a general purpose settlement layers. A specific settlement layer would be cheap and easier to set up with less complexities but also comes at a UX degrade of not having many use-cases like DeFi pooling, dAMMs, etc.\n\nBut a separate settlement layer could pose more challenges for censorship and force transaction inclusion which is supported in SCRs. Participants can leave rollup to post their transaction to L1 anytime just trading off the cheap fees, or sequencers are forced to include transactions within a certain time period which will get decentralised with more sequencers joining eventually.\n\nBut how do you do this with shared SRs? If settlement layer is censoring a rollup, then there needs to be a way to submit transactions directly to DA layer and skip settlement layer altogether.\n\nLiveness risks are still associated as liveness failure of settlement layer will render the rollup useless, but not in the case of sequencers failure of rollup itself, as then you can settle to settlement layer directly.\n\n## Questions\n\n- What do we really mean by a settlement layer?\n- Does DA layer need economic security or its function is just to provide data cheaply and quickly?\n- Atomic cross-chain MEV?\n- Danksharding and EIP-4844 specs\n- IVG\n- ZK Bridging\n- Cevmos\n- statelessness\n\n## Readings\n\n- [[posts/Optimistic Rollups|ORUs]]\n- [The Complete Guide to Rollups - Delphi Digital](https://members.delphidigital.io/reports/the-complete-guide-to-rollups)\n- [Understanding rollup economics from first principles](https://barnabe.substack.com/p/understanding-rollup-economics-from)\n- [l2Beat](https://l2beat.com/scaling/tvl/)\n- [Modular war on Twitter](https://twitter.com/dystopiabreaker/status/1531102983894597632ï¿¼)\n- [EF Reddit AMA Enshrined Rollups](https://www.reddit.com/r/ethereum/comments/vrx9xe/comment/if11ljm/?utm_source=share\u0026utm_medium=web2x\u0026context=3)\n- [Sovereign Rollup Chains](https://blog.celestia.org/sovereign-rollup-chains/)\n- [Fraud Proofs Explainer](https://medium.com/infinitism/optimistic-time-travel-6680567f1864)","lastmodified":"2022-10-06T10:43:51.524593808Z","tags":null},"/thoughts/tendermint":{"title":"Tendermint","content":"\nTendermint is the consenus protocol used by Cosmos to reach consensus in a distributed system.","lastmodified":"2022-10-06T10:43:51.524593808Z","tags":null},"/thoughts/uncovering-celestia":{"title":"Uncovering Celestia","content":"\n\u003e Privileged.\n\u003e A Chosen Few.\n\u003e Blessed With our time in Hell.\n\nAnother song today would be [Hourglass](https://open.spotify.com/track/2pd1Lm8Jsslf2VdWQv0Je8?si=5b90339ee6804f32) from Lamb of God. I discovered this band when I just started listening to Metal and was stealthily going around the woods but listening to this song just instils a strange sense of rage in you.\n\nAnywas, these are my thoughts towards an amazing piece of tech, redesigning the [[thoughts/rollups|Rollups]] architecture from ground up, with several challenges in front of them.\n\n## Let's Dive In\n\nCurrent problems are Monolithic chains. Celestia answers what minimum a blockchain can do to provide shared security to other blockchains (rollups)?\n\n\u003e Celestia combines the best of both worlds: Ethereumâ€™s Rollups with its shared security and Cosmosâ€™s sovereign interoperable app-chains.\n\nIn a series of transactions, a blockchain cares about two things, *consensus* and *validity*.\n\n**Validity** rules determine which transactions are considered valid to be included in the blockchain while **Consensus** rules determine the order of the valid transactions to be included.\n\nCelestia uses [[thoughts/tendermint|Tendermint]] as its consensus protocol and is only responsible to enforce consensus rules. It doesnâ€™t care about transaction validity nor is it responsible to execute them. Rollups on top of celestia monitor for transactions important for them, download and execute them accordingly.\n\n![9448d3db-ignore.png](https://storage.googleapis.com/members-portal-bucket/uploads/2022/02/9448d3db-ignore.png)\n\nQn: If execution doesnâ€™t happen on celestia core, then does every app running on top of celestia has to execute each transaction and how does that help solve scalability problems found in other monolithic chains?\n\n## Scalability Bottleneck\n\nMost common issue suffered by major blockchains is **state bloat,** i.e. growth of nodes storage requirements with each added transactions to the history. Solution to this problem is existence of light clients, which doesn't download complete block data but only the block header and represents 99% of the users.\n\nLight clients, however are not able to tell if a tx is invalid or not as it doesnâ€™t have the data and works under the assumption that consensus nodes are honest. As the state continues to increase, full nodes start decreasing and light nodes start increasing leading to more centralization around full nodes.\n\nFraud/Validity proofs negate the occurrence of this phenomenon as any light client can run these succinct proof to verify whether the contents of the blocks are valid or not.\n\n## How Fraud/Validity proofs work\n\n![02571c37-simplified-fraud-proof.png](https://storage.googleapis.com/members-portal-bucket/uploads/2022/02/02571c37-simplified-fraud-proof.png)\n\nFull nodes can provide light nodes with just enough data for them to verify that a particular transaction in included in block or not. This happens using merkle trees, which can efficiently prove that a particular tx is included w/o requiring them to download the whole block. So, these full nodes can provide just a subset of data and these light clients can verify against that.\n\nBut for full nodes to generate *fraud/validity proofs*, they need access to complete block data to run these transactions. If a malicious miner withholds block data, then light client wonâ€™t notice and continue to follow the chain as full nodes wonâ€™t be able to provide any proofs for the invalidity of that block. Thus, **Data Availability** is a necessary condition for fraud/validity proofs to exist.\n\nCelestia solved this problem by making data availability a necessary rule such that this can be enforced even by light nodes through `Data Availability Sampling (DAS)` .\n\n## Data Availability Sampling\n\nDAS is implemented in celestia using a technique called `Erasure Coding` , which is a data protection technique used at various places. Erasure coding to a data extends the data such that complete data can be recovered using just a fraction of the extended data.\n\nLight nodes randomly sample small fixed-sized chunks of block data from the block and have probabilistic assurances that other chunks have been made available to the network.\n\n![a4e60aab-eithercase.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/CD732BFD-616B-4A45-BDB8-132508E3914F_2/OKzZK6rzyYtwYyNf0SI2pK0Bpv03SG6uzxxRTPczC78z/a4e60aab-eithercase.png)\n\nWhen light clients start sampling the chunks of data along with the merkle proofs of inclusion of data in the block, from the block producer, it has two options:\n\n1. To make the data available, in which case enough light clients can request a significant amount of chunks such that full nodes can recover the whole block with that data.\n2. Data is not available, in which case full nodes are not able to recover the block data and light clients stop following the chain as the sampling process was not completed.\n\nThis results in liveness failure of the blockchain and the nodes stop following the chain. DAS guarantees that data availability is the necessary condition for blockchain to resume block production with both full and light nodes under same security assumptions.\n\n## How much can Celestia scale\n\nSome tradeoffs exist in DAS light nodes, i.e. the block header grow in proportion to the square root of blockâ€™s size. For any blockchain to scale, it means processing higher throughput â†’ processing more transactions â†’ either more blocks or larger blocks. Size of a block depends upon\n\n- Amount of data that can be collectively sampled\n- Target block header size a light node can process.\n\nNow, the aha moment of light clients comes in play here. Running a light client is as easy as running an app on a mobile phone and thus, as the demand of the blockspace rises, more and more nodes join the network and network bandwidth to sample the data rises and thus block size rises with it. This means, unlike monolithic blockchains which starts to congest as demand rises, modular blockchain flourish and can easily support the rise in demand along with stable low fees.\n\nThe second part where the size of the block header rises with rise in block size can be compensated with the rise in bandwidth which makes it easier for celestia throughput to grow.\n\n## Benefits\n\n1. **Self-sovereign blockchains**\n\nSelf-sovereignity represents entity which has complete control of his/her data.\n\nCurrent Ethereum rollups post tx data along with proofs to L1 in the form of smart contracts, thus there has to be a way of on-chain governance mechanisms. This is not fully secure and decentralised to operate a system trying to support loads of high amount of throughput and handling significant amount of value. Celestia differs here in the sense that it doesnâ€™t make any sense of the data and simply stores valid transactions, its the rollups that has to make sense of the required data and thus, has to make its own canonical chain.\n\nThis makes it significantly easier for any rollup to perform hard/soft forks which are regarded as a threat to any current monolithic blockchains. These forks dilute the security of the network and reduces itâ€™s users faith on the underlying. But Celestia rollups can freely change the algorithm it uses in its nodes to make sense of the data without any threat of security failure because the DA and consensus layer remains the same with all the valid transactions.\n\n2. **Flexibility**\n\nSince celestia can freely take the roles of DA and consensus layer for rollups, it is much easier for any rollup to have its own execution environment with native VMs specially made for their use case. Thus, the VM market becomes open for any players to spin up flexible execution environments.\n\n3. **Effortless deployment**\n\nDeploying a blockchain has become easier and easier with the passage of time and significant improvements in chain architectures. Cosmosâ€™ SDK is the prime example of how much easier it is to spin up their own app chain without writing millions of lines of node codes.\n\nCelestia takes it a step further with rollups not having to worry about the validators it needs to bootstrap a blockchain. New rollups can be deployed with the click of a button having same security assumptions as the oldest rollups on the network.\n\n4. ### Efficient Resource pricing\n\nIn ethereum scenario, rollups post data on to L1 which charges gas for each byte that is stored on-chain. This is then susceptible to L1 gas variations and cause fee spikes in L2s as well. Celestia, however only charges for historical state data that is stored as blockspace in bytes per sec and active state execution is handled by each rollup environment. Thus, spikes in one environment doesnâ€™t affect another.\n\n5. **Trust minimised bridges**\n\nL1 canâ€™t communicate with each other as there are no ways for one chain to verify state of another chain as they canâ€™t execute the fraud/validity proofs of each other. Whereas rollups can form trust minimised bridges with each other as they all share same security. Ethereum has access to all rollup data and can execute their proofs. This allows them to create trust minimised bridges or trusted bridges depending on the requirements of rollups.\n\n6. **Faster and minimal governance**\n\nSeparation of stacks also separates the amount of required to execute any improvements proposals as each layer can focus on themselves and iterate faster and much more effectively without worrying about the DA or consensus layer.\n\n7. **Decentralised block verification, not production**\n\nIt doesnâ€™t matter if in the long run various external factors tend to make block production centralised if you have sufficient amount of verifiers that can easily verify validity of a chain and continue to uphold the consensus rules to make them an efficient trustless machines.\n\n8. **Simplicity**\n\nCelestia made simpler architectural decisions that let them reduce any unnecessary tech burden and let them iterate and develop faster.\n\n## Challenges\n\n1. Bootstrapping execution environments: celestia as DA layer has no use if there are no execution environments on top of celestia to take advantage of all the DA throughput.\n2. Determining appropriate block sizes, although celestia has the property of scaling data sampling as more number of nodes join the network, but there are no mechanisms to determine number of light clients in the network. Also, light clients cannot be rewarded for data sampling processes, thus, has no incentives.\n\n![cevmos-celestiums.png](https://res.craft.do/user/full/e83dd57b-d460-d205-2243-2f6ed8de496a/EF825D6C-48E0-46D5-85B6-F4FE7DBDD949_2/CfHFGvj6vjieQD4EScjkPaAlunvQm3o1bbkJDOGB3JQz/cevmos-celestiums.png)\n\n3. Third, is the limitation of Celestia native token utility because DA layer is not where value accrual in a blockchain takes place. Value accrual in a blockchain occurs mostly at settlement layer as no thatâ€™t where state execution takes place. Celestiaâ€™s native token will have to be moved to settlement layer using trusted bridges.\n\n## Questions\n\n1. Details about erasure coding.\n2. sparse merkle trees\n3. light clients\n\n## Readings\n\n- [Pay Attention To Celestia - Delphi Digital](https://members.delphidigital.io/reports/pay-attention-to-celestia)\n- [A note on data availability and erasure coding Â· ethereum/research Wiki](https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding)\n- [https://arxiv.org/pdf/1809.09044.pdf](https://arxiv.org/pdf/1809.09044.pdf)\n- [A note on data availability and erasure coding Â· ethereum/research Wiki](https://github.com/ethereum/research/wiki/A-note-on-data-availability-and-erasure-coding)\n","lastmodified":"2022-10-06T10:43:51.524593808Z","tags":null},"/thoughts/uniswap-v3":{"title":"Uniswap V3","content":"\nFeatures:\n\n- Concentrated Liquidity\n- Improved Price Oracle\n- Flexible Fees\n\n## Uniswap V2 IL\n\nMarket with Liquidity $L$ and Asset $X$ and $Y$ with amounts $x$ and $y$ respectively.\n$$x*y = L^2$$\nInitial Price $P$ of asset $X$ in terms of $Y$ = $y/x$, with price movement from $P$ to $P' = Pk$.\nThen,\n$$x = \\frac{L}{\\sqrt{P}}$$\n$$y = L\\sqrt{P}$$\n\nWe, introduce three terms\n\n- $V_0$, value of initial holdings in terms of Y\n- $V_1$, value when kept in pool\n- $V_{held}$, value when held\n\n$$V_0 = y * 1 + x * P = 2L\\sqrt{P}$$\n\n$$V_1 = 2L\\sqrt{P'} = 2L\\sqrt{Pk}$$\n$$V_{held} = y + xP' = L\\sqrt{P}(1+k)$$\n$$IL(k) = \\frac{V_1 - V_{held}}{V_{held}} = \\frac{L\\sqrt{P}(2\\sqrt{k} - 1 - k)}{L\\sqrt{P}(1 + k)} = \\frac{2\\sqrt{k}}{1+k} - 1$$\n\n## Uniswap V3 Pool Maths\n\nUniswap V3 pools are different from V2 as the liquidity is not uniformly distributed from $0$ to $\\infty$, but each position functions as a separate CFMM curve.\n\n![Uniswap V3 Concetrated Liquidity](thoughts/images/UniswapV3Liquidty.png)\n\nFor an $xy=k$ curve, let $P_a$ and $P_b$ be the price range in which liquidity is deployed, then Real Reserves:\n\n$$(x + x_{offset})(y + y_{offset}) = L^2$$\n\nwhere, $x_{offset}$ and $y_{offset}$ is the point A and B at which lines $x=ky$ touch the liquidity curve. $P_b$ \u003e $P_a$, i.e. steeper the slope more is the price. Since, liquidity outside this position is irrelevant, the real reserves are obtained shifting the curve by the offsets.\n\n![Uniswap V3 Virtual Reserves](thoughts/images/UniswapV3Reserves.png)\n\nEquation of Real Reserves becomes:\n$$(x + x_{offset})(y + y_{offset}) = L^2$$\n$$(x + \\frac{L}{\\sqrt{P_b}})(y + L\\sqrt{P_a}) = L^2$$\n\nThis can be divided into three parts:\n\n1. Current Price, $P \u003c= P_a$\n\nWhen price \u003c $P_a$, all of the liquidity is in asset X, then\n\n$$(x + \\frac{L}{\\sqrt{P_b}})L\\sqrt{P_a} = L^2$$\n$$x = L\\frac{\\sqrt{P_b} - \\sqrt{P_a}}{\\sqrt{P_a}\\sqrt{P_b}}$$\n\n2. $P \u003e= P_b$\n\nWhen price \u003e $P_b$, all of the liquidity is in asset Y, then\n\n$$y = L(\\sqrt{P_b} - \\sqrt{P_a})$$\n\n3. $P_a \u003c P \u003c P_b$\n\nAssume that the price moves from P_a to P, then the amount of asset Y contributing to liquidity should be equal to amount of asset X contributing to liquidity when moving from P_b to P.\n\n$$L_x (P_b, P) = L_y(P, P_a)$$\n$$x\\frac{\\sqrt{P}\\sqrt{P_b}}{\\sqrt{P_b} - \\sqrt{P}} = \\frac{y}{\\sqrt{P} - \\sqrt{P_a}}$$\n$$x = L\\frac{\\sqrt{P_b} - \\sqrt{P}}{\\sqrt{P}\\sqrt{P_b}}$$\n$$y = L(\\sqrt{P} - \\sqrt{P_a})$$\n\n## Core\n\nUniswap's Core contracts functionality can be divided into three main components:\n\n1. Price and Liquidity\n2. TWAP Oracle\n3. Fees\n\nMain building blocks for these functionality is through `ticks` and `positions`.\n\n### Ticks\n\nA tick in terms of price is represented as:\n$$i_c = \\lfloor{\\log_{\\sqrt{1.001}}\\sqrt{P}}\\rfloor$$\n\nIn Uniswap V2, price space was continuous as the liquidity was divided across the whole space i.e. $[0, \\infty]$. V3 introduced the concept of concentrated liquidity in the form of ticks i.e. now, the price is divided discretely such that 1 tick represents 1 basis points (0.01% price change).\n\n```solidity\n// info stored for each initialized individual tick\nstruct Info {\n    // the total position liquidity that references this tick\n    uint128 liquidityGross;\n    // amount of net liquidity added (subtracted) when tick is crossed from left to right (right to left),\n    int128 liquidityNet;\n    // fee growth per unit of liquidity on the _other_ side of this tick (relative to the current tick)\n    // only has relative meaning, not absolute â€” the value depends on when the tick is initialized\n    uint256 feeGrowthOutside0X128;\n    uint256 feeGrowthOutside1X128;\n    // the cumulative tick value on the other side of the tick\n    int56 tickCumulativeOutside;\n    // the seconds per unit of liquidity on the _other_ side of this tick (relative to the current tick)\n    // only has relative meaning, not absolute â€” the value depends on when the tick is initialized\n    uint160 secondsPerLiquidityOutsideX128;\n    // the seconds spent on the other side of the tick (relative to the current tick)\n    // only has relative meaning, not absolute â€” the value depends on when the tick is initialized\n    uint32 secondsOutside;\n    // true iff the tick is initialized, i.e. the value is exactly equivalent to the expression liquidityGross != 0\n    // these 8 bits are set to prevent fresh sstores when crossing newly initialized ticks\n    bool initialized;\n}\n```\n\n### Position\n\nPosition refers to the liquidity earned after submiting tokens to the pool and measures fees earned over that position.\n\n```solidity\nstruct Info {\n  uint128 liquidity;\n\n  // fee growth after last update\n  uint256 feeGrowthInside0LastX128;\n  uint256 feeGrowthInside1LastX128;\n\n  // fees owed to position in tokens0/1\n  uint128 tokensOwed0;\n  uint128 tokensOwed1;\n}\n```\n\n### Fee Growth\n\nEach position update require updates in fees inside that range.\n\n$$\n\\begin{equation}\n  f_r = f_g - f_b(i_l) - f_a(i_u)\n\\end{equation}\n$$\n\n$$\n\\begin{equation}\n  f_a(i) =\n  \\begin{aligned}\n    \\begin{cases}\n      f_g - f_o(i) \u0026 {i_c \\ge i} \\\\\n      f_o(i) \u0026 i_c \u003c i\n    \\end{cases}\n  \\end{aligned}\n\\end{equation}\n$$\n\n$$\n\\begin{equation}\n  f_b(i) =\n  \\begin{aligned}\n    \\begin{cases}\n      f_o(i) \u0026 {i_c \\ge i} \\\\\n      f_g - f_o(i) \u0026 i_c \u003c i\n    \\end{cases}\n  \\end{aligned}\n\\end{equation}\n$$\n\n### Mint or Burn Flow\n\n```solidity\nmint(address recipient, int24 tickLower, int24 tickUpper, uint128 amount, bytes calldata data)\n\nburn(int24 tickLower, int24 tickUpper, uint128 amount)\n```\n\n1. Modify position\n   1. check ticks\n   2. update position\n      1. get position\n      2. if `liquidityDelta != 0`, `observeSingle`\n      3. update lower and upper tick with data\n      4. if either flipped, update ticks bitmap\n      5. get `feeGrowthInside{0,1}X128`\n      6. update position\n   3. if `liquidityDelta != 0`, calculate `amount0` and `amount1` according to tick range\n      1. `currentTick \u003c tickLower`: all liquidity in token0\n      2. `currentTick \u003e tickUpper`: all liquidity in token1\n      3. else, write oracle entry, calculate `amount0` and `amount1`\n2. call mintcallback or burn callback\n3. update balances if burn or check balances if mint\n\n## Oracles\n\nUniswap V2 `cumulativePrices` served as Oracles. Now, V3 introduces historical pricing\n\n## Resources and Acknowledgements\n\n- [Uniswap docs](https://docs.uniswap.org)\n- [Uniswap V3 LP Rekt](https://rekt.news/uniswap-v3-lp-rekt/)\n- [Uniswap v3 liquidity formula explained](https://atiselsts.medium.com/uniswap-v3-liquidity-formula-explained-de8bd42afc3c)\n- [Liquidty Maths in Uniswap V3](https://atiselsts.github.io/pdfs/uniswap-v3-liquidity-math.pdf)\n- [Uniswap V3 math Desmos](https://www.desmos.com/calculator/q2kxfue441)\n- [Technical Analysis of Uniswap V3](https://credmark.com/blog/a-technical-analysis-of-uniswap-v3)\n- [Impermanent Loss On Uniswap V3](https://medium.com/auditless/impermanent-loss-in-uniswap-v3-6c7161d3b445)\n- [Liquidity Providing in Uniswap v3](https://reuptake.medium.com/liquidity-providing-in-uniswap-v3-49bf3a0bd2ec)\n- [Uniswap V3 LP tokens as Perp](https://lambert-guillaume.medium.com/uniswap-v3-lp-tokens-as-perpetual-put-and-call-options-5b66219db827)\n\n### TWAP Oracles\n\n- [Uniswap Oracle attack simulator](https://blog.euler.finance/uniswap-oracle-attack-simulator-42d18adf65af)\n- [Uniswap V3 TWAP Oracle](https://medium.com/blockchain-development-notes/a-guide-on-uniswap-v3-twap-oracle-2aa74a4a97c5)\n\n### Academics\n\n- [Impermanent Loss In Uniswap V3](https://arxiv.org/abs/2111.09192)\n\n### Other Resources\n\n- [Awesome Uniswap V3](https://github.com/GammaStrategies/awesome-uniswap-v3)\n- [@Sabnock's Uniswap Resources](https://github.com/Sabnock01/uniswap-resources)\n","lastmodified":"2022-10-06T10:43:51.524593808Z","tags":null}}